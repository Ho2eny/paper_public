---
title: "ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark - Analysis"
arxiv_id: "2406.20015"
type: "analysis"
paper_type: "benchmark"
analyzed_date: "2026-01-10"
source: "./2406.20015.md"
details: "./2406.20015-details/"
venue: "ACL 2024"
---

# ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark - Analysis

## 1. Overview

**1문단: 필요성**
이 논문은 Tool-augmented LLM의 **hallucination 문제**를 진단하기 위한 벤치마크를 제안한다. 기존 tool-use 벤치마크들(AgentBench, ToolBench, AgentBoard)은 사용자가 **완전하고 사용 가능한 toolset을 제공한다는 가정** 하에 설계되어, LLM이 도구를 사용할 때 발생하는 hallucination 현상을 충분히 평가하지 못했다. 실제로 LLM은 해결 불가능한 문제에 대해 존재하지 않는 도구를 사용하거나 잘못된 도구를 선택하는 등의 hallucination을 보인다.

**2문단: 설계**
ToolBH는 **깊이(Depth)**와 **넓이(Breadth)** 두 관점에서 hallucination을 평가한다. 깊이 측면에서는 3단계 진단 프레임워크를 도입한다: (1) **Solvability Detection** - 태스크가 주어진 toolset으로 해결 가능한지 판단, (2) **Solution Planning** - 해결을 위한 도구 시퀀스 계획, (3) **Missing-tool Analysis** - 해결 불가능한 경우 누락된 도구의 기능 설명. 넓이 측면에서는 3가지 hallucination 유발 시나리오를 정의한다: Missing Necessary Tools(MNT), Potential Tools(PT), Limited Functionality Tools(LFT).

**3문단: 평가**
7개 subtask에 걸쳐 700개의 수동 어노테이션된 샘플을 수집하여 평가를 수행했다. 14개 LLM(7개 proprietary + 7개 open-weight) 평가 결과, 최고 성능인 Gemini-1.5-Pro도 100점 만점에 **45.3점**에 그쳤고, GPT-4o는 **37.0점**을 기록했다. 특히 open-weight 모델 중 최고인 Llama-3-70B는 Gemini-1.5-Pro의 32%, GPT-4o의 40% 수준에 불과했다. 에러 분석 결과, **Solvability hallucination**이 가장 주요한 실패 원인으로 나타났다.

**4문단: 의의**
ToolBH는 tool-augmented LLM의 **unsolvable task handling** 능력을 처음으로 체계적으로 평가하는 벤치마크다. 모델 파라미터 크기가 클수록 성능이 좋다는 가정을 반박하며, 학습 데이터와 응답 전략의 중요성을 강조한다. 또한 proprietary vs open-weight 모델 간의 성능 격차가 unsolvable 시나리오에서 특히 크다는 것을 밝혀, 향후 연구 방향을 제시한다.

---

## 2. Core Section

### TL;DR

> Tool-augmented LLM이 **해결 불가능한 태스크**에서 hallucination을 얼마나 잘 피하는지 평가하는 벤치마크로, **3단계 깊이(Solvability→Planning→Missing-tool)** × **3가지 시나리오(MNT/PT/LFT)**로 구성되며, 현재 SOTA 모델도 45점/100점에 그쳐 심각한 한계를 드러낸다.

→ 상세: [tldr.md](./2406.20015-details/tldr.md)

### Core Contributions

1. **Multi-level Diagnostic Framework**: 3단계(L1→L2→L3) 진단으로 hallucination 원인 심층 분석 → 단순 정오답 넘어 "어디서 왜 실패하는지" 파악 가능
2. **Hallucination-inducing Scenarios**: MNT/PT/LFT 3가지 toolset 특성 기반 시나리오 → 현실적 unsolvable 상황 재현
3. **Unsolvable Task Focus**: 기존 "solvable 가정" 벤치마크의 한계 극복 → Tool-use의 long-tail robustness 평가
4. **Progress Rate (PR) Metric**: 순차적 도구 호출의 정확성 평가 → 부분적 성공도 측정 가능

→ 상세: [contributions.md](./2406.20015-details/contributions.md)

### Key vs Non-Key Sections

| Priority | Sections | Reason |
|----------|----------|--------|
| ⭐⭐⭐ Must Read | Section 3.2 (Multi-level Diagnostic Task) | 3단계 평가 체계의 핵심 이해 |
| ⭐⭐⭐ Must Read | Section 4.3 (Main Results), Table 2 | 모델별 성능 비교 핵심 결과 |
| ⭐⭐ Important | Section 5.2 (Error Analysis), Figure 4 | 실패 원인 5가지 유형 분류 |
| ⭐⭐ Important | Section 3.3 (Hallucination-inducing Tasks) | MNT/PT/LFT 시나리오 이해 |
| ⭐ Reference | Section 5.3 (Solvable vs Unsolvable) | Proprietary vs Open-weight 격차 |
| Skip | Appendix A.3+ (Generation Prompts) | 데이터 생성 상세 (필요시 참조) |

→ 상세: [key-sections.md](./2406.20015-details/key-sections.md)

---

## 3. Paper Type

**Type**: Benchmark

| Aspect | Value |
|--------|-------|
| **Evaluation Target** | Tool-augmented LLM Hallucination |
| **Task Count** | 700 samples (350 solvable + 350 unsolvable) × 3 levels = 1,050 test samples |
| **Scenarios** | MNT (Missing Necessary Tools), PT (Potential Tools), LFT (Limited Functionality Tools) |
| **Main Metrics** | L1-EM (Exact Match), L2-PR (Progress Rate), L3-PR, L3-MS (Matching Score) |

→ 상세 방법론: [methodology.md](./2406.20015-details/methodology.md)

---

## 4. Visual Analysis

### Key Figures

#### Figure 2: ToolBH Pipeline

**구성 요소**:
- **In-breadth**: 3가지 시나리오 (MNT, PT, LFT) - toolset 특성 기반
- **In-depth**: 3단계 평가 (Solvability Detection → Solution Planning → Missing-tool Analysis)
- **UnsolvableQuery Tool**: 해결 불가능한 sub-goal 표시용 특수 도구

**핵심 통찰**:
- 단순 binary (solvable/unsolvable) 넘어서 **"어디까지 맞았나"** 측정
- 각 단계가 이전 단계의 정보를 활용 (progressive difficulty)
- Missing-tool analysis로 **개선 방향**까지 제시 가능

**Source**: [Figure 2](./2406.20015.md#figure-2-the-pipeline-of-toolbh-benchmark)

---

#### Figure 4: Error Analysis

**분류된 에러 유형**:
1. **Non-existent tools**: 제공되지 않은 도구 사용 (open-weight에서 빈번)
2. **Wrong tools**: 잘못된 도구 선택 (모든 모델에서 발생)
3. **Solvability hallucination**: 해결 불가능한 것을 가능하다고 판단 (**가장 빈번**)
4. **Wrong 'UnsolvableQuery' index**: 잘못된 위치에서 unsolvable 선언
5. **Wrong tool reasoning**: 도구 순서 오류 (proprietary에서 더 빈번)

**핵심 통찰**:
- **Solvability hallucination**이 L3에서 가장 빈번 → 핵심 연구 과제
- Proprietary 모델: non-existent tool 에러 적음 (도구 인식 우수)
- Open-weight 모델: solvability 판단 자체에서 큰 어려움

**Source**: [Figure 4](./2406.20015.md#figure-4-error-analysis-of-proprietary-and-open-weight-models)

---

### Math Formulations

#### Progress Rate (PR)

$$PR = \frac{k-1}{|G|}$$

**변수 설명**:
- $k$: 예측 시퀀스 $P$와 정답 시퀀스 $G$ 간 첫 번째 불일치 인덱스
- $|G|$: 정답 시퀀스 길이
- 완전 일치 시 $k = |G| + 1$ → PR = 1.0

**직관적 설명**:
- "첫 번째 틀린 곳까지 몇 %나 맞았는가"
- 완전히 틀리면 PR = 0, 완전히 맞으면 PR = 1
- **순서 민감**: 초반에 틀리면 이후 전부 0점

**예시**:
```
Ground Truth: [ToolA, ToolB, UnsolvableQuery, ToolC]
Prediction:   [ToolA, ToolB, ToolC, ToolD]
→ k = 3 (3번째에서 불일치)
→ PR = (3-1) / 4 = 0.5
```

#### Matching Score (MS)

$$MS_{sub\text{-}goal} = \begin{cases} 1.0 & \text{if exact match} \\ \cos(emb_u, emb_g) & \text{otherwise} \end{cases}$$

**변수 설명**:
- $emb_u$: 예측된 도구 설명의 embedding
- $emb_g$: 정답 도구 설명의 embedding
- all-MiniLM-L6-v2 사용

**Source**: [Section 3.2.2](./2406.20015.md#322-level-2-solution-planning), [Section 3.2.3](./2406.20015.md#323-level-3-missing-tool-analysis)

---

### Tables Interpretation

#### Table 2: Main Leaderboard

| Model | MNT (L1/L2/L3) | PT (L1/L2/L3) | LFT (L1/L2/L3) | Overall |
|-------|----------------|---------------|----------------|---------|
| Gemini-1.5-Pro | 62.7/70.7/42.8 | 56.0/68.6/43.6 | 69.0/27.1/8.5 | **45.3** |
| GPT-4o | 53.3/69.9/24.8 | 47.0/62.0/27.9 | 56.0/21.9/6.8 | 37.0 |
| Llama-3-70B | 31.3/35.5/4.8 | 19.0/23.6/5.2 | 34.0/4.0/0.0 | 14.6 |

**주요 발견**:
1. **L3 급락**: 모든 모델이 L1→L2→L3로 갈수록 성능 급감 (LFT에서 특히 심각)
2. **LFT 시나리오**: 기능 제한 도구 시나리오에서 모든 모델 고전
3. **Open vs Proprietary Gap**: Unsolvable에서 격차 극심 (39.4% 수준)
4. **모델 크기 ≠ 성능**: Llama-3-8B가 Llama-2-70B보다 높은 점수

**실무적 의미**:
- Tool-use 시스템에서 **"할 수 없다"고 말하는 능력** 중요
- 단순 solvability 판단조차 어려움 → Production 배포 시 주의

**Source**: [Table 2](./2406.20015.md#table-2-the-toolbh-leaderboard-with-14-llms)

---

## 5. Critique & Related Works

### Expert Critique

#### Strengths
1. **Unsolvable Focus**: 기존 벤치마크가 간과한 "불가능한 태스크" 평가 최초 체계화
2. **Multi-level Diagnosis**: 단순 정오답 넘어 **원인 진단** 가능한 설계
3. **에러 유형 분류**: 5가지 에러 타입으로 개선 방향 명확화
4. **Solvable/Unsolvable 비교**: Proprietary vs Open-weight 격차의 실제 원인 분석

#### Limitations
1. **Stateless 설계**: State dependency, 이전 호출 결과 의존성 미반영
2. **API 파라미터 미포함**: Tool 이름만 제공, 실제 API 스키마 복잡성 미반영
3. **Synthetic 데이터**: 실제 사용자 쿼리 기반 아님
4. **영어 중심**: Cross-linguistic 평가 미포함

#### Adoption Status
- [ ] Widely used (specialized benchmark)
- [x] Easy to set up (GitHub 제공)
- [ ] Clear leaderboard (논문 내 표만 제공)
- [ ] Active maintenance (단일 릴리스)

#### 2026 Perspective
- **Still Valid**: Unsolvable task 평가의 중요성, Error 분류 프레임워크
- **Outdated**: 평가 대상 모델 (GPT-4o 이전 버전), 단일 턴 위주
- **Missing**: Claude 3.5, GPT-4o 최신 버전, State dependency 평가

### Related Works

1. **MetaTool** (2023) - Reliability test with 'None' output → ToolBH가 이를 확장하여 원인 분석까지
2. **BFCL** (Berkeley, 2024) - Function calling de facto standard → Solvable 가정, ToolBH와 상호보완
3. **ToolSandbox** (Apple, 2024) - Stateful interactive evaluation → State dependency에서 ToolBH보다 강점
4. **AgentBoard** (2024) - Multi-turn agent evaluation → ToolBH가 hallucination에 특화
5. **HaluEval** (2023) - General hallucination benchmark → ToolBH가 tool-use에 특화

---

## Navigation

- **Source**: [원본 논문](./2406.20015.md)
- **Details**:
  - [TL;DR 상세](./2406.20015-details/tldr.md)
  - [Contributions 상세](./2406.20015-details/contributions.md)
  - [Key Sections 상세](./2406.20015-details/key-sections.md)
  - [Methodology 상세](./2406.20015-details/methodology.md)
