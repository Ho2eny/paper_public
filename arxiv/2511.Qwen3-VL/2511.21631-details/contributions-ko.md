---
parent: ../2511.21631-analysis.md
source: ../2511.21631.md
language: "ko"
---

# 핵심 기여 - 심층 분석

## 기여 1: Interleaved MRoPE

**무엇을**: 시간적 (t), 수평적 (h), 수직적 (w) 구성요소를 임베딩 차원에 걸쳐 균일하게 분산하여 원래의 청크 분할을 대체하는 재설계된 위치 인코딩 방식이다.

**기술 세부사항**: ([Section 2.1](../2511.21631.md#21-interleaved-mrope))

Qwen2-VL의 MRoPE는 임베딩 차원을 세 개의 별도 부분공간으로 분할했다:
```
원래 청크 방식 MRoPE:
[t₁, t₂, ..., tₖ | h₁, h₂, ..., hₖ | w₁, w₂, ..., wₖ]
└─ 시간적 ──┘ └─ 수평적 ─┘ └─ 수직적 ──┘
```

문제: 각 부분공간에 서로 다른 회전 주파수가 할당되어 **불균형한 주파수 스펙트럼**이 생성된다. 연구에 따르면 이것이 장시간 비디오 이해 벤치마크를 저하시켰다.

Qwen3-VL의 Interleaved MRoPE:
```
Interleaved MRoPE:
[t₁, h₁, w₁, t₂, h₂, w₂, t₃, h₃, w₃, ...]
```

이점: 각 공간-시간 축이 **저주파수 대역과 고주파수 대역 모두에 균일하게 표현되어** 스펙트럼 편향을 완화하고 비디오의 장거리 위치 모델링을 크게 개선한다.

**왜 중요한가**:

- **경험적 검증**: 논문은 Interleaved vs Chunked MRoPE에 대한 독립적인 제거 실험을 제공하지 않지만, 장시간 비디오 벤치마크에서 개선을 보여주는 외부 연구 [[28]](#ref-28)를 인용한다
- **단순성**: 추가 파라미터나 계산 오버헤드 없음—RoPE 적용 중 차원 재정렬만
- **일반화**: 이 수정은 MRoPE 또는 유사한 공간-시간 위치 인코딩을 사용하는 모든 VLM에 적용될 가능성이 있다

**출처**: [Section 2.1](../2511.21631.md#21-interleaved-mrope)

---

## 기여 2: DeepStack 통합

**무엇을**: 세 개의 서로 다른 ViT 계층에서 시각적 토큰을 처음 세 개의 LLM 계층에 경량 잔차 연결을 통해 주입하는 다층 비전-언어 정렬 메커니즘이다.

**기술 세부사항**: ([Section 2.2](../2511.21631.md#22-deepstack), [Figure 1](../2511.21631.md#L138))

전통적인 VLM 아키텍처:
```
ViT → [최종 계층 특징] → Merger → LLM Layer 1, 2, 3, ..., N
```
고수준 ViT 특징만 사용되며, 저수준 정보 (엣지, 텍스처)는 손실된다.

Qwen3-VL의 DeepStack:
```
ViT Layer 1 → Merger₁ → (잔차 추가) → LLM Layer 1
ViT Layer 2 → Merger₂ → (잔차 추가) → LLM Layer 2
ViT Layer 3 → Merger₃ → (잔차 추가) → LLM Layer 3
ViT Final → Merger → LLM Layer 1 (표준 경로)
```

**주요 설계 선택**:
1. 3개의 ViT 계층에서 특징 추출 (어느 계층인지 명시되지 않음—초기, 중간, 후기일 가능성)
2. 전용 merger 모듈 (2계층 MLP)이 각 레벨을 LLM 숨겨진 차원으로 투영
3. 해당 LLM 계층에 **잔차 추가** (연결이 아님)—컨텍스트 길이 유지
4. 처음 3개의 LLM 계층만 다층 특징을 받음—후반 계층은 그다지 이득이 없음

**제거 실험 결과**: ([Table 12](../2511.21631.md#51212-deepstack))

| 구성 | MMMU | MMBench | MathVista | DocVQA |
|---------------|------|---------|-----------|--------|
| 베이스라인 (DeepStack 없음) | 61.0 | 88.7 | 75.7 | 93.9 |
| + DeepStack | **63.2** | **89.5** | **76.6** | **95.0** |
| **개선** | **+2.2** | **+0.8** | **+0.9** | **+1.1** |

**왜 중요한가**:

- **작업 전반에 걸친 일관된 개선**: +2.2 MMMU (멀티모달 추론), +1.1 DocVQA (문서 이해), +0.9 MathVista (수학 추론)는 DeepStack이 다양한 능력에 도움이 됨을 보여준다
- **컨텍스트 길이 오버헤드 없음**: 잔차 추가는 시퀀스 길이 증가가 없음을 의미한다 (특징 연결과 달리)
- **경량**: 3개의 추가 merger 모듈과 잔차 연결만—최소한의 파라미터 증가
- **향후 작업을 위한 영감**: 더 많은 ViT 계층에서 추출하거나 더 깊은 LLM 계층에 주입할 수 있다 (현재는 처음 3개만)

**원래 DeepStack과의 비교**: ([Reference](../2511.21631.md#ref-56))

원래 DeepStack 논문 [[56]](#ref-56)은 **다중 스케일 시각 입력** (다른 이미지 해상도)에서 토큰을 쌓았다. Qwen3-VL은 이를 **다층 ViT 출력** (동일한 입력, 다른 처리 단계)에서 특징을 추출하도록 적응시켰다. 이는 핵심 아이디어—다층 추출을 통한 더 풍부한 시각 정보—를 보존하면서 Qwen의 단일 동적 해상도 아키텍처에 맞는 영리한 재해석이다.

**출처**: [Section 2.2](../2511.21631.md#22-deepstack), [Table 12](../2511.21631.md#51212-deepstack)

---

## 기여 3: 명시적 텍스트 타임스탬프

**무엇을**: 절대 시간 위치 인코딩 (Qwen2.5-VL의 time-synchronized MRoPE)을 `<3.0 seconds>`와 같은 명시적 텍스트 토큰으로 대체하여 비디오 프레임 그룹을 표시한다.

**기술 세부사항**: ([Section 2.3](../2511.21631.md#23-video-timestamp))

**Qwen2.5-VL 접근** (T-RoPE):
- 시간적 위치 ID를 절대 시간에 직접 연결 (예: 180초의 프레임은 위치 ID 180을 받음)
- 문제 1: 장시간 비디오에 대해 과도하게 크고 희소한 위치 ID (예: 2시간 비디오 = 위치 ID 최대 7200)
- 문제 2: 학습 중 균일하게 분산된 프레임 속도 샘플링 필요 (비용이 많이 드는 데이터 구축)

**Qwen3-VL 접근** (명시적 텍스트 토큰):
- 각 비디오 시간 패치에 형식화된 텍스트 문자열 접두사: `<3.0 seconds>` 또는 `<0h:0m:3s>`
- 학습 중 타임스탬프는 **초** (예: `<42.5 seconds>`)와 **HMS 형식** (예: `<0h:0m:42s>`) 모두로 생성되어 다양한 시간 코드 해석 보장

**트레이드오프**:

| 측면 | T-RoPE (Qwen2.5-VL) | 텍스트 타임스탬프 (Qwen3-VL) |
|--------|---------------------|----------------------------|
| 위치 ID 범위 | 장시간 비디오에 대해 0-7200+ | 항상 작음 (표준 텍스트 토큰 ID) |
| 학습 데이터 요구사항 | 균일한 fps 샘플링 필요 | 유연한 fps, 텍스트만 추가하면 됨 |
| 컨텍스트 길이 오버헤드 | 0 (위치 인코딩) | 프레임 그룹당 +1 토큰 (~10-20 토큰/비디오) |
| 해석 가능성 | 불투명한 위치 ID | 사람이 읽을 수 있는 타임스탬프 |
| 시간적 그라운딩 | 위치를 통한 암시적 | 텍스트 매칭을 통한 명시적 |

**왜 중요한가**:

- **단순성**: 텍스트 토큰은 특별한 처리가 필요 없음—LLM은 이미 사전학습을 통해 시간적 언어를 이해한다
- **유연성**: 아키텍처 변경 없이 여러 형식 (초, HMS, "3분 시점에서"까지)으로 타임스탬프 표현 가능
- **시간적 그라운딩 작업**: 비디오 그라운딩 및 밀집 캡셔닝은 명시적 타임스탬프의 이점을 받음—모델이 응답에서 `<42.5 seconds>`를 직접 참조 가능

**적당한 컨텍스트 길이 증가**: 논문은 이것이 토큰을 추가하지만 이점 (더 효과적인 시간 인식)이 비용을 능가한다고 주장한다. 100개 프레임이 10개의 시간 패치로 그룹화된 일반적인 비디오의 경우 10-20개 토큰을 추가한다—시각적 토큰 (수백~수천)에 비해 무시할 만하다.

**출처**: [Section 2.3](../2511.21631.md#23-video-timestamp)

---

## 기여 4: 제곱근 손실 정규화

**무엇을**: 샘플당 손실에서 제곱근 정규화된 토큰당 손실로 전환하여 각 샘플의 기여를 $n_{tokens}$ 대신 $\sqrt{n_{tokens}}$로 가중치를 부여한다.

**기술 세부사항**: ([Section 3.1](../2511.21631.md#31-training-recipe))

**문제**: 표준 토큰당 손실:
```
Loss_sample = Σ loss_token_i / n_tokens
Total_Loss = Σ Loss_sample × n_tokens  (그래디언트 스케일링용)
```
이미지는 수백 개의 토큰을 생성하고 텍스트는 수십 개를 생성한다. 긴 멀티모달 샘플이 그래디언트를 지배하여 텍스트 전용 성능을 저하시킨다.

**해결책**: 제곱근 정규화:
```
Total_Loss = Σ Loss_sample × √n_tokens
```
이는 동적 범위를 압축한다—100개 토큰이 있는 샘플은 이제 √100 = 10× 가중치를 기여한다 (100×가 아님), 10개 토큰 샘플은 √10 = 3.16× 가중치를 기여한다.

**경험적 결과**: ([Abstract](../2511.21631.md#abstract))

> "현저히 더 강력한 순수 텍스트 이해, 일부 경우 **비교 가능한 텍스트 전용 백본을 능가한다**"

이것은 주목할 만하다—대부분의 VLM은 멀티모달 학습 후 언어 벤치마크에서 성능이 저하된다. Qwen3-VL-235B-A22B는 일부 작업에서 Qwen3-235B-A22B (텍스트 전용)를 실제로 능가하며, 제곱근 정규화가 LLM 능력을 성공적으로 보존함을 보여준다.

**왜 중요한가**:

- **일반화 가능한 기법**: 이는 가변 길이 시퀀스를 혼합하는 모든 멀티모달 모델에 적용될 가능성이 있다 (VLM뿐만 아니라—오디오-텍스트, 모든 멀티모달 LLM)
- **하이퍼파라미터 튜닝 불필요**: 데이터 혼합 비율 (광범위한 탐색 필요)과 달리, 제곱근 정규화는 고정 공식이다
- **원칙적 접근**: 정보 이론에서 나옴—원시 토큰 수가 아닌 정보 내용을 기반으로 기여도 균형 조정

**대안적 접근**:

| 방법 | 트레이드오프 |
|--------|-----------|
| 데이터 혼합 비율 | 광범위한 탐색 필요; 데이터 분포 변경에 취약 |
| 샘플 레벨 가중치 | 작업/모달리티당 가중치를 수동으로 설계해야 함 |
| 제곱근 정규화 | 간단, 튜닝 불필요, 원칙적—하지만 여전히 휴리스틱 (왜 제곱근인가?) |

**미해결 질문**: 논문은 제곱근 정규화 대 표준 토큰당 손실을 독립적으로 분리하는 제거 실험을 제공하지 않는다. 향후 연구는 다음을 탐구할 수 있다:
- 제곱근이 최적인가, 아니면 세제곱근 / 로그 정규화가 더 나을까?
- 이것이 배치 크기, 학습률 스케줄과 상호작용하는가?

**출처**: [Section 3.1](../2511.21631.md#31-training-recipe), [Abstract](../2511.21631.md#abstract)

---

## 기여 5: 분기형 사후학습 (Thinking vs Non-Thinking)

**무엇을**: 동일한 사전학습된 베이스에서 두 가지 모델 변형을 학습한다—"non-thinking" (표준 형식)과 "thinking" (명시적 사고 과정 (CoT) 형식)—전용 long-CoT cold-start 데이터와 다단계 RL을 사용한다.

**기술 세부사항**: ([Section 4](../2511.21631.md#4-post-training))

**동기**: 다른 응용 프로그램은 다른 지연-품질 요구사항을 가진다:
- **Non-Thinking**: 간단한 인식 쿼리에 대한 빠른 응답 (예: "이 이미지에 무엇이 있습니까?")
- **Thinking**: 복잡한 작업에 대한 명시적 추론 추적 (예: "이 수학 문제를 단계별로 풀어라")

**학습 파이프라인**:

1. **지도 파인튜닝 (SFT)** ([Section 4.2](../2511.21631.md#42-cold-start-data)):
   - Non-Thinking: 표준 질문-답변 형식
   - Thinking: 명시적 추론 단계를 가진 Long-CoT 형식

2. **Strong-to-Weak 증류** ([Section 4.3](../2511.21631.md#43-strong-to-weak-distillation)):
   - Off-policy 증류 (교사가 응답 생성)
   - On-policy 증류 (학생이 생성, 교사가 KL 발산을 통해 로짓 제공)

3. **강화학습 (RL)** ([Section 4.4](../2511.21631.md#44-reinforcement-learning)):
   - **Reasoning RL**: 결정론적 검증이 있는 수학, 코딩, 논리, 그라운딩
   - **General RL**: 하이브리드 규칙 기반 + 모델 기반 보상이 있는 VQA, 캡셔닝, OCR

**Long-CoT Cold-Start 데이터 큐레이션** ([Section 4.2.2](../2511.21631.md#4-22-long-cot-cold-start-data)):

주요 필터링 단계:
1. **난이도 큐레이션**: 베이스라인 모델의 통과율이 낮거나 더 긴 응답을 생성한 인스턴스 유지
2. **멀티모달 필요성 필터링**: 비전-언어 수학 문제의 경우 Qwen3-30B-nothink가 **시각적 입력 없이** 해결할 수 있는 샘플 폐기—진정한 멀티모달 이해가 필요함을 보장
3. **응답 품질 관리**: 잘못된 최종 답변, 과도한 반복, 부적절한 언어 혼합 제거

**성능 개선**: ([Table 2](../2511.21631.md#L437))

| 벤치마크 | Non-Thinking (Instruct) | Thinking | 개선 |
|-----------|------------------------|----------|------|
| MMMU | 78.7 | 80.6 | +1.9 |
| MathVista | 84.9 | 85.8 | +0.9 |
| MathVision | 66.5 | 74.6 | +8.1 |
| We-Math | 72.5 | 74.8 | +2.3 |
| LogicVista | 65.8 | 72.2 | +6.4 |

**왜 중요한가**:

- **실전 유연성**: 배포 시 쿼리를 동적으로 라우팅 가능—간단한 인식 → non-thinking (낮은 지연), 복잡한 추론 → thinking (높은 품질)
- **추론에 대한 일관된 개선**: Thinking 모드는 STEM 및 논리 벤치마크에서 1-8점 개선을 제공하며, 가장 어려운 작업에서 가장 큰 개선 (MathVision: +8.1)
- **지연-품질 트레이드오프**: thinking 모드가 비용을 추가하지만 (더 긴 응답, 더 많은 컴퓨팅), 사용자는 응용 프로그램 요구사항에 따라 선택 가능

**미해결 질문**:

1. **지연 오버헤드 미정량화**: thinking 모드가 얼마나 느린가? 논문은 평균 응답 길이 증가나 추론 시간 배수를 보고하지 않음
2. **Thinking이 해를 끼칠 때**: 일부 벤치마크 (RealWorldQA: 79.0 instruct vs 78.0 thinking)는 non-thinking이 더 나은 성능을 보임—명시적 추론이 인식에 해를 끼치는 경우는?
3. **최적 CoT 길이**: 더 긴 추론 추적에 대한 수확 체감 지점이 있는가?

**OpenAI o1과의 비교**: OpenAI의 o1 모델은 숨겨진 추론 추적 (사용자에게 표시되지 않음)을 사용하는 반면, Qwen3-VL은 전체 CoT를 노출한다. 트레이드오프:
- **Qwen3-VL 접근**: 투명한 추론, 디버깅 용이, 사용자가 논리를 따를 수 있음
- **o1 접근**: 잠재적으로 더 효율적 (사람이 읽을 수 있는 CoT를 생성할 필요 없음), 실패 해석이 더 어려움

**출처**: [Section 4](../2511.21631.md#4-post-training), [Section 4.2.2](../2511.21631.md#422-long-cot-cold-start-data), [Table 2](../2511.21631.md#L437)

---

## 요약 표

| 기여 | 유형 | 영향 | 비용 |
|--------------|------|--------|------|
| Interleaved MRoPE | 아키텍처 | 장시간 비디오 이해 | 0 (차원 재정렬) |
| DeepStack | 아키텍처 | +2.2 MMMU | 낮음 (3개 merger + 잔차) |
| 텍스트 타임스탬프 | 아키텍처 | 시간적 그라운딩 | 낮음 (+10-20 토큰/비디오) |
| 제곱근 손실 | 학습 | 텍스트 능력 보존 | 0 (손실 공식 변경) |
| 분기형 사후학습 | 학습 | 추론에서 +1-8점 | 높음 (2개 모델 변형) |

**전체 평가**: 각 기여는 구체적이고 경험적으로 검증된 한계를 해결한다. 조합은 합보다 크다—아키텍처 개선은 더 긴 컨텍스트를 가능하게 하고, 학습 혁신은 모달리티 전반에 걸쳐 능력을 보존하며, 분기형 사후학습은 실전 유연성을 제공한다. 이것은 단순한 알고리즘적 참신함이 아닌 최고 수준의 시스템 엔지니어링이다.
