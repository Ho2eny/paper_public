---
title: "Agent-as-a-Judge"
arxiv_id: "2601.05111"
authors:
  - "Runyang You"
  - "Hongru Cai"
  - "Caiqi Zhang"
  - "Qiancheng Xu"
  - "Meng Liu"
  - "Tiezheng Yu"
  - "Yongqi Li"
  - "Wenjie Li"
published: "2026-01-08"
updated: "2026-01-08"
categories:
  - "cs.CL"
  - "cs.AI"
url: "https://arxiv.org/abs/2601.05111"
pdf: "https://arxiv.org/pdf/2601.05111.pdf"
converted_date: "2026-01-21"
---

# Agent-as-a-Judge: A Survey

## Authors

Runyang You[<sup>*</sup>](#fn-star)[<sup>1</sup>](#aff-1), Hongru Cai[<sup>*</sup>](#fn-star)[<sup>1</sup>](#aff-1), Caiqi Zhang[<sup>2</sup>](#aff-2), Qiancheng Xu[<sup>1</sup>](#aff-1), Meng Liu[<sup>3</sup>](#aff-3), Tiezheng Yu[<sup>4</sup>](#aff-4), Yongqi Li[<sup>†</sup>](#fn-dagger)[<sup>1</sup>](#aff-1), Wenjie Li[<sup>1</sup>](#aff-1)

### Affiliations

<a id="aff-1"></a>**[1]** The Hong Kong Polytechnic University
<a id="aff-2"></a>**[2]** University of Cambridge
<a id="aff-3"></a>**[3]** Shandong Jianzhu University
<a id="aff-4"></a>**[4]** Huawei Technologies

### Notes

<a id="fn-star"></a>**[*]** Equal contribution
<a id="fn-dagger"></a>**[†]** Corresponding author

**Contact:** runyang.y@outlook.com, henry.hongrucai@gmail.com, liyongqi0@gmail.com, cswjli@comp.polyu.edu.hk

**Project Page:** [https://github.com/ModalityDance/Awesome-Agent-as-a-Judge](https://github.com/ModalityDance/Awesome-Agent-as-a-Judge)

## Abstract

*LLM-as-a-Judge* has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of *LLM-as-a-Judge* has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to *Agent-as-a-Judge*, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.

## 1 Introduction

The rapid advancement of Large Language Models (LLMs) has revolutionized the field of AI evaluation, giving rise to the *LLM-as-a-Judge* paradigm [[1]](#ref-1). While traditional metrics fail to capture semantic nuance and human judgment remains unscalable, this new approach leverages LLMs' advanced understanding and decision-making abilities to deliver near-human quality assessments across diverse domains [[2]](#ref-2). Moreover, serving as a scalable proxy for human preference, LLM judges can provide reward signals for reinforcement learning [[3]](#ref-3) and enable the automated curation of massive synthetic datasets [[4]](#ref-4). As such, LLM judgment has established itself as a cornerstone of AI evaluation and optimization pipelines, where the precision of the judge critically determines the success of downstream applications [[5]](#ref-5).

However, as generative AI applications evolve from simple text responses to complex, multi-step tasks across specialized domains, the reliability of *LLM-as-a-Judge* has become inevitably constrained [[2]](#ref-2), [[6]](#ref-6). First, single-pass evaluators are prone to inherent parametric biases-such as favoring verbosity or their own output patterns-which compromise their neutrality when assessing high-complexity responses that deviate from their training distribution [[7]](#ref-7). Second, naive LLM judges are passive observers, unable to react to real-world observations; they assess answers based on linguistic patterns without verification, leading to hallucinated evaluations in specialized domains [[8]](#ref-8). Furthermore, in evaluation tasks that require multifaceted assessment rubrics, traditional LLM judges experience cognitive overload when attempting to evaluate all dimensions comprehensively within a single inference step, which results in coarse-grained scores that fail to reflect specific nuances [[9]](#ref-9).

These limitations have catalyzed the transition from *LLM-as-a-Judge* to *Agent-as-a-Judge*. As shown in Figure 1, agentic judges

![Figure 1: Comparison between LLM-as-a-Judge (1a) and Agent-as-a-Judge (1b). The former performs direct single-pass evaluation, while the latter leverages planning, memory, and tool-augmented capabilities for enhanced evaluation.](resources/figure_2.png)

*Figure 1: Comparison between LLM-as-a-Judge (1a) and Agent-as-a-Judge (1b). The former performs direct single-pass evaluation, while the latter leverages planning, memory, and tool-augmented capabilities for enhanced evaluation.*

Agentic judges proactively engage in evaluation through multiple capabilities: they decompose complex objectives into subtasks, mitigate biases through multi-agent collaboration [[10]](#ref-10), ground assessments via tool-augmented evidence collection and correctness verification [[8]](#ref-8), and enable finegrained assessment by persisting intermediate states, autonomously planning the evaluation across reasoning steps [[11]](#ref-11), [[12]](#ref-12). This paradigm shift enables more robust, verifiable, and nuanced assessments that effectively address the multifaceted nature of sophisticated AI-generated evaluands.

Despite the above potentials and rapid proliferation of agentic evaluation systems, the field lacks a survey to summarize and navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey for Agent-as-a-Judge through the following contributions:

- We identify and characterize the shift from *LLM-as-a-Judge* to Agent-as-a-Judge and summarize the agentic judges' development trend into three progressive stages (Section 2).
- We organize core methodologies into five key parts according to agent's abilities (Section 3) and survey their applications across general and professional domains (Section 4).
- We analyze frontier challenges and identify promising research directions, providing a strategic roadmap for the next generation of robust and verifiable AI judgment.

## 2 Evolution: From LLM-as-a-Judge to Agent-as-a-Judge

This section traces the evolution of automated evaluation from LLM-as-a-Judge to Agent-as-a-Judge paradigms. We begin by reviewing the foundational LLM-as-a-Judge and its limitations. We then examine the shift toward Agent-as-a-Judge , analyzing key dimensions that characterize the agentic approach. Finally, we summarize Agent-as-a-Judge 's development trend into three progressive stages with distinct levels of autonomy and adaptability.

## 2.1 LLM-as-a-Judge

LLM-as-a-Judge paradigm emerged to overcome the scalability limits of human evaluation and the semantic insensitivity of traditional metrics. Zheng et al., [[1]](#ref-1) formalized this approach by introducing benchmarks like MT-Bench to assess model alignment. Building on this, G-Eval [[59]](#ref-59) leveraged chainof-thought prompting for better alignment in natural language generation (NLG), while Prometheus [[60]](#ref-60) induced fine-grained evaluation in open-source models via specialized tuning. To mitigate systematic issues like position and verbosity bias [[7]](#ref-7), JudgeLM [[61]](#ref-61) utilized fine-tuning to develop more robust evaluators.

## 2.2 From LLM-as-a-Judge to Agent-as-a-Judge

As evaluands evolve from simple text responses to complex, multi-step tasks across specialized domains, traditional LLM-as-a-Judge has become increasingly inadequate, focusing on final outputs while failing to verify intermediate actions or satisfy the rigorous standards of professional fields [[2]](#ref-2), [[6]](#ref-6). To bridge this gap, the paradigm is shifting toward Agent-as-a-Judge that employs decentralized deliberation, executable verification, and fine-grained assessment to mitigate these limitations.

**Evolving Robustness: From Monolithic to Decentralized.** To mitigate the inherent parametric biases of monolithic LLM judges-such as the tendency to favor verbosity or their own output patterns-*Agent-as-a-Judge* paradigms

![Figure 2: A taxonomy of Agent-as-a-Judge organizing Methodologies and Applications. Background gradients illustrate the coverage of developmental stages, from Procedural to Reactive and then to Self-Evolving.](resources/figure_3.png)

*Figure 2: A taxonomy of Agent-as-a-Judge organizing Methodologies (Section 3) and Applications (Section 4). Background gradients illustrate the coverage of developmental stages, from* Procedural *to* Reactive *and then to* Self-Evolving.

These paradigms employ specialized, decentralized agents that collaborate through autonomous decision-making [[10]](#ref-10), [[13]](#ref-13). Crucially, this decentralized architecture facilitates the injection of expert prior knowledge: by decomposing complex evaluation goals into sub-tasks or structuring specific interaction workflows, we can enforce domain-specific constraints that a generalist model typically overlooks [[16]](#ref-16), [[24]](#ref-24). Furthermore, multi-agent deliberation ensures collective robustness; distinct roles can isolate specific information points to neutralize bias, while debate and self-reflection allow agents to audit their own cognitive shortcuts, ensuring the final judgment transcends the heuristics of any single model [[62]](#ref-62), [[7]](#ref-7).

**Evolving Verification: From Intuition to Execution.** Static LLM judges are fundamentally passive observers, unable to react to real-world feedback. They assess answers based on linguistic plausibility - how correct a response looks - without verification or evidence collection, leading to "hallucinated correctness" in complex tasks [[8]](#ref-8). Agent-as-a-Judge bridges this reality gap by replacing intuition with ex- ecution. By interacting with external environments, agentic judges can query system states to verify side effects (e.g., file operations) [[48]](#ref-48), [[51]](#ref-51), use code interpreters or theorem provers to validate logical consistency [[37]](#ref-37), and employ search tools to ground factual claims in real-time documentation [[38]](#ref-38), [[8]](#ref-8). This shifts the evaluative anchor from internal model knowledge to objective verification.

**Evolving Granularity: From Global to Fine-grained.** Many evaluation tasks inherently require multifaceted assessment rubrics, yet traditional LLM judges face a cognitive overload to evaluate these dimensions comprehensively within a single inference step, results in coarse-grained scores that fail to reflect specific nuances [[9]](#ref-9). Agent-as-a-Judge addresses this by transforming evaluation from a single-pass inference into autonomous, hierarchical reasoning [[9]](#ref-9). Instead of a monolithic assessment, an agentic judge can dynamically select or create task-specific rubrics, autonomously planning the evaluation to examine each component of the evaluand independently [[44]](#ref-44), utilizing memory to track historical reasoning states and synthesize fragmented evidence into a coherent verdict. Consequently, these agents can pinpoint

specific flaws that would otherwise be obscured in a global score, providing fine-grained feedback on each aspect [[45]](#ref-45).

## 2.3 Agent-as-a-Judge

Agent-as-a-Judge represents a rapidly expanding field where the term "agent" is often applied loosely, spanning a heterogeneous range from procedural agentic workflows to autonomous self-evolvers [[10]](#ref-10), [[45]](#ref-45), [[12]](#ref-12). To provide a clear roadmap through this complexity, we summarize the ongoing development of agency as follows.

*Procedural* Agent-as-a-Judge decouples monolithic inference into agentic predefined workflows [[57]](#ref-57), [[24]](#ref-24) or engages in structured discussions among fixed sub-agents [[10]](#ref-10), [[56]](#ref-56). These systems enable complex judgments through coordinated multi-agent interactions, yet remain constrained by predetermined decision rules that cannot adapt to novel evaluation scenarios.

*Reactive* Agent-as-a-Judge enables adaptive decision-making by routing execution paths [[28]](#ref-28), [[45]](#ref-45) and invoking external tools [[8]](#ref-8) or sub-agents [[13]](#ref-13) based on intermediate feedback. However, such reactivity remains confined to conditional routing within fixed decision spaces, lacking autonomy to refine underlying rubrics.

*Self-Evolving* Agent-as-a-Judge represents the cutting edge of the field, characterized by high autonomy and the ability to refine internal components during operation-synthesizing evaluation rubrics on-the-fly [[53]](#ref-53) and updating memory with lessons learned. This paradigm opens new frontiers for adaptive evaluation systems, though challenges remain in ensuring stability during self-modification [[63]](#ref-63).

## 3 Methodologies

This section categorizes *Agent-as-a-Judge* methodologies into five dimensions: multi-agent collaboration, planning, tool integration, memory and personalization, and optimization paradigms. As shown in Figure 2, implementation sophistication reveals the evolutionary stages: foundational methodologies (collaboration, tool integration, optimization) evolve across all stages, while others (planning, memory) emerge more prominently in advanced paradigms.

![Figure 3: Multi-agent collaboration paradigms showing (a) Collective Consensus and (b) Task Decomposition.](resources/figure_4.png)

*Figure 3: Multi-agent collaboration paradigms: (a) Collective Consensus and (b) Task Decomposition.*

The following subsections examine how each methodology manifests across these stages.

## 3.1 Multi-Agent Collaboration

Multi-agent collaboration leverages collective reasoning to mitigate single-LLM biases in *Agent-as-a-Judge* systems. Early systems followed *Procedural* paradigms with fixed protocols, while recent work evolves toward *Reactive* approaches that adapt agent selection based on feedback. We categorize these into two topologies:

**Collective Consensus.** Horizontal debate mechanisms leverage agents representing diverse perspectives to counteract the inherent biases of single-LLM evaluators, illustrated in Figure 3. Early approaches exemplified the *Procedural* stage: ChatEval [[10]](#ref-10) pioneered this with a courtroom-inspired discussion mechanism where agents debate as equals following predefined protocols. This paradigm was later extended to machine translation in M-MAD [[56]](#ref-56), while subsequent research [[64]](#ref-64) introduced explicit stances and "judge" roles to prevent agents from blindly conforming to the majority. Recent methods have become more *Self-Evolving*: approaches like Multi-agent-as-judge [[13]](#ref-13) have moved beyond static ensembles by creating domain-specific experts based on intermediate feedback.

**Task Decomposition.** Task Decomposition employs a "Divide and Conquer" strategy, delegating distinct subtasks to specialized agents for systematic evaluation, illustrated in Figure 3. Early frameworks followed *Procedural* designs: sequential approaches like CAFES [[57]](#ref-57) and GEMA-Score [[24]](#ref-24), [[58]](#ref-58) structure evaluation into predefined stages (e.g., Evidence Gathering, Reasoning, Scoring), while SAGEval [[44]](#ref-44) introduces supervision via a "Judge the Judge" metaevaluator that reviews previous agents' decisions,

with hierarchical approaches like HiMATE [[9]](#ref-9) organizing agents into tree structures for varying error granularities. More recent work has shifted toward *Reactive* paradigms: AGENT-X [[45]](#ref-45) employs adaptive router agent that dynamically selects the most relevant base agents based on intermediate analysis results.

**Takeaway** Multi-agent evaluation frameworks adopt two main topologies: Collective Consensus and **Task Decomposition.** Recent advances have evolved toward more autonomous systems that can select or generate subagents.

## 3.2 Planning

Planning serves as a core capability in the *Agent-as-a-Judge* paradigm, enabling the decomposition of highlevel evaluation objectives into executable sub-tasks and the dynamic adaptation of assessment trajectories based on intermediate analysis. This section examines planning capabilities from two perspectives:

**Workflow Orchestration.** Workflow orchestration in Agent-as-a-Judge systems spans from static frameworks to dynamic agency, primarily characterizing *Procedural* and *Reactive* stages of agentic evaluation. Approaches like MATEval [[52]](#ref-52) rely on static decomposition, breaking tasks into fixed sequences of sub-dimensions. While this ensures systematic assessment through predefined control flows, it limits adaptability in complex scenarios. Conversely, Evaluation Agent [[28]](#ref-28) introduces dynamic multi-round planning, where agents adjust strategies based on intermediate feedback. This system further optimizes efficiency through autonomous termination, allowing the agent to self-monitor information gain and proactively halt execution once sufficient evidence is gathered.

**Rubric Discovery.** Unlike general agents focused on task completion, Judge Agents have the distinct capability to autonomously formulate and refine rubrics, representing a hallmark of the *Self-Evolving* stage, where agents can refine their internal evaluation components. EvalAgents [[53]](#ref-53) exemplifies this by employing a Query Generator that plans web searches to discover implicit rubrics, while AGENT-X [[45]](#ref-45) uses an Adaptive Router to infer domain context and plan be- spoke detection guidelines. ARJudge [[54]](#ref-54) adaptively formulates rubrics by iteratively generating contextsensitive questions, and OnlineRubrics [[55]](#ref-55) integrates planning into reinforcement learning, evolving rubrics alongside policy optimization to detect reward hacking.

**Takeaway** Serving as the strategic engine, planning shifts evaluation from rigid flows to adaptive exploration, enabling agents to optimize how they evaluate (workflow orchestration) and what they evaluate (rubric discovery).

## 3.3 Tool Integration

Tool integration is a defining capability of Agent-asa-Judge frameworks, enabling judges to ground evaluation in external evidence and explicit checks. As shown in Table 1, existing approaches can be grouped into evidence collection and correctness verification based on the purpose of tool use.

**Evidence Collection.** A common use of tools in Agent-as-a-Judge frameworks is to collect additional evidence that supports evaluations. Such evidence includes intermediate artifacts, execution results, and perceptual signals that cannot be reliably obtained through text-based reasoning. In code-related tasks, Agent-as-a-Judge [[48]](#ref-48) and CodeVisionary [[51]](#ref-51) allow judges to inspect execution artifacts or run automated checks to expose execution feedback for evaluation. Similar methods are adopted in multimodal settings. Evaluation Agent [[28]](#ref-28) enables judges to invoke external visual models to obtain visual quality or alignment signals, while ARM-Thinker [[12]](#ref-12) gathers fine-grained visual and contextual evidence through document access and localized visual operations. Overall, these works integrate tools to surface observable and task-relevant evidence, expanding the judge's access to execution-level, perceptual, and contextual information, and supporting more reliable evaluation.

**Correctness Verification.** Another line of work employs tools to verify whether the evaluand's outputs or intermediate reasoning steps satisfy explicit correctness constraints, such as logical validity, mathematical soundness, or factual consistency. In these frameworks, the judge agent identifies which claims

Table 1: Tool integration in representative Agent-as-a-Judge methods, grouped by primary tool usage purpose.

| Tool Purpose             | Method                                                                          | Evaluation Task                                                         | Tool Type                                                                                                                                                                                                  |
|--------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Evidence collection      | Agent-as-a-Judge [[48]](#ref-48) CodeVisionary [[51]](#ref-51) Evaluation Agent [[28]](#ref-28) ARM-Thinker [[12]](#ref-12) | Code generation Code generation Visual generation Multimodal generation | Graph, locate, read, search, retrieve Code execution, static linter, unit tests, screenshot, web browsing Visual generative models Instruction following checks, crop/zoom tools, document retrieval tools |
| Correctness verification | HERMES [[37]](#ref-37) VerifiAgent [[38]](#ref-38) Agentic RM [[8]](#ref-8)                                     | Math reasoning Factual & Math reasoning Factual & Math reasoning        | Translator, theorem prover Search engine, Python interpreter, theorem prover Search engine, Python interpreter                                                                                             |

or steps require verification and invokes appropriate tools to check them. The resulting verification signals are then interpreted by the agent in context to inform the final evaluation. HERMES [[37]](#ref-37) verifies mathematical reasoning through formal theorem proving, while VerifiAgent [[38]](#ref-38) invokes programmatic and symbolic checkers to validate factual and computational claims. Agentic Reward Modeling [[8]](#ref-8) further integrates correctness verification by combining factchecking tools and programmatic validators to produce structured correctness signals that inform the final evaluation.

**Takeaway** Tool integration in Agent-as-a-Judge grounds evaluation in observable and verifiable signals by allowing judges to actively gather evidence and check correctness through external tools.

## 3.4 Memory and Personalization

Memory enables Agent-as-a-Judge frameworks to retain information across evaluation steps, supporting multi-step reasoning, consistent judgment, and reuse of prior results. We categorize prior work by the role of memory, including intermediate state tracking and personalized context preservation.

**Intermediate State.** In multi-step evaluation settings, Agent-as-a-Judge frameworks use memory to retain intermediate evaluation states generated during assessment, providing the necessary context for conditional routing and adaptive decision-making based on intermediate feedback-a fundamental mechanism for *Reactive* Agent-as-a-Judge. HERMES [[37]](#ref-37) retains intermediate proof states when combining reasoning with formal theorem proving, enabling consistent verification across long reasoning chains. ARMThinker [[12]](#ref-12) preserves intermediate evidence such as visual reasoning outputs and tool interaction results, which are later reused to ground evaluation. Agent-as-a-Judge [[48]](#ref-48) records execution traces and step-level feedback, enabling evaluation beyond final outputs to account for intermediate behavior. Collectively , these methods use memory to retain intermediate states that support cumulative, step-aware evaluation.

**Personalized Context.** Agent-as-a-Judge frameworks often incorporate memory to retain userrelated information that conditions evaluation across interactions. Such memory captures user preferences, evaluation standards, or prior feedback, allowing judgments to remain consistent over time. PersRMR1 [[49]](#ref-49) and FSPO [[50]](#ref-50) store preference data derived from historical interactions, including preference labels or few-shot examples, which are reused to condition subsequent evaluations for the same user. More advanced approaches abstract historical preference signals into persistent user personas or long-term profiles. RLPA [[11]](#ref-11) and SynthesizeMe [[46]](#ref-46) exemplify this by constructing and maintaining user personas that are stored and reused to guide evaluation. Such long-term user profiling often serves to support *Self-Evolving* Agent-as-a-Judge, enabling continuous optimization based on evolving preferences. Together, these methods use memory to preserve personalized

context that shapes evaluation behavior and ensures consistency across interactions.

**Takeaway** Memory enables Agent-as-a-Judge to preserve intermediate states and personalized context, supporting multi-step evaluation, consistent judgment, and efficient reuse of prior information.

## 3.5 Optimization Paradigms

Optimization paradigms define how Agent-as-a-Judge improves evaluation quality by updating model parameters or adapting evaluation behaviors. We organize prior work into two groups: training-time optimization and inference-time optimization.

**Training-Time Optimization.** Training-time optimization improves Agent-as-a-Judge by updating model parameters to align judgment behavior with evaluation objectives. Supervised fine-tuning is commonly used to standardize judge behavior, training models to follow explicit criteria, and produce structured judgments across tasks. For example, SynthesizeMe [[46]](#ref-46) shapes evaluation behavior using persona-guided supervision derived from historical data. Reinforcement learning optimizes judges to perform evaluation actions more effectively, especially in settings that require tool use and multi-step verification. TIR-Judge [[47]](#ref-47) and ARM-Thinker [[12]](#ref-12) train judges to decide when and how to invoke tools, integrate external signals, and verify intermediate results. Together, training-time optimization shapes internal decision processes, enabling more reliable, structured evaluation.

**Inference-Time Optimization.** Inference-time optimization adapts evaluation behavior without updating model parameters by controlling how judgments are produced through prompts, workflows, or agent interactions. Existing approaches can be broadly grouped into two types. 1) The first type follows predefined evaluation procedures, where reasoning steps, verification routines, or prompts are fixed in advance to ensure consistency and efficiency. Evaluation Agent [[28]](#ref-28) and HERMES [[37]](#ref-37) exemplify this setting by adopting structured, step-by-step evaluation pipelines. 2) The second type allows evaluation behavior to adapt during inference, where the evaluation process, participating agents, or applied

![Figure 4: An overview of Agent-as-a-Judge application domains and their fine-grained task categories.](resources/figure_6.png)

*Figure 4: An overview of Agent-as-a-Judge application domains and their fine-grained task categories.*

criteria change based on intermediate results. MultiAgent LLM Judge [[43]](#ref-43) iteratively refines prompts and context through multi-judge coordination, while SAGEval [[44]](#ref-44) introduces a meta-judge to monitor and revise judge behavior. ChatEval [[10]](#ref-10) and AGENTX [[45]](#ref-45) further support adaptive evaluation through agent interaction and dynamic guideline selection. Overall, inference-time optimization enables flexible control over evaluation behavior, ranging from fixed procedures to adaptive, interaction-driven judgment.

**Takeaway** Optimization improves Agent-as-a-Judge by either learning evaluation behavior through parameter updates at training-time or adjusting evaluation strategies at inference time.

## 4 Application

Building on the methods above, this section describes how Agent-as-a-Judge methods are applied in different evaluation tasks. As shown in Figure 4, we organize representative applications into two groups: general domains and professional domains.

## 4.1 General Domains

**Math and Code.** In math and code evaluation, Agent-as-a-Judge systems move beyond single-pass scoring by grounding judgment in verifiable reasoning signals. One line of work augments free-form

reasoning with explicit correctness checks. HERMES [[37]](#ref-37) anchors LLM reasoning to intermediate formal proof steps, reducing drift in long derivations. VerifiAgent [[38]](#ref-38) decouples high-level reasoning assessment from tool-based correctness verification, enabling adaptive checking across reasoning types. CompassVerifier [[39]](#ref-39) and xVerify [[40]](#ref-40) focus on mathematical and logical outputs, addressing equivalence checking under diverse surface forms. Other approaches strengthen judgment by aggregating multiple evaluation signals. Multi-Agent Verification [[41]](#ref-41) distributes evaluation across aspect-specific judges. Agentic Reward Modeling [[8]](#ref-8) integrates preferencebased supervision with verifiable correctness signals to improve reward reliability. Popper [[42]](#ref-42) formulates judgment as controlled falsification, using statistical tests to validate free-form claims.

**Fact-Checking.** In fact-checking, Agent-as-a-Judge reframes evaluation from static label prediction to interactive verification with evidence gathering and justification. FACT-AUDIT [[34]](#ref-34) models fact-checking as an agentic loop with multi-agent collaboration, jointly evaluating verdict accuracy and justification quality. This paradigm is particularly effective when evidence is scarce or inconsistencies are subtle. UrduFactCheck [[35]](#ref-35) improves robustness in low-resource settings through multilingual retrieval and evidence boosting. NarrativeFastScore [[36]](#ref-36) addresses longcontext factual consistency by constructing characterlevel knowledge representations, enabling detection of state and relation errors with actionable feedback.

**Conversation and Interaction.** In conversation and interaction, Agent-as-a-Judge shifts from grading isolated replies to constructing multi-turn exchanges, enabling evaluation under evolving goals, constraints, and user reactions. For task-oriented dialogue, IntellAgent [[30]](#ref-30) uses interactive user simulations to synthesize conversational benchmarks, while Kazi et al. [[65]](#ref-65) introduces frameworks for controllable user goals and automatic measures. For affective and social interaction, ESC-Judge [[31]](#ref-31) constructs emotional-support agents via standardized counseling skills, Sentient Agent [[32]](#ref-32) tracks emotion trajectories over time to reflect higher-order social cognition, and PSYCHE [[33]](#ref-33) builds psychiatric patient profiles for ethical assessment validation. Wu et al.

[[66]](#ref-66) frames evaluation as multi-perspective role play with diverse reviewer personas to cover both objective and subjective dimensions.

**Multimodal and Vision.** In the multimodal and vision domain, Agent-as-a-Judge shifts from static scoring to interactive inspection. For visual generation, CIGEval [[27]](#ref-27) orchestrates specialized tools to probe control adherence and subject consistency, while Evaluation Agent [[28]](#ref-28) runs multi-round checks to provide user-tailored, explainable analyses. For truthfulness evaluation, LRQ-Fact [[29]](#ref-29) generates targeted factchecking questions across image and text to guide evidence retrieval, while ARM-Thinker [[12]](#ref-12) selectively invokes tools like image inspection for finalizing judgments.

## 4.2 Professional Domains

**Medicine.** In high-stakes clinical NLP, *Agent-as-a-Judge* appears in two forms: 1) multi-agent evaluators that decompose clinical quality into specialized roles, and 2) agentic simulators that interactively elicit clinical behaviors. For 1), MAJ-Eval [[13]](#ref-13) constructs multiple evaluator personas to debate and cross-verify responses, while GEMA-Score[[24]](#ref-24) uses agent collaboration to compute granular, toolassisted scores covering disease severity and uncertainty. For 2), Chat-Coach [[25]](#ref-25) pairs autonomous patient and coach agents to critique trainee-doctor dialogues, while AI Hospital [[26]](#ref-26) evaluates LLM 'doctors' in multi-agent simulators, though final scoring often still requires conventional metrics.

**Law.** In the legal domain, Agent-as-a-Judge simulates the adversarial and deliberative nature of jurisprudence through multi-agent interaction. AgentsCourt [[21]](#ref-21) introduces adversarial debate frameworks where agents role-play as prosecutors, defense attorneys, and judges, exposing the evaluating agent to conflicting arguments to improve verdict robustness. SAMVAD [[22]](#ref-22) and AgentsBench [[23]](#ref-23) model judicial consensus by simulating bench deliberation processes, capturing interactions between concurring and dissenting opinions to enhance legal judgment prediction.

**Finance.** In finance, Agent-as-a-Judge addresses two limitations of static benchmarks: 1) capturing the internal research logic of long-form analyst reports, and

2) detecting deployment risks like hallucinations and temporal staleness. For 1), FinResearchBench [[17]](#ref-17) extracts logic trees from reports as intermediate structures for comprehensive assessment, whereas FinDeepResearch [[18]](#ref-18) can synthesize hierarchical rubrics but still relies on predefined workflows. For 2), SAEA [[19]](#ref-19) proposes auditing agent trajectories to mitigate hallucinations and temporal misalignment. From Tasks to Teams [[20]](#ref-20) extends this approach with M-SAEA to trace multi-agent failures, such as crossagent divergence and error propagation.

**Education.** In the educational domain, *Agent-as-a-Judge* systems emulate pedagogical nuance through collaborative, role-specialized workflows. GradeLike-Human [[16]](#ref-16) and AutoSCORE [[14]](#ref-14) decompose grading into staged processes (rubric construction, evidence recognition, cross-review) to improve grounding and consistency. Beyond static scoring, MAJ-Eval [[13]](#ref-13) uses multi-persona debates to align with multi-dimensional human evaluation, while GradeOpt [[15]](#ref-15) introduces agents that diagnose discrepancies and iteratively refine grading guidelines.

## 5 Discussion

This section discusses broader issues that arise when deploying Agent-as-a-Judge systems in practice. We first summarize key challenges that limit scalability, reliability , and real-world adoption, and then outline several future directions that may help address these limitations and further advance agentic evaluation.

## 5.1 Challenges

Agent-as-a-Judge improves evaluation reliability through planning, tool use, memory, and multi-agent collaboration, but these capabilities also introduce new challenges beyond static LLM-as-a-Judge . Key challenges include computational cost, latency, safety, and privacy.

**Computational Cost.** Agent-as-a-Judge introduces a heavier computational burden in both training and inference. 1) Training a judge agent is expensive. Supervised fine-tuning alone is often insufficient to support agentic behaviors such as tool invocation, long-horizon planning, and adaptive decision making. Reinforcement learning provides a natural way to acquire these capabilities, but it significantly increases training cost, especially when the judge operates over long trajectories or complex tool-calling sequences. 2) Inference with Agent-as-a-Judge is also costly . Unlike single-pass judgment, agentic evaluation typically involves multiple reasoning steps, intermediate decisions, and coordination among multiple agents, all of which increase computation per evaluation.

**Latency.** In addition to higher computational cost, Agent-as-a-Judge often suffers from increased inference latency. Agentic evaluation requires sequential reasoning steps, external tool calls, or multi-agent communication, each of which introduces additional delays. This latency can be particularly problematic in real-time or interactive settings, such as online model evaluation, user-facing content moderation, or reinforcement learning loops where rapid feedback is required. As a result, there exists a tension between evaluation reliability and practical deployment constraints, where more thorough agentic judgment may not be feasible under strict latency budgets.

**Safety.** While Agent-as-a-Judge is designed to improve evaluation robustness, it also raises new safety concerns. Tool-augmented judges may access external systems such as search engines, code executors, or databases, which expands the attack surface for prompt injection, tool misuse, or unintended side effects. Multi-agent collaboration can further amplify risks if unsafe behaviors propagate across agents or if adversarial interactions emerge. Moreover, when judge agents are used to provide reward signals for model optimization, systematic biases or errors in agentic judgment may be reinforced and amplified during training, leading to unintended model behaviors.

**Privacy.** Agent-as-a-Judge also introduces privacy challenges, particularly in settings that involve persistent memory or personalized evaluation. To maintain consistency or adapt judgments to specific users or contexts, judge agents may store intermediate states, user information, or historical interaction data. If not carefully designed, such memory mechanisms can increase the risk of sensitive data leakage or unauthorized inference about user attributes. This issue becomes more pronounced in professional domains such as medicine, law, or education, where evaluation

often relies on confidential or personally identifiable information.

## 5.2 Future Directions

**Personalization.** Current Agent-as-a-Judge systems are constrained by static, one-size-fits-all evaluation criteria, failing to align with diverse individual preferences. To bridge this gap, future research should focus on enhancing the autonomy and adaptivity of judge agents. A critical enabler is proactive memory management: rather than passively retrieving history, agents must actively manage the lifecycle of user-specific knowledge-autonomously deciding when to register new preferences, update evolving standards, or prune obsolete feedback. This agentic control transforms memory into a dynamic belief system, allowing the judge to continuously refine its criteria and maintain alignment with the user's specific values and usage contexts.

**Generalization.** Current systems rely on predefined rubrics constructed offline, limiting their ability to generalize across diverse or open-ended tasks. Future judge agents should leverage planning capabilities to dynamically discover and adapt evaluation criteria. 1) Context-Aware Rubric Generation: Agents should synthesize evaluation criteria on-the-fly by analyzing the specific intent and complexity of responses, identifying relevant assessment dimensions not anticipated during design. 2) Adaptive Multi-Granularity Scoring: Rubrics should dynamically scale based on task difficulty-applying high-level holistic criteria for straightforward tasks, while decomposing into fine-grained sub-rubrics for complex workflows.

**Interactivity.** Current systems operate as passive, one-way observers. Future agents should evolve into interactive evaluators that actively engage with both the environment and human stakeholders. 1) Interactive Environmental Feedback: Instead of static test suites, judge agents should dynamically tailor evaluation trajectories-autonomously escalating task complexity or isolating edge cases to rigorously probe the evaluand's failure boundaries. 2) Human-Agent Collaborative Calibration: To address subjective or ambiguity-rich scenarios, agents should leverage human-in-the-loop mechanisms. By proactively consulting experts to verify intent or resolve conflicts, the judge refines its criteria through multi-turn alignment, ensuring higher trust and interpretability.

**Optimization.** Current approaches predominantly rely on inference-time engineering, which is fundamentally bottlenecked by the fixed capabilities of frozen backbones. To transcend these limits, the field must pivot towards **Training-based Optimization.** This paradigm shift entails two key levels: 1) Individual Capability: Utilizing Reinforcement Learning (RL) to internalize complex agentic behaviors-such as sequential planning and adaptive tool use-that are difficult to elicit via prompting alone. 2) Learned Coordination: Extending optimization to multi-agent settings. Rather than ad-hoc inference collaboration, agents should be trained with joint objectives to intrinsically learn effective communication and consensus strategies.

**Concluding Remarks: Towards True Autonomy.** As characterized in Section 2, existing implementations exhibit varying degrees of agency. The future directions discussed above-personalization, generalization, interactivity , and optimization-collectively point towards an evolutionary trajectory towards autonomy. The next generation of judge agents must transcend fixed protocols to become genuinely agentic entities capable of self-directed adaptation, active context curation, and continuous self-refinement, ultimately realizing the full potential of agents that actively perceive, reason, and evolve alongside the models they assess.

## 6 Conclusion

This paper provides the first comprehensive survey of Agent-as-a-Judge . We established a novel taxonomy and demonstrated how agentic capabilities, including multi-agent collaboration, autonomous planning, tool integration, and memory, overcome the limitations of naive LLM judges to deliver more robust, verifiable and nuanced judgments across general and professional domains. While promising, this evolution presents challenges in computational cost, latency, safety, and privacy. Future progress should prioritize personalization, generalization, and optimization, ultimately realizing truly autonomous evaluators that continuously adapt to the evolving AI landscape.

## Limitations

**Early Stage of Paradigm Consensus.** As a pioneering survey exploring the evolution of Agent-asa-Judge , this study faces the challenge that the field has not yet gained complete widespread recognition in academia. Although the transition from LLM-as-aJudge to Agent-as-a-Judge has begun to take shape, there is still a lack of long-term consensus regarding the definition of evaluation agents. Nevertheless, establishing this foundational framework is essential to orienting future research. We are committed to iteratively refining this taxonomy as the paradigm matures and gains broader recognition.

**Inclusion of Early Prompting Methods.** We acknowledge a potential gap between early methodologies and the increasingly rigorous definitions of agents. Many pioneering works in automated evaluation, though named as "agent", rely heavily on prompting engineering, such as fixed role-play, which may not align with the strict criteria for autonomy, dynamic planning, or tool-use held by the current community. Nevertheless, we deliberately include these prompt-based frameworks as they represent the initial shift from monolithic inference toward dynamic decomposition and self-evolving systems. Excluding them would obscure the transition thus compromising a complete understanding of the field's evolution.

## Ethics Statement

This work does not involve the use or creation of datasets or scientific artifacts that would require specific ethical clearance, data privacy considerations, or licensing agreements. We believe this work adheres to the ethical guidelines of the conference and poses no immediate negative social impact.

## References

<a id="ref-1"></a>**[1]** Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. *Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.* In Proceedings of the 37th International Conference on Neural Information Processing Systems (NeurIPS '23), 2023.

<a id="ref-2"></a>**[2]** Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. *From Generation to Judgment: Opportunities and Challenges of LLM-as-a-Judge.* In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2757-2791, 2025.

<a id="ref-3"></a>**[3]** Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. *RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback.* In Proceedings of the 41st International Conference on Machine Learning (ICML'24), 2024.

<a id="ref-4"></a>**[4]** Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. *On LLMs-driven Synthetic Data Generation, Curation, and Evaluation: A Survey.* In Findings of the Association for Computational Linguistics: ACL 2024, pages 11065-11082, 2024. URL [https://aclanthology.org/2024.findings-acl.658/](https://aclanthology.org/2024.findings-acl.658/).

<a id="ref-5"></a>**[5]** Hanyu Lai, Xiao Liu, Junjie Gao, Jiale Cheng, Zehan Qi, Yifan Xu, Shuntian Yao, Dan Zhang, Jinhua Du, Zhenyu Hou, Xin Lv, Minlie Huang, Yuxiao Dong, and Jie Tang. *A Survey of Post-Training Scaling in Large Language Models.* In Proceedings of the 63rd Annual Meeting of the ACL (Volume 1: Long Papers), pages 2771-2791, 2025. URL [https://aclanthology.org/2025.acl-long.140/](https://aclanthology.org/2025.acl-long.140/).

<a id="ref-6"></a>**[6]** Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. *The Rise and Potential of Large Language Model Based Agents: A Survey.* Science China Information Sciences, 68(2):121101, 2025.

<a id="ref-7"></a>**[7]** Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, and Zhifang Sui. *Large Language Models are Not Fair Evaluators.* In Proceedings of the 62nd Annual Meeting of the ACL (Volume 1: Long Papers), pages 9440-9450, 2024. URL [https://aclanthology.org/2024.acl-long.511/](https://aclanthology.org/2024.acl-long.511/).

<a id="ref-8"></a>**[8]** Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, and Juanzi Li. *Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems.* In Proceedings of the 63rd Annual Meeting of the ACL (Volume 1: Long Papers), pages 15934-15949, 2025. URL [https://aclanthology.org/2025.acl-long.775/](https://aclanthology.org/2025.acl-long.775/).

<a id="ref-9"></a>**[9]** Shijie Zhang, Renhao Li, Songsheng Wang, Philipp Koehn, Min Yang, and Derek F. Wong. *HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation.* In Findings of the ACL: EMNLP 2025, pages 11121-11145, 2025. URL [https://aclanthology.org/2025.findings-emnlp.593/](https://aclanthology.org/2025.findings-emnlp.593/).

<a id="ref-10"></a>**[[10]](#ref-10)** Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. *ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate.* In The Twelfth International Conference on Learning Representations (ICLR), 2024. URL [https://openreview.net/forum?id=FQepisCUWu](https://openreview.net/forum?id=FQepisCUWu).

<a id="ref-11"></a>**[[11]](#ref-11)** Weixiang Zhao, Xingyu Sui, Yulin Hu, Jiahe Guo, Haixiao Liu, Biye Li, Yanyan Zhao, Bing Qin, and Ting Liu. *Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment.* In Proceedings of the Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS 2025), 2025.

<a id="ref-12"></a>**[[12]](#ref-12)** Shengyuan Ding, Xinyu Fang, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiangyu Zhao, Haodong Duan, Xiaoyi Dong, Jianze Liang, Bin Wang, Conghui He, Dahua Lin, and Jiaqi Wang. *ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning.* 2025.

<a id="ref-13"></a>**[[13]](#ref-13)** Jiaju Chen, Yuxuan Lu, Xiaojie Wang, Huimin Zeng, Jing Huang, Jiri Gesi, Ying Xu, and Dakuo Wang. *Multi-Agent-as-Judge: Aligning LLM-Agent-based Automated Evaluation with Multi-Dimensional Human Evaluation.* In First Workshop on Multi-Turn Interactions in Large Language Models, 2025. URL [https://openreview.net/forum?id=7AetgL7eVL](https://openreview.net/forum?id=7AetgL7eVL).

<a id="ref-14"></a>**[[14]](#ref-14)** Yun Wang, Zhaojun Ding, Xuansheng Wu, Siyue Sun, Ninghao Liu, and Xiaoming Zhai. *AutoScore: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition.* [arXiv:2509.21910](https://arxiv.org/abs/2509.21910), 2025.

<a id="ref-15"></a>**[[15]](#ref-15)** Yucheng Chu, Hang Li, Kaiqi Yang, Harry Shomer, Hui Liu, Yasemin Copur-Gencturk, and Jiliang Tang. *A LLM-Powered Automatic Grading Framework with Human-Level Guidelines **Optimization.*** [arXiv:2410.02165](https://arxiv.org/abs/2410.02165), 2024.

<a id="ref-16"></a>**[[16]](#ref-16)** Wenjing Xie, Juxin Niu, Chun Jason Xue, and Nan Guan. *Grade Like a Human: Rethinking Automated Assessment with Large Language Models.* [arXiv:2405.19694](https://arxiv.org/abs/2405.19694), 2024.

<a id="ref-17"></a>**[[17]](#ref-17)** Rui Sun, Zuo Bai, Wentao Zhang, Yuxiang Zhang, Li Zhao, Shan Sun, and Zhengwen Qiu. *FinResearchBench: A Logic Tree Based Agent-as-a-Judge Evaluation Framework for Financial Research Agents.* In Proceedings of the 6th ACM International Conference on AI in Finance, pages 656-664, 2025.

<a id="ref-18"></a>**[[18]](#ref-18)** Fengbin Zhu, Xiang Yao Ng, Ziyang Liu, Chang Liu, Xianwei Zeng, Chao Wang, Tianhui Tan, Xuan Yao, Pengyang Shao, Min Xu, et al. *FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis.* [arXiv:2510.13936](https://arxiv.org/abs/2510.13936), 2025.

<a id="ref-19"></a>**[[19]](#ref-19)** Zichen Chen, Jiaao Chen, Jianda Chen, and Misha Sra. *Standard Benchmarks Fail - Auditing LLM Agents in Finance Must Prioritize Risk.* 2025. URL [https://arxiv.org/abs/2502.15865](https://arxiv.org/abs/2502.15865).

<a id="ref-20"></a>**[[20]](#ref-20)** Zichen Chen, Jianda Chen, Jiaao Chen, and Misha Sra. *From Tasks to Teams: A Risk-First Evaluation Framework for Multi-Agent LLM Systems in **Finance.*** In ICML 2025 Workshop on Reliable and Responsible Foundation Models, 2025.

<a id="ref-21"></a>**[[21]](#ref-21)** Zhitao He, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Jiexin Xu, Huaijun Li, Kang Liu, and Jun Zhao. *AgentsCourt: Building Judicial Decision-Making Agents with Court Debate Simulation and Legal Knowledge Augmentation.* In Findings of the ACL: EMNLP 2024, pages 9399-9416, 2024.

<a id="ref-22"></a>**[[22]](#ref-22)** Prathamesh Devadiga, Omkaar Jayadev Shetty, and Pooja Agarwal. *SAMVAD: A Multi-Agent System for Simulating Judicial Deliberation Dynamics in India.* [arXiv:2509.03793](https://arxiv.org/abs/2509.03793), 2025.

<a id="ref-23"></a>**[[23]](#ref-23)** Cong Jiang and Xiaolei Yang. *AgentsBench: A Multi-Agent LLM Simulation Framework for Legal Judgment Prediction.* Systems, 13(8):641, 2025.

<a id="ref-24"></a>**[[24]](#ref-24)** Zhenxuan Zhang, Kinhei Lee, Weihang Deng, Huichi Zhou, Zihao Jin, Jiahao Huang, Zhifan Gao, Dominic C. Marshall, Yingying Fang, and Guang Yang. *GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report Evaluation.* CoRR, abs/2503.05347, 2025. URL [https://doi.org/10.48550/arXiv.2503.05347](https://doi.org/10.48550/arXiv.2503.05347).

<a id="ref-25"></a>**[[25]](#ref-25)** Hengguan Huang, Songtao Wang, Hongfu Liu, Hao Wang, and Ye Wang. *Benchmarking Large Language Models on Communicative Medical Coaching: A Dataset and A Novel System.* In Findings of the ACL: ACL 2024, pages 1624-1637, 2024.

<a id="ref-26"></a>**[[26]](#ref-26)** Zhihao Fan, Lai Wei, Jialong Tang, Wei Chen, Wang Siyuan, Zhongyu Wei, and Fei Huang. *AI Hospital: Benchmarking Large Language Models in a Multi-Agent Medical Interaction Simulator.* In Proceedings of the 31st International Conference on Computational Linguistics, pages 10183-10213, 2025.

<a id="ref-27"></a>**[[27]](#ref-27)** Jifang Wang, Xue Yang, Longyue Wang, Zhenran Xu, Yiyu Wang, Yaowei Wang, Weihua Luo, Kaifu Zhang, Baotian Hu, and Min Zhang. *A Unified Agentic Framework for Evaluating Conditional Image Generation.* In Proceedings of the 63rd Annual Meeting of the ACL (Volume 1: Long Papers), pages 12626-12646, 2025. URL [https://aclanthology.org/2025.acl-long.620/](https://aclanthology.org/2025.acl-long.620/).

<a id="ref-28"></a>**[[28]](#ref-28)** Fan Zhang, Shulin Tian, Ziqi Huang, Yu Qiao, and Ziwei Liu. *Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models.* In Proceedings of the 63rd Annual Meeting of the ACL (Volume 1: Long Papers), pages 7561-7582, 2025. URL [https://aclanthology.org/2025.acl-long.374/](https://aclanthology.org/2025.acl-long.374/).

<a id="ref-29"></a>**[[29]](#ref-29)** Alimohammad Beigi, Bohan Jiang, Dawei Li, Tharindu Kumarage, Zhen Tan, Pouya Shaeri, and Huan Liu. *LRQ-Fact: LLM-Generated Relevant Questions for Multimodal **Fact-Checking.*** [arXiv:2410.04616](https://arxiv.org/abs/2410.04616), 2024.

<a id="ref-30"></a>**[[30]](#ref-30)** Elad Levi and Ilan Kadar. *IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems.* [arXiv:2501.11067](https://arxiv.org/abs/2501.11067), 2025.

<a id="ref-31"></a>**[[31]](#ref-31)** Navid Madani and Rohini Srihari. *ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents.* In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 16059-16076, 2025. URL [https://aclanthology.org/2025.emnlp-main.811/](https://aclanthology.org/2025.emnlp-main.811/).

<a id="ref-32"></a>**[[32]](#ref-32)** Bang Zhang, Ruotian Ma, Qingxuan Jiang, Peisong Wang, Jiaqi Chen, Zheng Xie, Xingyu Chen, Yue Wang, Fanghua Ye, Jian Li, et al. *Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models.* [arXiv:2505.02847](https://arxiv.org/abs/2505.02847), 2025.

<a id="ref-33"></a>**[[33]](#ref-33)** Jingoo Lee, Kyungho Lim, Young-Chul Jung, and Byung-Hoon Kim. *PSYCHE: A Multi-Faceted Patient Simulation Framework for Evaluation of Psychiatric Assessment Conversational Agents.* [arXiv:2501.01594](https://arxiv.org/abs/2501.01594), 2025.

<a id="ref-34"></a>**[[34]](#ref-34)** Hongzhan Lin, Yang Deng, Yuxuan Gu, Wenxuan Zhang, Jing Ma, See-Kiong Ng, and Tat-Seng Chua. *FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models.* In Proceedings of the 63rd Annual Meeting of the ACL (Volume 1: Long Papers), pages 360-381, 2025. URL [https://aclanthology.org/2025.acl-long.17/](https://aclanthology.org/2025.acl-long.17/).

<a id="ref-35"></a>**[[35]](#ref-35)** Sarfraz Ahmad, Hasan Iqbal, Momina Ahsan, Numaan Naeem, Muhammad Ahsan Riaz Khan, Arham Riaz, Muhammad Arslan Manzoor, Yuxia Wang, and Preslav Nakov. *UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking.* In Findings of the ACL: EMNLP 2025, pages 22788-22802, 2025. URL [https://aclanthology.org/2025.findings-emnlp.1240/](https://aclanthology.org/2025.findings-emnlp.1240/).

<a id="ref-36"></a>**[[36]](#ref-36)** Yeonseok Jeong, Minsoo Kim, Seung-won Hwang, and Byung-Hak Kim. *Agent-as-Judge for Factual Summarization of Long Narratives.* In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 23602-23619, 2025. URL [https://aclanthology.org/2025.emnlp-main.1204/](https://aclanthology.org/2025.emnlp-main.1204/).

<a id="ref-37"></a>**[[37]](#ref-37)** Azim Ospanov, Zijin Feng, Jiacheng Sun, Haoli Bai, Xin Shen, and Farzan Farnia. *HERMES: Towards Efficient and Verifiable Mathematical Reasoning in LLMs.* 2025. URL [https://arxiv.org/abs/2511.18760](https://arxiv.org/abs/2511.18760).

<a id="ref-38"></a>**[[38]](#ref-38)** Jiuzhou Han, Wray Buntine, and Ehsan Shareghi. *VerifiAgent: A Unified Verification Agent in Language Model Reasoning.* In Findings of the ACL: EMNLP 2025, 2025.

<a id="ref-39"></a>**[[39]](#ref-39)** Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, and Kai Chen. *CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward.* In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 33454-33482, 2025. URL [https://aclanthology.org/2025.emnlp-main.1698/](https://aclanthology.org/2025.emnlp-main.1698/).

<a id="ref-40"></a>**[[40]](#ref-40)** Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, Feiyu Xiong, Xinchi Li, Minchuan Yang, and Zhiyu Li. *xVerify: Efficient Answer Verifier for Reasoning Model Evaluations.* [arXiv:2504.10481](https://arxiv.org/abs/2504.10481), 2025.

<a id="ref-41"></a>**[[41]](#ref-41)** Shalev Lifshitz, Sheila A McIlraith, and Yilun Du. *Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers.* [arXiv:2502.20379](https://arxiv.org/abs/2502.20379), 2025.

<a id="ref-42"></a>**[[42]](#ref-42)** Kexin Huang, Ying Jin, Ryan Li, Michael Y Li, Emmanuel Candes, and Jure Leskovec. *Automated Hypothesis Validation with Agentic Sequential Falsifications.* In Proceedings of the 42nd International Conference on Machine Learning, 2025.

<a id="ref-43"></a>**[[43]](#ref-43)** Hongliu Cao, Ilias Driouich, Robin Singh, and Eoin Thomas. *Multi-Agent LLM Judge: Automatic Personalized LLM Judge Design for Evaluating Natural Language Generation Applications.* 2025.

<a id="ref-44"></a>**[[44]](#ref-44)** Reshmi Ghosh, Tianyi Yao, Lizzy Chen, Sadid Hasan, Tianwei Chen, Dario Bernal, Huitian Jiao, and HM Hossain. *SAGEval: The Frontiers of Satisfactory Agent Based NLG Evaluation for Reference-Free Open-Ended Text.* [arXiv:2411.16077](https://arxiv.org/abs/2411.16077), 2024.

<a id="ref-45"></a>**[[45]](#ref-45)** Jiatao Li, Mao Ye, Cheng Peng, Xunjian Yin, and Xiaojun Wan. *AGENT-X: Adaptive Guideline-Based Expert Network for Threshold-Free AI-Generated Text Detection.* [arXiv:2505.15261](https://arxiv.org/abs/2505.15261), 2025.

<a id="ref-46"></a>**[[46]](#ref-46)** Michael J. Ryan, Omar Shaikh, Aditri Bhagirath, Daniel Frees, William Held, and Diyi Yang. *SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs.* In Proceedings of the 63rd Annual Meeting of the ACL (Volume 1: Long Papers), 2025.

<a id="ref-47"></a>**[[47]](#ref-47)** Ran Xu, Jingjing Chen, Jiayu Ye, Yu Wu, Jun Yan, Carl Yang, and Hongkun Yu. *Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning.* 2025.

<a id="ref-48"></a>**[[48]](#ref-48)** Mingchen Zhuge, Changsheng Zhao, Dylan R. Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, and Jurgen Schmidhuber. *Agent-as-a-Judge: Evaluate Agents with Agents.* In Forty-second International Conference on Machine Learning, 2025.

<a id="ref-49"></a>**[[49]](#ref-49)** Mengdi Li, Guanqiao Chen, Xufeng Zhao, Haochen Wen, Shu Yang, and Di Wang. *PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning.* 2025.

<a id="ref-50"></a>**[[50]](#ref-50)** Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, and Chelsea Finn. *FSPO: Few-Shot Preference Optimization of Synthetic Preference Data Elicits LLM Personalization to Real Users.* In 2nd Workshop on Models of Human Feedback for AI Alignment, 2025.

<a id="ref-51"></a>**[[51]](#ref-51)** Xinchen Wang, Pengfei Gao, Chao Peng, Ruida Hu, and Cuiyun Gao. *CodeVisionary: An Agent-Based Framework for Evaluating Large Language Models in Code Generation.* 2025.

<a id="ref-52"></a>**[[52]](#ref-52)** Yu Li, Shenyu Zhang, Rui Wu, Xiutian Huang, Yongrui Chen, Wenhao Xu, Guilin Qi, and Dehai Min. *MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation.* In Database Systems for Advanced Applications: DASFAA 2024, pages 415-426, 2024. URL [https://doi.org/10.1007/978-981-97-5575-2_31](https://doi.org/10.1007/978-981-97-5575-2_31).

<a id="ref-53"></a>**[[53]](#ref-53)** Manya Wadhwa, Zayne Rea Sprague, Chaitanya Malaviya, Philippe Laban, Junyi Jessy Li, and Greg Durrett. *EvalAgents: Discovering Implicit Evaluation Criteria from the Web.* In Second Conference on Language Modeling, 2025. URL [https://openreview.net/forum?id=erGpkHCybv](https://openreview.net/forum?id=erGpkHCybv).

<a id="ref-54"></a>**[[54]](#ref-54)** Kaishuai Xu, Tiezheng Yu, Yi Cheng, Wenjun Hou, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, and Wenjie Li. *Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework.* In Findings of the ACL: ACL 2025, pages 9488-9502, 2025. URL [https://aclanthology.org/2025.findings-acl.494/](https://aclanthology.org/2025.findings-acl.494/).

<a id="ref-55"></a>**[[55]](#ref-55)** MohammadHossein Rezaei, Robert Vacareanu, Zihao Wang, Clinton Wang, Bing Liu, Yunzhong He, and Afra Feyza Akyurek. *OnlineRubrics Elicitation from Pairwise Comparisons.* [arXiv:2510.07284](https://arxiv.org/abs/2510.07284), 2025.

<a id="ref-56"></a>**[[56]](#ref-56)** Zhaopeng Feng, Jiayuan Su, Jiamei Zheng, Jiahan Ren, Yan Zhang, Jian Wu, Hongwei Wang, and Zuozhu Liu. *M-MAD: Multidimensional Multi-Agent Debate for Advanced Machine Translation Evaluation.* In Proceedings of the 63rd Annual Meeting of the ACL (Volume 1: Long Papers), pages 7084-7107, 2025. URL [https://aclanthology.org/2025.acl-long.351/](https://aclanthology.org/2025.acl-long.351/).

<a id="ref-57"></a>**[[57]](#ref-57)** Jiamin Su, Yibo Yan, Zhuoran Gao, Han Zhang, Xiang Liu, and Xuming Hu. *CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring.* [arXiv:2505.13965](https://arxiv.org/abs/2505.13965), 2025.

<a id="ref-58"></a>**[[58]](#ref-58)** Sandeep Kumar, Abhijit A Nargund, and Vivek Sridhar. *CourtEval: A Courtroom-Based Multi-Agent Evaluation Framework.* In Findings of the ACL: ACL 2025, pages 25875-25887, 2025.

<a id="ref-59"></a>**[[59]](#ref-59)** Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. *G-Eval: NLG Evaluation Using GPT-4 with Better Human Alignment.* In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2511-2522, 2023. URL [https://aclanthology.org/2023.emnlp-main.153/](https://aclanthology.org/2023.emnlp-main.153/).

<a id="ref-60"></a>**[[60]](#ref-60)** Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. *Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models.* In The Twelfth International Conference on Learning Representations, 2023.

<a id="ref-61"></a>**[[61]](#ref-61)** Lianghui Zhu, Xinggang Wang, and Xinlong Wang. *JudgeLM: Fine-tuned Large Language Models are Scalable Judges.* In The Thirteenth International Conference on Learning Representations, 2025. URL [https://openreview.net/forum?id=xsELpEPn4A](https://openreview.net/forum?id=xsELpEPn4A).

<a id="ref-62"></a>**[[62]](#ref-62)** Yougang Lyu, Shijie Ren, Yue Feng, Zihan Wang, Zhumin Chen, Zhaochun Ren, and Maarten de Rijke. *Self-Adaptive Cognitive Debiasing for Large Language Models in Decision-Making.* 2025. URL [https://arxiv.org/abs/2504.04141](https://arxiv.org/abs/2504.04141).

<a id="ref-63"></a>**[[63]](#ref-63)** Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. *A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence.* [arXiv:2507.21046](https://arxiv.org/abs/2507.21046), 2025.

<a id="ref-64"></a>**[[64]](#ref-64)** Mahnaz Koupaee, Jake W. Vincent, Saab Mansour, Igor Shalyminov, Han He, Hwanjun Song, Raphael Shu, Jianfeng He, Yi Nian, Amy Wing-mei Wong, Kyu J. Han, and Hang Su. *Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation.* In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the ACL: Human Language Technologies (Volume 1: Long Papers), pages 12209-12246, 2025. URL [https://aclanthology.org/2025.naacl-long.609/](https://aclanthology.org/2025.naacl-long.609/).

<a id="ref-65"></a>**[[65]](#ref-65)** Taaha Kazi, Ruiliang Lyu, Sizhe Zhou, Dilek Hakkani-Tur, and Gokhan Tur. *Large Language Models as User-Agents for Evaluating Task-Oriented Dialogue Systems.* In 2024 IEEE Spoken Language Technology Workshop (SLT), pages 913-920, 2024.

<a id="ref-66"></a>**[[66]](#ref-66)** Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang. *Large Language Models are Diverse Role-Players for Summarization Evaluation.* In CCF International Conference on Natural Language Processing and Chinese Computing, pages 695-707, 2023.

