---
title: "When2Call Analysis"
arxiv_id: "2504.18851"
source: "./2504.18851.md"
type: analysis
created: 2026-01-09
details_folder: "./2504.18851-details/"
---

# When2Call: When (not) to Call Tools - Analysis

> **Paper**: [When2Call: When (not) to Call Tools](./2504.18851.md)
> **Authors**: Hayley Ross (Harvard), Ameya Sunil Mahabaleshwarkar (NVIDIA), Yoshi Suhara (NVIDIA)
> **Published**: 2025-04-26 | **arXiv**: [2504.18851](https://arxiv.org/abs/2504.18851)

---

## 1. Overview

When2Call은 LLM의 도구 호출(tool-calling) 의사결정 능력을 평가하는 새로운 벤치마크이다. 기존 벤치마크들이 "올바른 도구를 올바른 파라미터로 호출했는가"에만 집중하는 반면, When2Call은 **도구를 호출하지 말아야 할 때 모델이 어떻게 행동하는지**를 정량화한다.

이 벤치마크는 4가지 선택지의 다중 선택 형식을 채택한다: (a) 직접 답변 (항상 환각), (b) 도구 호출, (c) 추가 정보 요청, (d) 답변 불가 인정. 저자들은 BFCL Live 데이터셋을 기반으로 3,652개 테스트 샘플과 6,000개 학습 샘플을 생성했으며, RPO(Reward-aware Preference Optimization)를 통해 도구 호출 능력을 유지하면서 환각을 크게 줄이는 학습 방법을 제안한다. 실험 결과, GPT-4o조차 26%의 도구 환각률을 보이며, 이 영역에서 개선의 여지가 크다는 것을 보여준다.

---

## 2. Core Section

### TL;DR
> 도구 호출 LLM이 "언제 도구를 호출하지 말아야 하는지"를 평가하는 벤치마크. 4가지 행동 유형(직접답변/도구호출/추가질문/불가인정) 중 선택하게 하여 환각을 정량화하고, RPO 학습으로 1.2% 수준까지 도구 환각 감소.

**[Extended TL;DR](./2504.18851-details/tldr.md)**: 상세한 문제 정의, 해결 방안, 핵심 발견 포함

### Core Contributions

| # | Contribution | Type | Impact |
|---|--------------|------|--------|
| 1 | When2Call 벤치마크 (3,652 테스트 샘플) | Benchmark | High |
| 2 | 학습 데이터셋 (6,000 SFT + 10,500 RPO) | Dataset | High |
| 3 | RPO 기반 균형 학습 방법 | Method | Medium-High |
| 4 | 환각 유형별 정량화 프레임워크 | Analysis | Medium |

**[Contributions Deep Analysis](./2504.18851-details/contributions.md)**: 각 기여의 상세 분석

### Key vs Non-Key Sections

| Priority | Sections | Description |
|----------|----------|-------------|
| **Must Read** | Abstract, 1, 2.1, Table 3 | 문제 정의, 벤치마크 설계, 핵심 결과 |
| **Should Read** | 3.2, 3.4, 5, Table 1 | 실험 결과, 학습 방법 비교, 논의 |
| **Optional** | 2.2-2.4, 4, 6, Appendix | 데이터 생성, LLM-as-Judge, 관련 연구 |

**[Section Reading Guide](./2504.18851-details/key-sections.md)**: 목적별 읽기 경로 안내

---

## 3. Paper Type: Benchmark

### Classification
- **Primary**: Benchmark (평가 데이터셋 + 메트릭)
- **Secondary**: Method (RPO 학습 방법)

### Key Aspects

**Benchmark Characteristics**:
- 규모: 3,652 테스트 / 6,000 학습 샘플
- 형식: Multiple-choice (4 options)
- 소스: BFCL Live (인간 생성) + APIGen (합성)
- 도메인: 실시간 정보, 데이터베이스, 전문 도구

**Evaluation Dimensions**:
```
┌──────────────────────────────────────────────────┐
│              When2Call Evaluation                │
├────────────────┬─────────────────────────────────┤
│ Tool Call      │ 올바른 도구 + 올바른 파라미터   │
│ Follow-up      │ 필수 파라미터 누락 시 질문      │
│ Unable         │ 적절한 도구 없을 때 인정        │
│ Direct Answer  │ 환각 (항상 오답)               │
└────────────────┴─────────────────────────────────┘
```

**Metrics**: Accuracy (norm), Macro F1, Tool/Answer/Parameter Hallucination Rate

---

## 4. Visual Analysis

### Key Figures

#### Figure 1: Benchmark Example ([Source](./2504.18851.md#figure-1))
학생 기록 조회 도구가 주어졌을 때 성적을 묻는 질문. 도구가 의미적으로 유사하지만 정확히 맞지 않는 상황을 보여줌.

**Insight**: BFCL Irrelevance의 명백한 불일치와 달리, 미묘한 도메인 불일치 테스트

#### Figure 2: BFCL vs When2Call ([Source](./2504.18851.md#figure-2))
BFCL Irrelevance 점수와 When2Call 점수의 관계를 보여주는 산점도.

**Insight**: BFCL Irrelevance 고득점이 When2Call 고득점을 보장하지 않음 → When2Call이 더 어려운 과제

### Key Tables

#### Table 1: Benchmark Comparison ([Source](./2504.18851.md#table-1))

| Feature | When2Call | BFCL | ToolSandbox | ToolBeHonest |
|---------|-----------|------|-------------|--------------|
| Tool provided, one correct | Y | Y | Y | Y |
| No tools provided | Y | N | N | ~ |
| Missing information | Y | N | Y | Y |
| Quantifies tool hallucination | Y | N | N | Implicit |
| Quantifies follow-up questions | Y | N | ~ | N |

#### Table 3: Main Results ([Source](./2504.18851.md#table-3))

| Model | When2Call F1 | Tool Hall% | BFCL AST |
|-------|-------------|------------|----------|
| Llama 3.1 70B | 37.8 | 57% | 68.3% |
| Qwen 2.5 72B | 32.8 | 23% | 69.3% |
| GPT-4o | 61.3 | 26% | 79.8% |
| **MNM 8B RPO** | **52.4** | **1.2%** | 62.5% |

**Core Finding**: 모델 크기와 When2Call 성능이 비례하지 않음. RPO 학습이 환각 감소에 효과적.

### Mathematical Formulations

**Length-normalized Accuracy**:
```
Acc_norm = argmax_i (log P(option_i) / length(option_i))
```

**RPO Objective** (from [Nemotron-4 Technical Report](../2504.18851.md#ref-18)):
선호 응답과 비선호 응답 쌍에서 KL-penalty 0.05로 학습

---

## 5. Critique & Related Works

### Strengths

1. **Gap 식별**: 기존 벤치마크가 놓친 "도구 미호출" 상황 체계화
2. **실용적 메트릭**: 환각 유형별 정량화로 모델 약점 진단 가능
3. **학습 방법 제안**: RPO를 통해 균형 잡힌 개선 달성
4. **재현성**: 코드, 데이터, 평가 스크립트 모두 공개

### Limitations

1. **합성 데이터 품질**: 92% 질문, 94% 답변 품질 (8%/6% 오류)
2. **영어 전용**: 다국어 지원 부재
3. **폐쇄형 모델 제한**: GPT 계열만 평가
4. **Direct Answer 가정**: 모든 직접 답변이 환각이라는 단순화

### 2026 Perspective

- **Agent 시대와의 연관**: 도구 호출 의사결정은 자율 에이전트의 핵심 역량
- **Safety 관점**: 도구 환각은 실제 API 호출로 이어질 수 있어 보안 위험
- **Multi-modal 확장**: 비전/오디오 도구까지 확장 가능성
- **BFCL 포화 해결**: 저자 언급대로 BFCL Irrelevance가 포화 중, When2Call이 대안

### Related Works Comparison

| Work | Focus | When2Call과의 차이 |
|------|-------|-------------------|
| **BFCL** [[31](./2504.18851.md#ref-31)] | 도구 호출 정확도 | 미호출 시 행동 미평가 |
| **ToolSandbox** [[14](./2504.18851.md#ref-14)] | 대화형 도구 사용 | 호출 여부만 확인 |
| **ToolBeHonest** [[36](./2504.18851.md#ref-36)] | 해결가능성 판단 | 간접적 환각 측정 |
| **APIGen** [[13](./2504.18851.md#ref-13)] | 학습 데이터 생성 | 부정 예시 부족 |

### Recommended Follow-up

1. 다국어 When2Call 구축
2. Multi-modal 도구 호출 벤치마크
3. 에이전트 환경에서의 연쇄 도구 호출 의사결정

---

## Navigation

| File | Description |
|------|-------------|
| [TL;DR](./2504.18851-details/tldr.md) | Extended summary with source references |
| [Contributions](./2504.18851-details/contributions.md) | Deep analysis of core contributions |
| [Key Sections](./2504.18851-details/key-sections.md) | Reading priority guide |
| [Methodology](./2504.18851-details/methodology.md) | Detailed methodology analysis |
| [Original Paper](./2504.18851.md) | Full paper markdown |

---

## Quick Reference

```
@article{ross2025when2call,
  title={When2Call: When (not) to Call Tools},
  author={Ross, Hayley and Mahabaleshwarkar, Ameya Sunil and Suhara, Yoshi},
  journal={arXiv preprint arXiv:2504.18851},
  year={2025}
}
```

**Code & Data**: [https://github.com/NVIDIA/When2Call](https://github.com/NVIDIA/When2Call)
