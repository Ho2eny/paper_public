---
title: "Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases"
arxiv_id: "2512.10398"
authors:
  - "Sherman Wong"
  - "Zhenting Qi"
  - "Zhaodong Wang"
  - "Nathan Hu"
  - "Samuel Lin"
  - "Jun Ge"
  - "Erwin Gao"
  - "Wenlin Chen"
  - "Yilun Du"
  - "Minlan Yu"
  - "Ying Zhang"
published: "2025-12-11"
updated: "2026-02-03"
categories:
  - "cs.CL"
  - "cs.AI"
  - "cs.LG"
  - "cs.SE"
url: "https://arxiv.org/abs/2512.10398"
pdf: "https://arxiv.org/pdf/2512.10398.pdf"
converted_date: "2026-02-06"
---

# Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases

## Authors

Sherman Wong[<sup>1,*</sup>](#aff-1), Zhenting Qi[<sup>2,*</sup>](#aff-2), Zhaodong Wang[<sup>1,*</sup>](#aff-1), Nathan Hu[<sup>1,*</sup>](#aff-1), Samuel Lin[<sup>1</sup>](#aff-1), Jun Ge[<sup>1</sup>](#aff-1), Erwin Gao[<sup>1</sup>](#aff-1), Wenlin Chen[<sup>1</sup>](#aff-1), Yilun Du[<sup>2</sup>](#aff-2), Minlan Yu[<sup>1,2</sup>](#aff-1), Ying Zhang[<sup>1</sup>](#aff-1)

<a id="aff-1"></a>**<sup>1</sup>** Meta &nbsp; <a id="aff-2"></a>**<sup>2</sup>** Harvard University

<a id="fn-star"></a>**<sup>*</sup>** Core Contributors

## Abstract

Real-world software engineering tasks require coding agents that can operate on massive repositories, sustain long-horizon sessions, and reliably coordinate complex toolchains at test time. Existing research-grade coding agents offer transparency but struggle when scaled to heavier, production-level workloads, while production-grade systems achieve strong practical performance but provide limited extensibility, interpretability, and controllability. We introduce the Confucius Code Agent (CCA), a software engineering agent that can operate at large-scale codebases. CCA is built on top of the Confucius SDK, an agent development platform structured around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK supports a unified orchestrator with advanced context management for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension system for reliable tool use. In addition, we introduce a meta-agent that automates the construction, evaluation, and refinement of agents through a build-test-improve cycle, enabling rapid agent development on new tasks and tool stacks. Instantiated on the Confucius SDK using the meta-agent, CCA demonstrates strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a Resolve@1 of 59%, exceeding prior research baselines as well as commercial results, under identical repositories, model backends, and tool access.

---

**Date:** February 4, 2026
**Correspondence:** Zhenting Qi at zhentingqi@g.harvard.edu
**GitHub:** [https://github.com/facebookresearch/cca-swebench](https://github.com/facebookresearch/cca-swebench)

![Figure 1: Performance comparison on SWE-Bench-Pro benchmark](resources/figure_1.png)

**Figure 1** Performance comparison on SWE-Bench-Pro benchmark. (* reported from Anthropic's Claude Opus 4.5 system card / OpenAI's GPT-5.2-Codex system card.)

## 1 Introduction

Software engineering has rapidly emerged as a frontier application area for large language models (LLMs). As models have grown more capable, they have progressed from simple program synthesis ([Austin et al., 2021](#ref-austin-2021)), to automatic code completion ([Chen et al., 2021](#ref-chen-2021)), to general-purpose code generation ([Li et al., 2022](#ref-li-2022); [Lai et al., 2023](#ref-lai-2023)), to understanding code execution ([Gu et al., 2024](#ref-gu-2024)), and competition-level programming ([Jain et al., 2024](#ref-jain-2024)). Most recently, LLMs have demonstrated strong software engineering ability to tackle real-world issue resolution in open-source repositories ([Jimenez et al., 2023](#ref-jimenez-2023); [Yang et al., 2024](#ref-yang-2024); [Xia et al., 2025](#ref-xia-2025); [Zeng et al., 2025](#ref-zeng-2025)). To support such capabilities, more sophisticated agentic frameworks such as OpenHands ([Wang et al., 2024](#ref-wang-2024)) scaffold LLMs with tools for search, code editing, and command execution, while agentless prompting-based approaches ([Xia et al., 2024](#ref-xia-2024)) have shown that carefully structured prompts alone can also perform well on multi-step software engineering tasks.

While model capabilities continue to improve, success in real-world software engineering depends not only on the underlying LLM, but also on the *agent scaffold*: the orchestration, memory structures, and tool abstractions surrounding the model. Empirically, even when the same backbone model is used, different scaffolding strategies can lead to large performance disparities ([Xia et al., 2025](#ref-xia-2025)), suggesting that the design of the agent's cognitive and operational environment is a fundamental research dimension. However, existing coding agents often rely on flat interaction histories, heuristic prompt engineering, or tightly coupled tool pipelines, which are difficult to scale to the long-horizon, multi-file, multi-step workflows characteristic of enterprise-level software engineering. This gap is most clearly manifested in two core **challenges**:

- **C1: Long-context reasoning.** Agents must efficiently localize relevant code within massive repositories and perform multi-hop reasoning across dispersed modules, long tool traces, and deep execution histories.
- **C2: Long-term memory.** Agents should accumulate persistent knowledge across tasks and sessions, capturing reusable patterns, failure modes, and invariants, rather than repeatedly rediscovering information or reproducing past mistakes.

These challenges highlight that scalability in agentic software engineering requires more than longer context windows or larger models: it requires a principled approach to how agents structure, maintain, and interact with external information.

We argue that addressing these challenges requires a broader, system-level design perspective. In particular, we decompose agentic interaction into three complementary axes: **Agent Experience (AX)**, **User Experience (UX)**, and **Developer Experience (DX)**. AX concerns the agent's internal cognitive workspace: how information is distilled, organized, and presented to the LLM for stable reasoning. UX concerns the transparency, controllability, and interpretability required for human users to understand the agent's behavior. DX concerns observability, evaluation, and modularity for researchers and practitioners developing and improving agent systems. Most existing frameworks conflate these axes. For example, passing human-oriented logs directly into the agent's prompt, thereby degrading AX, limiting UX, and restricting DX. Treating AX, UX, and DX as first-class and distinct design principles provides a foundation for scalable, analyzable, and reproducible agent behavior.

We first introduce the **Confucius SDK** (Figure 2), an agent development platform explicitly structured around AX, UX, and DX. On this platform, we instantiate the **Confucius Code Agent (CCA)**, a concrete agent tailored to large-scale software engineering. We introduce four key mechanisms, each served for AX, UX, and/or DX, and instantiated in CCA to address the two challenges above:

1. **Context management (AX; C1).** A hierarchical working memory coupled with context compression enables the agent to retain essential state while supporting long-horizon reasoning without exceeding context limits.
2. **Note-taking (AX, UX; C2).** A dedicated note-taking agent distills trajectories into persistent, hierarchical Markdown notes, including hindsight notes that capture failure modes, thereby supporting both durable knowledge for the agent (AX) and interpretable artifacts for humans (UX).
3. **Extensions (AX, DX; C1).** Modular extensions define tool-use behavior, parsing, prompt shaping, and interaction policies through typed callbacks. This separation improves agent control and reasoning stability (AX) while providing observability and composability for developers (DX). CCA binds together coding-specific extensions such as file search, file editing, and CLI tools.

![Figure 2: Confucius SDK overview](resources/figure_2.png)

**Figure 2** Confucius SDK overview. The SDK unifies an orchestrator for iterative reasoning and action execution, long-term memory for continual learning, and modular extensions for tool use and interacting with the external environment.

4. **Meta-agent (DX).** A meta-agent automates a build-test-improve loop that synthesizes, evaluates, and refines agent configurations, enabling rapid agent development and adaptation to new environments.

To evaluate the mechanisms introduced by Confucius SDK and CCA, we conduct experiments on SWE-BenchVerified ([Jimenez et al., 2023](#ref-jimenez-2023)) and SWE-Bench-Pro ([Deng et al., 2025](#ref-deng-2025)). CCA achieves strong performance compared with prior coding agents, demonstrating how principled scaffolding can substantially amplify the effectiveness of the same underlying LLM. We also evaluate CCA on a custom PyTorch-Bench targeting debugging workflows on larger-scale codebases. Ablations isolate the contributions of each crucial mechanism, and analyses categorize remaining failure cases. In summary, our contributions are:

- We present the **Confucius Code Agent (CCA)**, a coding agent designed for large-scale software engineering.
- We present the **Confucius SDK**, a principled AX/UX/DX-balanced agent development platform with advanced context management, extensions, and long-term memory.
- We provide evaluations across multiple benchmarks, supported by ablations and case studies. Notably, on SWE-Bench-Pro, CCA reaches a Resolve@1 performance of **59%**, exceeding prior research baselines and commercial results, under identical repositories, model backend, and tool access.
- We empirically show that *agent scaffolding*, not just model capability, is also a primary determinant of agent performance, with appropriate orchestration and memory structures outperforming stronger models.

## 2 Method

In this section, we describe both the algorithmic design of agentic reasoning and the system design of the Confucius SDK that supports it. We first introduce the overarching design philosophy (AX, UX, DX) (Section 2.1), then separate the presentation into (i) the SDK abstractions that realize these design choices in a modular and extensible manner and how they are instantiated on CCA (Section 2.2) and (ii) the core agentic algorithms that govern how agents reason and improve over time (Section 2.3).

### 2.1 Design Philosophy: AX, UX, and DX

Most agent frameworks optimize for a single audience (users, agents, or developers). In contrast, the Confucius SDK adopts a three-axis philosophy that treats *Agent Experience (AX)*, *User Experience (UX)*, and *Developer Experience (DX)* as distinct yet coupled design goals. AX concerns the agent's cognitive workspace: what context the model receives, how it is structured, and how it invokes tools. It should be concise and structured to avoid distraction or bias. UX governs how humans observe and interact with the system, prioritizing transparency via readable logs, traces, and artifact previews. DX supports building and improving agents through observability and control over prompts, tools, and memory, with visibility into both AX and UX.

![Figure 3: Context compression overview](resources/figure_3.png)

**Figure 3** Context compression overview. When the context window approaches configurable thresholds, the Architect agent summarizes earlier turns into a structured plan containing goals, decisions, errors, and open TODOs. These compressed summaries replace original large spans of history while preserving a short window of recent interactions, enabling the agent to sustain multi-step reasoning over long trajectories without exceeding context limits.

Many systems conflate AX and UX by feeding human-facing traces directly to the model, causing context bloat, spurious anchors, and brittle debugging. CCA instead separates channels: users get rich instrumented traces, the agent gets compressed structured inputs, and developers can inspect and modify both.

Below shows a concrete example of the distinction between AX and UX:

> **For UX (Users See):**
> ```
> Creating file at config.py
> File created successfully at config.py
> Here is the diff:
> + PORT=8080
> + DEBUG=true
> + MAX_CONNECTIONS=100
> ```
>
> **For AX (Agent Sees):**
> ```
> Human: [previous user message]
> AI: <file_edit type="create" file_path="config.py">...</file_edit>
> Human: <result>File created successfully</result>
> ```

In this case, users are presented with rich, streaming updates, whereas the agent receives only a compressed summary of the outcome stored in the memory manager, without the lengthy file diff message.

### 2.2 Confucius SDK Features & Instantiations on CCA

#### 2.2.1 Context Management

Running agents on large-scale repositories quickly stresses even long-context LLMs: long debugging sessions, multi-file refactors, and nested tool calls all contribute to unbounded conversation growth. In many existing coding agent frameworks, agents either accumulate a single flat history (risking hard context limits and 'forgotten' early decisions) or rely on naive truncation and ad-hoc retrieval, which can silently drop important information and are difficult to tune for different workloads. The Confucius SDK addresses this by providing an explicit agent context management layer that combines hierarchical working memory with adaptive context compression.

At the SDK level, each instantiated agent is backed by a file-system-based **hierarchical working memory** with configurable visibility scopes (e.g., session, entry, runnable). The memory is organized as a tree-structured namespace where internal nodes represent semantic groupings and leaf nodes store Markdown documents annotated with metadata tags. Agents interact with this memory through a set of primitive operations, including search, read, write, edit, and delete, which are exposed as callable tools during generation. Below is an example of a constructed hierarchical memory of CCA on a SWE-Bench-Pro instance. The agent maintains this hierarchy throughout execution so that when context is pruned, important insights and intermediate artifacts can be stored and later retrieved efficiently:

```
+-- instance_qutebrowser__qutebrowser-c09e1439...
    +-- hierarchical_memory_3a7488c6-bf8c-11f0-...
        +-- qutebrowser_process_cleanup
            |-- analysis.md
            |-- implementation_summary.md
        +-- todo.md
```

On top of this hierarchy, Confucius SDK integrates an adaptive **context compression** mechanism (Figure 3) driven by an *Architect Agent*. When the context length approaches configurable thresholds, the Architect Agent is invoked in a separate call to analyze the conversation history and construct a structured summary that explicitly preserves key information categories (e.g., task goals, decisions made, open TODOs, and critical error traces). The system then replaces marked historical messages with this compressed summary while maintaining a rolling window of recent messages in their original form. The summary is inserted as a new message, and all future turns will see both the compact summary and the recent raw history.

This context management design provides two key benefits. First, structured summarization triggered only when needed preserves semantically important information and maintains access to long reasoning chains, avoiding the brittleness of fixed-window truncation or simple retrieval. Second, the hierarchical memory stores and refines key insights throughout execution, complementing the summaries and ensuring that important state persists even as the raw history is compressed. In CCA, these mechanisms are essential for handling long-running software engineering sessions on industrial-scale codebases, improving performance on long-context coding tasks without requiring changes to the underlying orchestrator or extensions. Similar context engineering techniques are also reported in recent production-level LLMs ([Anthropic, 2025](#ref-anthropic-2025); [OpenAI, 2025](#ref-openai-2025)).

#### 2.2.2 Note-Taking

Flat chat logs are not an ideal representation for long-term memory: they are verbose and difficult to reuse without manually rereading entire transcripts. In typical frameworks, any cross-session 'memory' is either absent or implemented via coarse-grained embeddings over whole turns, which tends to miss important structure such as architectures, design decisions, and failure modes. To support agents that improve over time and can pick up long-running projects where they left off, the Confucius SDK includes an explicit note-taking mechanism that memorizes interaction traces as structured persistent knowledge.

At the SDK level, every interaction session is logged into a structured 'trajectory', including user messages, tool invocations, LLM outputs, and system events. A dedicated **note-taking agent** can distill these trajectories into compact notes without affecting the online latency of the primary agent. Persistent notes are stored as Markdown files in a file-system-like tree: each session has an associated directory, under which the note-taking agent can create paths such as `research/findings.md` and `solutions/bug_fix.md`. Leaves in this hierarchy are Markdown documents with lightweight tags, maintained as typed memory nodes. The SDK exposes structured tools to search, read, write, edit, delete, and import these nodes, so notes can be programmatically updated and reused across sessions. Examples of taken notes are shown in Appendix F.

A distinctive aspect of the Confucius SDK's note-taking is its emphasis on *hindsight notes* for failures. The note-taking layer encourages agents to record not only successful solutions but also compilation errors, runtime exceptions, and unproductive strategies, together with eventual resolutions or reasons for abandonment. Over time, this yields a corpus of failure cases indexed by error messages, stack traces, and affected components. When a similar failure appears in a future session, an agent can retrieve the corresponding hindsight note and immediately surface known fixes or workarounds, rather than rediscovering them from scratch. In CCA, these mechanisms turn day-to-day usage on large codebases into a steadily growing, human-readable body of durable knowledge that improves continuity across sessions and reduces repeated 'thrashing' on recurring issues.

#### 2.2.3 Extensions

The Confucius SDK implements output parsing, tool invocation, and side-effect management via *extensions*: modular components that attach to the orchestrator and run at every execution step. Whereas many frameworks hard-code these behaviors in ad-hoc logic or model-specific prompting, Confucius exposes them as first-class, composable modules that are easier to reuse, audit, and adapt.

Each extension is a typed configuration object that registers ordered callbacks (e.g., `on_input_messages`, `on_llm_output`) executed within the orchestrator loop. Callbacks share a run context providing I/O, session state, memory, and artifacts, enabling extensions to shape prompts, interpret structured outputs (XML tags or native tool calls), and inject or filter messages while maintaining local state.

Conceptually, extensions cover perception, reasoning, and action: they (i) parse and validate model outputs into structured actions, (ii) rewrite or annotate inputs before LLM calls, and (iii) execute tools (e.g., shell, file edits) and summarize results into memory or artifacts. By routing tool use and prompt shaping through extensions, Confucius cleanly separates orchestration from capabilities. CCA is thus an orchestrator paired with a particular extension bundle; our ablations (Table 2) vary only enabled extensions and configurations while holding the loop fixed, so improvements to extensions transfer across all Confucius-based agents.

### 2.3 Agentic Algorithms: Orchestration and Automatic Agent Development

#### 2.3.1 The Confucius Orchestrator

The Confucius Orchestrator is a minimal, extensible loop that repeatedly calls the LLM, interprets its outputs, and coordinates tool execution. The same orchestrator is reused across all Confucius SDK agents; CCA is one configuration of this loop with coding-oriented extensions enabled.

*Output Processing.* The orchestrator supports two LLM interfaces: (1) native tool-use models emit structured tool calls that are routed to extension handlers; (2) other models emit XML-style tags (e.g., `<bash>...</bash>`), which are parsed into the same action representation. This preserves broad model compatibility while leveraging native tool-use when available.

![Algorithm 1: Confucius Orchestrator Loop](resources/algorithm_1.png)

*Iteration Control.* Execution is bounded by a maximum iteration cap, but termination is typically agent-driven. Each iteration parses the LLM output into actions; if no further actions are produced, the run ends. Extensions can also force continuation (e.g., a bash execution returns an interrupt with command output), causing the orchestrator to re-invoke the LLM with updated context. Together, these mechanisms support multi-step reasoning and tool use within safe bounds.

#### 2.3.2 Meta-agent: A Build-Test-Improve Loop

A recurring limitation of existing agent frameworks is that agent behavior is largely *static*: humans hand-design prompts, tool wiring, and guardrails, then periodically revise them by trial and error. This is labor-intensive and does not scale with growing tool ecosystems. Moreover, we find that naive implementations of file-editing or command-line tools, even when they are functionally correct, often underperform because the surrounding prompts and error-handling conventions are not tuned to realistic workloads. We address this by introducing a *Meta Agent*, an agent that automatically builds and refines other agents through an explicit *build-test-improve* loop, turning agent design itself into an agentic, evaluation-driven automatic process. More implementation details of Meta Agent can be found at Appendix C.1.

This Meta-agent capability lives at the Confucius SDK level but directly benefits CCA. The proposed CCA is itself the outcome of the Meta-agent's build-improve-test loop: we start from a high-level description of a repository-level software engineering assistant, let the Meta-agent synthesize the orchestrator configuration, tool wiring, and prompts, and then repeatedly refine them against a production-grade test set until performance stabilizes.

## 3 Experiments

### 3.1 Setup

*Models and agent scaffold.* We use Claude 4 Sonnet, Claude 4.5 Sonnet, Claude 4.5 Opus, and GPT-5.2 as the primary backbone LLMs to ensure comparability with published baselines. We use SWE-Agent ([Yang et al., 2024](#ref-yang-2024)) as the baseline scaffold. Our CCA agent replaces the SWE-Agent stack with the Confucius Code Agent. We also report results from the Live-SWE-Agent ([Xia et al., 2025](#ref-xia-2025)) as baseline, while keeping the tool environment and repository setup identical.

*Benchmark.* For main results, we evaluate CCA on the SWE-Bench-Pro ([Deng et al., 2025](#ref-deng-2025)) public split consisting of 731 tasks, following the identical environment configuration and infrastructure used by the SWE-Agent baseline ([Yang et al., 2024](#ref-yang-2024)). We also report results from the SWE-Bench-Verified ([Jimenez et al., 2023](#ref-jimenez-2023)) consisting of 500 tasks to compare with existing open-sourced coding agents, including SWE-Agent and OpenHands ([Wang et al., 2024](#ref-wang-2024)).

*Metrics.* We follow the official SWE-Bench-Pro Resolve Rate metric, defined as the percentage of tasks for which the agent's proposed patch successfully passes all repository-provided tests without human intervention.[<sup>1</sup>](#fn-1) Each trial is repeated with different random seeds for trajectory sampling to account for randomness in tool invocation and LLM responses. We report mean Resolve Rate (Pass@1) across three runs.

### 3.2 Main Results on SWE-Bench-Pro

Table 1 SWE-Bench-Pro public split comparison across scaffolds and backbone models. All methods share identical environments; improvements arise solely from the agent scaffolds. (* Anthropic's proprietary scaffold, from Claude Opus 4.5 system card; * OpenAI's proprietary scaffold, from GPT-5.2 report.)

| Backbone Model    | Scaffold                        | Resolve Rate (Pass@1) |
|-------------------|---------------------------------|-----------------------|
| Claude 4 Sonnet   | SWE-Agent ([Yang et al., 2024](#ref-yang-2024))   | 42.7                  |
| Claude 4 Sonnet   | CCA                             | 45.5                  |
| Claude 4.5 Sonnet | SWE-Agent                       | 43.6                  |
| Claude 4.5 Sonnet | Live-SWE-Agent ([Xia et al., 2025](#ref-xia-2025)) | 45.8               |
| Claude 4.5 Sonnet | CCA                             | 52.7                  |
| Claude 4.5 Opus   | Anthropic*                      | 52.0                  |
| Claude 4.5 Opus   | CCA                             | **54.3**              |
| GPT-5.2           | OpenAI*                         | 56.0                  |
| GPT-5.2           | CCA                             | **59.0**              |

Table 1 summarizes our main results on the SWE-Bench-Pro public split. Under identical environment and tool conditions, CCA consistently surpasses the SWE-Agent baseline across settings with different backbone models. With Claude 4 Sonnet, CCA reaches Resolve@1 at **45.5%**. With Claude 4.5 Sonnet, CCA reaches **52.7%**, largely surpassing the best research-grade coding agent, Live-SWE-Agent, at 45.8%. With Claude

<a id="fn-1"></a>**[^1]**: [https://scale.com/leaderboard/swe_bench_pro_public](https://scale.com/leaderboard/swe_bench_pro_public)

Table 2 Ablation on context management (hierarchichal working memory & context compression) and tool-use sophistication. Results are obtained from evaluating CCA on a 100-example subset of the SWE-Bench-Pro public set.

| Backbone Model    | Context Management   | Tool Use   |   Resolve Rate (Pass@1) |
|-------------------|----------------------|------------|-------------------------|
| Claude 4 Sonnet   | No                   | advanced   |                    42   |
| Claude 4 Sonnet   | Yes                  | advanced   |                    48.6 |
| Claude 4.5 Sonnet | No                   | simple     |                    44   |
| Claude 4.5 Sonnet | No                   | advanced   |                    51   |
| Claude 4.5 Sonnet | Yes                  | advanced   |                    51.6 |

4.5 Opus, CCA achieves **54.3%**, achieving higher performance than results reported by Anthropic. And with GPT-5.2, we achieve **59.0%**, surpassing the official result reported by OpenAI and setting new leading performance on SWE-Bench-Pro. These improvements arise purely from stronger agentic scaffolding, i.e. enhanced orchestration, context management, and tool-use extensions, rather than differences in backbone models or evaluation setups. More broadly, these results underscore the central role of scaffolding: even a weaker model equipped with a strong agent scaffold (Claude 4.5 Sonnet + CCA at **52.7%**) can outperform a stronger model (Claude 4.5 Opus + Anthropic's proprietary scaffold at **52.0%**).

Because Claude Code (CC) does not expose a programmatic tool interface compatible with containerized evaluation environments such as SWE-rex ([SWE-agent, 2025](#ref-sweagent-2025)), we cannot compare CCA with CC's results on SWE-Bench-Pro. Instead, to provide a qualitative comparison, we constructed a small curated benchmark (a mini PyTorch-Bench) and executed CC solutions using the Claude Code CLI directly on a host machine where CC is installed w/o a Docker-based runtime, as seen in Appendix G. These complementary experiments highlight behavioral differences between CC and CCA in realistic debugging and development tasks, but they are not directly comparable to SWE-Bench-Pro due to differences in execution environments.

### 3.3 Meta-Agent Learned Tool-Use

CCA's tool-use behavior is not purely hand-engineered; instead, it is *learned* through the Meta-agent, which automatically refines how the agent invokes tools such as file editors and command-line utilities. To measure the contribution of this learned tool-use stack, we perform an ablation that disables these Meta-agent-derived tools and instead reverts CCA to a simpler, 'naive' tool-use pattern similar to traditional SWE-Agent-like scaffolds with simple file editing and command-line operations only. Table 2 reports the results of this ablation alongside a separate ablation on context management. Experiments are conducted on a 100-example subset of the SWE-Bench-Pro public set. As shown in the Claude 4.5 Sonnet rows, removing the learned tool-use features leads to a large decline in Resolve@1, even when the context management method is held constant. This confirms that tool-use conventions learned by the Meta-agent are a major driver of CCA's performance, independent of (and complementary to) hierarchical working memory and context compression.

The function below shows an example of meta-agent's improvement on CCA's prompts. The part emphasized here is a failure message that gets raised when the best match is not exact. This message was iteratively refined by the meta-agent to be maximally actionable for an LLM: it shows the closest match, provides a minimal unified diff, and gives explicit constraints that force the agent to keep using `<file_edit>` and to repair the `<find>`/`<find_after>` tag into an exact `<line_number>|<exact_line_content>` format rather than switching tools or attempting unsafe workarounds.

**Listing 1** Excerpt of the file-edit chunk matcher; non-prompt logic is collapsed as "...", while the meta-agent-refined prompt is shown verbatim.

```python
def _get_matched_chunk(...):
    # ...

    if similarity == 1.0:
        return matched_chunk

    # ...

    # --- meta-agent-refined prompt begins
    raise ValueError(
        dedent(
            """\
            No exact occurrence found for the search string you provided.

            # ... shows match and diff

            ACTION REQUIRED: Please update your '<find>' or '<find_after>' tag to
            match the exact content in the file with '<line_number>|
            <exact_line_content>' format.

            IMPORTANT: YOU MUST CONTINUE USING <file_edit> tag until successful.
            DO NOT attempt alternative approaches such as:
            - Creating a new file to override the existing one
            - Using command line tools (e.g., 'sed', 'awk', etc.)

            Continue refining your '<find>' or '<find_after>' tag until it exactly
            matches the file content.
            """
        ).format(...)
    )
    # --- meta-agent-refined prompt ends
```

### 3.4 Evaluations on Long-context Reasoning

#### 3.4.1 Context Management

To quantify the impact of hierarchical working memory and context compression, we evaluate CCA on a subset of SWE-Bench-Pro, where both variants (with and without context management) successfully produced executable solutions.[<sup>2</sup>](#fn-2) Results in Table 2 demonstrate a clear improvement in problem resolution when hierarchical memory and context compression are enabled. For Claude 4 Sonnet, advanced context management improves Resolve@1 from 42.0 to 48.6 on this subset (a +6.6 performance gain). On Claude 4.5, the improvement between the no-context-management and advanced variants is smaller, but both substantially outperform the simple tool-use configuration. This supports the hypothesis that structured context compression not only prevents overflow but also improves reasoning quality by enforcing periodic consolidation of long-horizon plans. To ensure a controlled comparison, we use the same backbone LLM for both context compression and the main orchestrator agent across all experiments; see detailed study in the Appendix B.

#### 3.4.2 Endless-Read Robustness

We further analyze CCA's robustness under tasks that require editing multiple files. Each SWE-Bench-Pro task is grouped by the number of modified files ('edited-file bucket'), and we measure the Resolve Rate within each group. As shown in Table 3, the agent maintains stable performance across varying edit volumes, with only moderate regression when more files are touched. The degradations likely stem from cumulative localization uncertainty and compounding diffs, suggesting future work on finer-grained diff validation and multi-file dependency tracking. Overall, these results show that CCA's hierarchical memory and context compression yield substantial gains in both efficiency and robustness for long-context reasoning.

### 3.5 Evaluations on Long-term Memory

We next study CCA's *note-taking* module, designed to accumulate durable cross-session memory. Unlike transient hierarchical working memory, the note-taking agent asynchronously summarizes each session into structured Markdown notes with multiple steps of reasoning, which capture both successful strategies and failure cases. This persistent 'memory' is then available for retrieval in subsequent tasks, supporting test-time self-improvement.

<a id="fn-2"></a>**[^2]**: Without any context control, many trajectories exceed model token limits and fail to complete, hence the restricted subset.

Table 3 CCA's resolve rate on SWE-Bench-Pro as a function of the number of files modified. Performance remains robust even for multi-file refactoring scenarios.

| Edited Files   |   Resolve Rate (Pass@1) |   #Samples |
|----------------|-------------------------|------------|
| 1-2 files      |                    57.8 |        294 |
| 3-4 files      |                    49.2 |        203 |
| 5-6 files      |                    44.1 |         86 |
| 7-10 files     |                    52.6 |         38 |
| 10+ files      |                    44.4 |         18 |

Since no public benchmark explicitly evaluates memory in coding agents, we assess CCA's memory module by running it on two consecutive passes, i.e., with memory maintained, of SWE-Bench-Pro instances. During the first run, the note-taking agent analyzes each trajectory and produces persistent notes for 151 instances-skipping cases where no meaningful insight can be distilled. We then rerun exactly these 151 tasks, providing CCA with the corresponding note directory to measure how prior experience improves efficiency and solution quality. For Run 1: We execute the task from scratch (no context editing either); use note taker agent to write down notes. For Run 2: We pass the notes from Run 1 to CCA and rerun.

Table 4 CCA performance across repeated runs using notes. For Run 1, all tasks are processed from scratch, and notes are taken and stored. For Run 2, each task resolving session is accompanied by the taken notes as the long-term memory. Token cost excludes system prompt tokens; the underlying model is Claude 4.5 Sonnet.

| Trial   | Avg. Turns ( ↓ )   | Avg. Token Cost ( ↓ )   | Resolve Rate (Pass@1, ↑ )   |
|---------|--------------------|-------------------------|-----------------------------|
| Run 1   | 64                 | 104k                    | 53.0                        |
| Run 2   | 61 (-3)            | 93k (-11k)              | 54.4 (+1.4)                 |

Cumulative note-taking reduces the iteration turns (from 64 to 61) and the token cost (from 104 k to 93 k), and also yield improvements on resolve rate (from 53% to 54.4%). These gains indicate that the notes distilled in the first run capture actionable, reusable knowledge. In effect, the note-taking system provides CCA with a lightweight form of *cross-session learning*, enabling more efficient reasoning and more reliable patch generation in subsequent attempts. A detailed example of the notes produced by the note-taking agent is provided in Appendix F.

### 3.6 Comparison with Open-Sourced Scaffolds on SWE-Bench-Verified

We further conduct evaluations on the SWE-Bench-Verified benchmark ([Jimenez et al., 2023](#ref-jimenez-2023)) to compare CCA against existing open-source scaffolds.[<sup>3</sup>](#fn-3) Using Claude 4 Sonnet, CCA achieves a Resolve Rate of **74.6%**, exceeding the strongest open-source system (OpenHands) under identical backbone conditions and outperforming a mini-SWE-Agent variant that relies on the more capable Claude 4.5 Sonnet model. These results reinforce the central role of agentic scaffolding: improved orchestration, memory handling, and tool-use abstractions can close-or even surpass-the gap introduced by differences in backbone model capability. We also observe that SWE-Bench-Verified is sensitive to Claude's internal thinking budget; a detailed analysis appears in Appendix D.

<a id="fn-3"></a>**[^3]**: As of Dec 2025, OpenHands remains the strongest open-sourced coding agent on SWE-Bench-Verified, reported from SWE-Bench's official leaderboard.

Table 5 CCA performance on SWE-Bench-Verified. CCA matches the best open-source framework (OpenHands) under the same Claude 4 Sonnet backbone, and outperforms a mini-SWE-Agent variant even when that variant uses a stronger Claude 4.5 Sonnet backbone.

| Backbone Model    | Scaffold                      |   Resolve Rate (Pass@1) |
|-------------------|-------------------------------|-------------------------|
| Claude 4 Sonnet   | SWE-Agent                     |                    66.6 |
| Claude 4 Sonnet   | OpenHands ([Wang et al., 2024](#ref-wang-2024)) |                    72.8 |
| Claude 4 Sonnet   | CCA                           |                    74.6 |
| Claude 4.5 Sonnet | mini-SWE-Agent                |                    70.6 |

## 4 Related Work on Coding Agents

A range of agent designs have been proposed to improve LLM-based software engineering. SWE-Agent ([Yang et al., 2024](#ref-yang-2024)) is first proposed as a foundational system showing that an LLM paired with a small tool set (file editing, command execution, testing, etc.) can iteratively interact with real repositories to resolve issues. Subsequent work refines this paradigm in different directions. Live-SWE-Agent ([Xia et al., 2025](#ref-xia-2025)) studies test-time self-evolution, adapting strategies and occasionally updating prompts, tools, or configuration mid-run based on partial progress. Satori-SWE ([Zeng et al., 2025](#ref-zeng-2025)) explores population-based evolution at inference time, evolving multiple agent instances or candidates to improve sample efficiency via systematic refinement. In contrast, Agentless ([Xia et al., 2024](#ref-xia-2024)) argues for reducing agent complexity by replacing the open-ended loop with a fixed three-stage pipeline (localization, patch generation, test-case generation), achieving strong results on SWE-Bench Lite. Beyond academic prototypes, open-source platforms also shape the ecosystem: OpenHands ([Wang et al., 2024](#ref-wang-2024)) provides a community toolkit with a unified API for file I/O and code execution and implements a ReAct-style planner over popular base models. More related work can be found at Appendix A.

## 5 Conclusion

We presented the **Confucius Code Agent (CCA)**, a coding agent for large-scale codebases. Built on the **Confucius SDK**, CCA separates and optimizes **Agent Experience (AX)**, **User Experience (UX)**, and **Developer Experience (DX)**, enabling multi-step reasoning with modular tools, structured memory, and interpretable traces. Across public benchmarks and real-world settings, CCA shows that *agentic scaffolding* -orchestration, memory, and tool abstractions-can matter as much as, or more than, the backbone model. The SDK's hierarchical memory, context compression, and persistent notes support long-horizon stability, while extensions and the meta-agent facilitate rapid adaptation to new tools and workflows.

More broadly, Confucius SDK establishes a new foundation for AI agent research. Its modular architecture invites experimentation: from studying long-context reasoning and long-term memory, to exploring test-time adaptation, to integrating reinforcement learning with structured trajectory traces. We hope this framework accelerates progress toward AI developers that are powerful, interpretable, and continuously improving, bridging the persistent gap between research prototypes and the demands of real-world software engineering.

## References

- <a id="ref-anthropic-2025"></a>Anthropic. Effective context engineering for ai agents, 2025. [https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents). Accessed: 2025-12-10.
- <a id="ref-austin-2021"></a>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint [arXiv:2108.07732](https://arxiv.org/abs/2108.07732), 2021.
- <a id="ref-chen-2021"></a>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating Large Language Models Trained on Code. arXiv e-prints, art. [arXiv:2107.03374](https://arxiv.org/abs/2107.03374), July 2021. doi: 10.48550/arXiv.2107.03374.
- <a id="ref-deng-2025"></a>Xiang Deng, Jeff Da, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, et al. Swe-bench pro: Can ai agents solve long-horizon software engineering tasks? arXiv preprint [arXiv:2509.16941](https://arxiv.org/abs/2509.16941), 2025.
- <a id="ref-gu-2024"></a>Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I Wang. Cruxeval: A benchmark for code reasoning, understanding and execution. arXiv preprint [arXiv:2401.03065](https://arxiv.org/abs/2401.03065), 2024.
- <a id="ref-han-2025"></a>Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning. In Findings of the Association for Computational Linguistics: ACL 2025 , pages 24842-24855, 2025.
- <a id="ref-jain-2024"></a>Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint [arXiv:2403.07974](https://arxiv.org/abs/2403.07974), 2024.
- <a id="ref-jimenez-2023"></a>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint [arXiv:2310.06770](https://arxiv.org/abs/2310.06770), 2023.
- <a id="ref-lai-2023"></a>Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data science code generation. In International Conference on Machine Learning , pages 18319-18345. PMLR, 2023.
- <a id="ref-li-2025"></a>Junyan Li, Wenshuo Zhao, Yang Zhang, and Chuang Gan. Steering llm thinking with budget guidance. arXiv preprint [arXiv:2506.13752](https://arxiv.org/abs/2506.13752), 2025.
- <a id="ref-li-2022"></a>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science , 378 (6624):1092-1097, 2022.
- <a id="ref-lin-2025"></a>Hannah Lin, Martin Maas, Maximilian Roquemore, Arman Hasanzadeh, Fred Lewis, Yusuf Simonson, Tzu-Wei Yang, Amir Yazdanbakhsh, Deniz Altinbüken, Florin Papa, et al. Eco: An llm-driven efficient code optimizer for warehouse scale computers. arXiv preprint [arXiv:2503.15669](https://arxiv.org/abs/2503.15669), 2025.
- <a id="ref-luo-2025"></a>Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna K. Qiu, and Yuqing Yang. Agent lightning: Train any ai agents with reinforcement learning, 2025. [arXiv:2508.03680](https://arxiv.org/abs/2508.03680).
- <a id="ref-ma-2025"></a>Jeffrey Jian Ma, Milad Hashemi, Amir Yazdanbakhsh, Kevin Swersky, Ofir Press, Enhui Li, Vijay Janapa Reddi, and Parthasarathy Ranganathan. Swe-fficiency: Can language models optimize real-world repositories on real workloads? arXiv preprint [arXiv:2511.06090](https://arxiv.org/abs/2511.06090), 2025.
- <a id="ref-openai-2025"></a>OpenAI. Session memory in the openai agents sdk, 2025. [https://cookbook.openai.com/examples/agents_sdk/session_memory](https://cookbook.openai.com/examples/agents_sdk/session_memory). Accessed: 2025-12-10.
- <a id="ref-pan-2024"></a>Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint [arXiv:2412.21139](https://arxiv.org/abs/2412.21139), 2024.
- <a id="ref-potvin-2016"></a>Rachel Potvin and Josh Levenberg. Why google stores billions of lines of code in a single repository. Communications of the ACM , 59:78-87, 2016. [http://dl.acm.org/citation.cfm?id=2854146](http://dl.acm.org/citation.cfm?id=2854146).
- <a id="ref-pytorch-2025"></a>PyTorch. Pytorch, 2025. [https://github.com/pytorch/pytorch](https://github.com/pytorch/pytorch). Open-source deep learning library.
- <a id="ref-sweagent-2025"></a>SWE-agent. Swe-rex: A framework for swe-agent docker-based execution. [https://github.com/SWE-agent/SWE-ReX](https://github.com/SWE-agent/SWE-ReX), 2025. [Online; accessed 10-December-2025].
- <a id="ref-wang-2024"></a>Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint [arXiv:2407.16741](https://arxiv.org/abs/2407.16741), 2024.
- <a id="ref-wei-2025"></a>Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint [arXiv:2502.18449](https://arxiv.org/abs/2502.18449), 2025.
- <a id="ref-wen-2025"></a>Hao Wen, Xinrui Wu, Yi Sun, Feifei Zhang, Liye Chen, Jie Wang, Yunxin Liu, Yunhao Liu, Ya-Qin Zhang, and Yuanchun Li. Budgetthinker: Empowering budget-aware llm reasoning with control tokens. arXiv preprint [arXiv:2508.17196](https://arxiv.org/abs/2508.17196), 2025.
- <a id="ref-xia-2024"></a>Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint [arXiv:2407.01489](https://arxiv.org/abs/2407.01489), 2024.
- <a id="ref-xia-2025"></a>Chunqiu Steven Xia, Zhe Wang, Yan Yang, Yuxiang Wei, and Lingming Zhang. Live-swe-agent: Can software engineering agents self-evolve on the fly? arXiv preprint [arXiv:2511.13646](https://arxiv.org/abs/2511.13646), 2025.
- <a id="ref-yang-2024"></a>John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems , 37:50528-50652, 2024.
- <a id="ref-yang-2025a"></a>John Yang, Carlos E Jimenez, Alex L Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R Narasimhan, Diyi Yang, Sida Wang, and Ofir Press. Swe-bench multimodal: Do ai systems generalize to visual software domains? In The Thirteenth International Conference on Learning Representations , 2025a. [https://openreview.net/forum?id=riTiq3i21b](https://openreview.net/forum?id=riTiq3i21b).
- <a id="ref-yang-2025b"></a>John Yang, Kilian Lieret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents, 2025b. [arXiv:2504.21798](https://arxiv.org/abs/2504.21798).
- <a id="ref-zeng-2025"></a>Guangtao Zeng, Maohao Shen, Delin Chen, Zhenting Qi, Subhro Das, Dan Gutfreund, David Cox, Gregory Wornell, Wei Lu, Zhang-Wei Hong, et al. Satori-swe: Evolutionary test-time scaling for sample-efficient software engineering. arXiv preprint [arXiv:2505.23604](https://arxiv.org/abs/2505.23604), 2025.

## Appendix

## A Additional Related Work

### A.1 Benchmarking Coding Agents

The past two years have seen the emergence of comprehensive benchmarks to evaluate autonomous code-writing and code-fixing agents on realistic tasks. One prominent example is SWE-Bench ([Jimenez et al., 2023](#ref-jimenez-2023)), which poses real-world GitHub issues and provides the full repository context; an agent succeeds by producing a patch that passes all project tests. It has since evolved into a family of benchmarks targeting different scenarios: for instance, variants like SWE-Bench-Multilingual ([Yang et al., 2025b](#ref-yang-2025b)) and SWE-Bench-Multimodal ([Yang et al., 2025a](#ref-yang-2025a)) extend the evaluation to codebases with multiple programming languages and to tasks that involve not only code but other modalities (such as modifying documentation or configurations), respectively. An expanded challenge, SWE-Bench Pro ([Deng et al., 2025](#ref-deng-2025)), was released to assess long-horizon problem solving: it includes complex, enterprise-level issues that may require dozens of files to be modified across a codebase. These benchmarks have become a driving force for the community, with public leaderboards spurring rapid progress. Beyond bug-fixing, entirely new benchmarks are probing other dimensions of software work. SWE-fficiency ([Ma et al., 2025](#ref-ma-2025)) is a recent benchmark that challenges agents to optimize the runtime performance of real codebases given defined workloads.

### A.2 Large-scale Software Engineering

Modern software engineering at scale has driven interest in AI assistance that can handle massive codebases and performance-critical systems. Potvin and Levenberg's seminal description of Google's single vast code repository illustrates the challenges and benefits of the monorepo model ([Potvin and Levenberg, 2016](#ref-potvin-2016)). This approach centralizes billions of lines of code, enabling unified tooling and refactoring, but it demands automated support for code discovery, understanding, and consistent changes at scale. Recent LLM-based systems are beginning to tackle such issues. For instance, Lin et al. introduce ECO, an LLM-driven code optimizer designed for warehouse-scale computers ([Lin et al., 2025](#ref-lin-2025)). ECO leverages a code-generating model to suggest performance improvements in large distributed software, aiming to reduce runtime and resource usage while preserving correctness. Early results show that AI-powered optimization can uncover non-trivial efficiency gains in complex systems, hinting at a future where coding agents assist not only in writing code but also in optimizing and maintaining it across ultra-large codebases. The combination of monorepo development and LLM-based tools like ECO underscores a trend toward holistic scale: treating an entire organization's code as a single evolvable system, with AI agents providing the intelligence to manage global changes, dependency analysis, and performance tuning in ways humans alone could not easily scale. This context also motivates advanced context management techniques - instead of feeding billions of lines directly into an LLM, agents must learn to retrieve and focus on the relevant project fragments, a theme that connects to memory and tool-use innovations discussed later.

### A.3 Training LLMs for Software Engineering

SWE-Gym ([Pan et al., 2024](#ref-pan-2024)) provides the first publicly available executable environment tailored for real-world software engineering tasks: it bundles complete Python repositories with dependencies, unit tests, and realistic issue descriptions, enabling agents to propose patches which can be validated via execution. Later, SWE-Smith ([Yang et al., 2025b](#ref-yang-2025b)) generalizes the idea: given any Python repository, it automatically generates hundreds to thousands of new bug-fix or issue-resolution tasks by perturbing code or simulating realistic faults, producing a dataset of around 50,000 instances across 128 GitHub projects. Training on this large-scale synthetic data significantly improves agent performance on benchmark tasks, indicating that domain-specific, execution-aware fine-tuning is critical for bringing coding agents closer to real-world software engineering demands. Recent research have explored reinforcement learning for code agents. SWE-RL ([Wei et al., 2025](#ref-wei-2025)) takes advantage of the abundant software evolution data in open-source repositories - commit histories, diff patches, and issue resolutions - and uses these as implicit demonstrations to fine-tune an LLM via RL. The insight is that by learning from how human developers iteratively improve code over many commits, an agent can internalize more realistic problem-solving trajectories. Early results from SWE-RL show improved reasoning and success rates on long-horizon software tasks, as the RL-trained model learns to recover intermediate reasoning steps (e.g. the decision to run tests or check certain files) that are often needed for complex bug fixes.

## B Context Summarization Ablations

The architect agent summarizes its working memory during execution when it exceeds given threshold; we evaluate whether the quality of context summarization affects downstream agent performance. In this ablation, we fix the summary *format*, and approximately keep the summary *length*, and summarization *cadence* in comparable ranges, and vary only the LLM used to generate summaries. All runs use the same main orchestrator (Claude 4 Sonnet) and are evaluated on SWE-Bench Pro. We compare Claude 3.5 haiku and Claude 4 Sonnet as summarizers. haiku is chosen as the 'weak' model because it reliably produces well-formed, schema-compliant summaries, while being substantially weaker in reasoning ability: on SWE-Bench, haiku achieves 40.6% versus Sonnet's 72.7%, and lacks extended multi-step reasoning, hence this choice avoids confounding effects from malformed summaries or instruction non-compliance.

We select 50 long-context instances in which summarization is triggered at least once during execution (up to five times in the longest cases). Under identical conditions, haiku-based summarization resolves 22/50 instances, while Sonnet-based summarization resolves 26/50. The observed gap directly demonstrates that higher-fidelity, reasoning-capable summaries provide better guidance for subsequent planning and lead to higher final resolve rates.

To illustrate why the quality of the summarization matters, here we provide summary examples of `instance_gravitational__teleport-db89206db6c2969266e664c7c0fb51b70e958b64` .

Here is the context summary of Claude Haiku 3.5:

```xml
<summary>

[CONVERSATION CONTEXT]
- Initial Requirements: Replace fatal error handling in TSH CLI commands with error return mechanisms
- Scope Changes: Modify command handlers to return errors instead of calling utils.FatalError
- User Preferences: Maintain existing logging and error handling patterns while improving test environment flexibility

[TECHNICAL DECISIONS]
- Architecture Decisions: Convert command handler functions to return errors
- Technology Stack: Go language, Teleport client library
- Design Patterns: Error wrapping using trace.Wrap(), trace.BadParameter()
- APIs/Interfaces: Modify function signatures in tool/tsh/tsh.go and lib/client/api.go

[IMPLEMENTATION PROGRESS]
- Completed Work:
  * Added SSOLoginFunc type
  * Modified ssoLogin method
  * Updated onSSH, onPlay command handlers
  * Started modifying onLogin and onLogout
- Current State: Partial implementation of error handling refactoring
- failed Attempts: Initial attempts to replace utils.FatalError with simple return statements
- Debugging History: systematic approach to replacing error handling across multiple files

[TECHNICAL Details]
- Data Structures: CLIConf, Config structs modified
- Algorithms: Error propagation and wrapping
- Edge Cases: Handling various error scenarios in CLI commands
- Performance Considerations: Minimal performance impact, improved testability

[OUTSTANDING ITEMS]
- Known Issues: Incomplete error handling in complex functions like onLogin
- Open Questions: How to handle early returns in long functions
- TODOs: Complete error handling for remaining command handlers

</summary>

<plan>
<thinking>
- Problem Analysis: Complex error handling refactoring across multiple files
- Impact Assessment: Improved testability and error management in Teleport CLI
- Solution Strategy: systematic conversion of command handlers to return errors
</thinking>
<issues>
1. utils.FatalError calls prevent programmatic error handling
2. Complex functions with multiple error points need careful refactoring
3. Function signatures must change to support error returns
</issues>
<next_step sequence_num="1">
Complete error handling conversion for onLogin function
- Replace all utils.FatalError calls with return trace.Wrap(err)
- Handle early return cases by returning nil or specific errors
- Ensure function signature returns error
</next_step>
```

Here is the summary of Claude Sonnet 4:

```xml
<summary>
[CONVERSATION CONTEXT]
- Initial Requirements: Fix TSH login and proxy address handling for test environments
  by making CLI commands return errors instead of exiting, supporting mock SSO login
  injection, and using runtime-assigned addresses for services bound to ':0'
- Scope Changes: None - requirements remained consistent throughout
- User Preferences: systematic approach with todo.md planning, focus on minimal changes,
  backward compatibility

[TECHNICAL DECISIONS]
- Architecture Decisions: Chose to implement error returns instead of process termination
  for all command handlers, added pluggable SSO login interface
- Technology Stack: Go, existing Teleport codebase structure maintained
- Design Patterns: Used function type for SSO login injection, trace.Wrap for error
  propagation
- APIs/Interfaces: Defined SSOLoginFunc type with specific signature matching existing
  ssoLogin method

[IMPLEMENTATION PROGRESS]
- Completed Work:
  * Phase 1: Added SSOLoginFunc type, MockSSOLogin field to Config struct, mockSSOLogin
    field to CLIConf struct
  * Partial Phase 2: Updated refuseArgs helper and several command handlers (onLogout,
    onListNodes, onListClusters, onSSH) to return errors
- Current State: In middle of Phase 2, about to update Run function in Phase 3
- failed Attempts: None significant
- Debugging History: None required yet

[TECHNICAL Details]
- Data Structures: Added SSOLoginFunc type, MockSSOLogin field to Config, mockSSOLogin
  field to CLIConf
- Algorithms: Error propagation using trace.Wrap, replacing utils.FatalError and
  os.Exit calls
- Edge Cases: Handling exit status preservation in SSH command, ambiguous host error
  handling
- Performance Considerations: Minimal impact, just changing error handling flow

[OUTSTANDING ITEMS]
- Known Issues: None identified yet
- Open Questions: Need to complete remaining command handlers and service address
  binding logic
- TODOs: Complete Phase 2 (remaining command handlers), Phase 3 (Run function),
  Phase 4 (SSO mock), Phase 5 (service binding), Phase 6 (validation)
</summary>

<plan>
<thinking>
- Current State: Successfully implemented Phase 1 (new types/interfaces) and partially
  completed Phase 2 (error handling for some command handlers)
- Architecture Implications: The error handling changes are fundamental and affect the
  entire CLI flow
- Risk Assessment: Need to ensure all command handlers are updated consistently and the
  Run function properly handles all error cases
</thinking>
<next_step sequence_num="1">
Update the Run function signature to return error and modify the switch statement to
capture errors from all command handlers, including updating the refuseArgs call for
logout and replacing utils.FatalError with proper error return
</next_step>
```

![Figure 4: Meta-agent build-test-improve loop](resources/figure_4.png)

**Figure 4** Meta-agent build-test-improve loop. The Confucius Code Agent (CCA) is a specific agent built on top of the Confucius SDK, with the help of the meta-agent. The Meta-agent synthesizes agent configurations, wires together orchestrator components and extensions, evaluates candidate agents on representative tasks, and iteratively refines prompts and tool-use policies based on observed failures.

The summary format and token count are almost identical between the 2 summaries, while Sonnet 4 appears much better in quality. The "[CONVERSATION CONTEXT]" of sonnet preserves maximum information: 'Fix TSH login + proxy handling for test environments by returning errors instead of exiting...", while the haiku summary 'Replace fatal error handling with error returns." loses important keywords. Most notably, Sonnet shows phases of work: it specifies state of the system (Phase 1 done, Phase 2 mid-way, next is Run function) , making progress legible and actionable..

## C Meta-Agent

### C.1 Implementation Details

As Figure 4 shows, the Meta-agent is implemented as an extra agent that interactively constructs new agents from high-level specifications based on the Confucius SDK. A developer begins by describing, in natural language, what the target agent should do and under what constraints (e.g., 'an agent that triages CI failures for our monorepo' or 'a refactoring agent with read-only access to production configs'). The Meta-agent then generates a structured configuration form that asks for more concrete requirements: repository scope, latency or safety constraints, which existing extensions (file editing, bash, code search, etc.) to attach, and what evaluation tasks or test suites should be used. After the user confirms this specification, the Meta-agent automatically (i) synthesizes the agent's configuration and prompts and (ii) wires in the selected extensions and memory policies. We present a brief illustration of this user interface in Appendix C.2.

Importantly, the Meta-agent also automates testing and debugging of the newly created agent. Using the same SDK runtime, it spins up the candidate agent locally, drives it on a suite of regression tasks (e.g., representative GitHub issues or internal tickets), and observes the agent's outputs, logs, and tool traces. When failures or undesirable behaviors are detected, such as brittle tool selection, incorrect file-edit patterns, or poor recovery from compiler errors, the Meta Agent proposes concrete modifications to prompts, extension configurations, or even new tool wrappers. These patches are applied to the agent, and the test loop is rerun, yielding a 'build-test-improve' process that incrementally improves the agent until target metrics are met. The same mechanism can be invoked not only to build new agents, but also to assist in designing and debugging new tools that plug into the extension layer.

### C.2 Development Cycle

The Confucius SDK promotes an easy-to-use agent development cycle where the meta-agent assist in onboarding and refining other agents. This iterative process of build-test-improve loop is further supported by a full suite of developer tools:

- Onboarding Experience: Meta-agent provides an easy-to-use interface for developers to create an agent from scratch, the onboarding experience offers multiple coding agent templates (orchestrators + extensions) and a multi-turn Q&A session with developer to clarify requirements, it also offers E2E testing capabilities, where user can on-the-fly build test cases and optimize the agent code/prompt.
- Trace UI: fine-grained visualization of call stacks, tool interactions, and memory flows, as shown in Figure 5
- Playground: an interactive environment for prompt refinement and parameter tuning;
- Eval UI: built-in support for regression tests, A/B comparisons, and benchmark evaluations;
- Centralized agent management: a unified interface for developing, integrating, deploying, and monitoring agents at scale.

*Tracing session for Task #1234567*

![Figure 5: CCA Trace UI](resources/figure_5.png)

**Figure 5** CCA Trace UI with call stack visualization and tool invocation Details, providing developers with detailed visibility into agent execution, showing the hierarchical call stack, latency metrics, token usage, and tool invocations for debugging and performance optimization.

## D Thinking Budget Scaling

We define the thinking budget as the maximum number of reasoning or chain-of-thought 'thinking' tokens that the LLM is permitted to generate before producing a response. Recent work shows that reasoning length can be controlled by prompting or internal budget-aware mechanisms. At inference time, the model can be guided either by: (i) prompt instructions like 'use at most X tokens of reasoning' ([Han et al., 2025](#ref-han-2025)); (ii) control tokens inserted periodically that signal remaining budget to the model during generation ([Wen et al., 2025](#ref-wen-2025)); or (iii) a predictor estimating task complexity and adjusting budget dynamically [Li et al. (2025)](#ref-li-2025).

Anthropic's Claude model series expose a thinkingBudget parameter at inference time to directly cap the reasoning tokens. We run our CCA agent using Claude 4 Sonnet with identical setup except varying its thinking budget in three settings: 8 k, 16 k, and 32 k tokens. We record Resolve Rate on a subset of SWE-Bench-Verified.

Table 6 CCA Resolve Rate as a function of thinking budget on SWE-Bench-Verified subset.

| Thinking Budget   |   Resolve Rate (Pass@1) |
|-------------------|-------------------------|
| 8k                |                    67.3 |
| 16k               |                    68.4 |
| 32k               |                    68.7 |

We observe diminishing returns beyond 16 k thinking tokens. Note that the thinkingBudget parameter cannot precisely control the internal thinking trace length of the Claude model, and that during inference, Claude only returns a summarized version of the reasoning without exposing full traces. As a result, we do not have the exact scaling curve of Resolve Rate with respect to the true thinking length on SWE-Bench-Verified.

## E Example Execution Trace

Below, we show an example execution trace when running CCA on a SWE-Bench-Pro example: `instance_tutao__tutanota-da4edb7375c10f...`

```
Running Confucius Code Agent with prompt from file: <PROMPT_FILE> <SYSTEM> Viewing directory at '<WORKDIR>' Directory content viewed at '<WORKDIR>', depth: 2, show_hidden: False <AI> The repository is in '<REPO_ROOT>'. Let me explore its structure: <SYSTEM> Viewing directory at '<REPO_ROOT_NAME>' Directory content viewed at '<REPO_ROOT_NAME>', depth: 2, show_hidden: False <AI> Let me search for 'EntityRestClient' and related files: <SYSTEM> Validating command 'find <REPO_ROOT> -type f -name "*.ts" | xargs grep -l "EntityRestClient" ... | head -20' Running command in '<WORKDIR>': find <REPO_ROOT> -type f -name "*.ts" | xargs grep -l "EntityRestClient" ... | head -20 Output: <REPO_ROOT>/src/api/common/EntityClient.ts <REPO_ROOT>/src/api/worker/rest/EntityRestClient.ts <REPO_ROOT>/src/api/worker/facades/BlobAccessTokenFacade.ts ... Viewing file at 'EntityRestClient.ts' File content viewed at 'EntityRestClient.ts', lines: None - None <AI> Now let me search for 'requestReadTokenArchive' and 'requestReadTokenBlobs' in the codebase: <SYSTEM> Validating command 'grep -rn "requestReadTokenArchive\|requestReadTokenBlobs" <REPO_ROOT>/src --include="*.ts" ...' Running command in '<WORKDIR>': grep -rn "requestReadTokenArchive\|requestReadTokenBlobs" <REPO_ROOT>/src --include="*.ts" ... Output: <REPO_ROOT>/src/api/worker/facades/BlobAccessTokenFacade.ts:... requestReadTokenBlobs(...) <REPO_ROOT>/src/api/worker/facades/BlobAccessTokenFacade.ts:... requestReadTokenArchive(...) <REPO_ROOT>/src/api/worker/rest/EntityRestClient.ts:... requestReadTokenArchive(..., listId) ... <SYSTEM> Viewing file at 'BlobAccessTokenFacade.ts' File content viewed at 'BlobAccessTokenFacade.ts', lines: None - None <AI>
```

```
Now let me look at the 'createBlobAccessTokenPostIn' and related types to understand the structure: <SYSTEM> Validating command 'grep -rn "createBlobAccessTokenPostIn\|BlobAccessTokenPostIn" <REPO_ROOT>/src --include="*.ts" ... | head -20' Running command in '<WORKDIR>': grep -rn "createBlobAccessTokenPostIn\|BlobAccessTokenPostIn" <REPO_ROOT>/src --include="*.ts" ... | head -20 Output: <REPO_ROOT>/src/api/entities/storage/TypeRefs.ts:... export type BlobAccessTokenPostIn = { ... } ... Viewing file at 'TypeRefs.ts' File content viewed at 'TypeRefs.ts', lines: None - None <AI> (Problem analysis) - 'requestReadTokenArchive' / 'requestReadTokenBlobs' currently require 'archiveDataType' - 'EntityRestClient' hardcodes 'ArchiveDataType.MailDetails' - Requirement: allow 'archiveDataType = null' for owned archives, keep behavior unchanged for non-owned archives ... <AI> Now let me check the git status to see if there are any changes already made: <SYSTEM> Validating command 'cd <REPO_ROOT> && git status' Command rejected (disallowed): 'cd ...' Validating command 'git -C <REPO_ROOT> status' Running command in '<WORKDIR>': git -C <REPO_ROOT> status Output: HEAD detached from <COMMIT_SHA> nothing to commit, working tree clean <AI> This is a moderately complex task that involves changes to multiple files. Let me plan this out systematically. <SYSTEM> Writing to memory node 'todo.md'... Created memory node 'todo.md' with ... characters and tags: ... <AI> Now let me create a reproduction script to understand the current behavior: <SYSTEM> Creating file at 'reproduce.py' File created successfully at 'reproduce.py' <AI> Now let's run the reproduction script to confirm the issue: <SYSTEM> Validating command 'python3 <REPO_ROOT>/reproduce.py' Running command in '<WORKDIR>': python3 <REPO_ROOT>/reproduce.py Output: Test 1: ... archiveDataType allows null? FAIL Test 2: ... method signatures accept null? FAIL Test 3: ... EntityRestClient passes null? FAIL Some tests failed. The issue needs to be fixed. <AI> Implementing the fix. <SYSTEM> Replacing content in file at 'TypeRefs.ts' Diff: - archiveDataType: NumberString; + archiveDataType: NumberString | null; <SYSTEM> Replacing content in file at 'BlobAccessTokenFacade.ts' Diff: - requestReadTokenBlobs(archiveDataType: ArchiveDataType, ...) + requestReadTokenBlobs(archiveDataType: ArchiveDataType | null, ...) - requestReadTokenArchive(archiveDataType: ArchiveDataType, ...) + requestReadTokenArchive(archiveDataType: ArchiveDataType | null, ...) <SYSTEM> Replacing content in file at 'EntityRestClient.ts'
```

```
Diff: - requestReadTokenArchive(ArchiveDataType.MailDetails, listId) + requestReadTokenArchive(null, listId) <SYSTEM> Replacing content in file at 'EntityRestClient.ts' Diff: - import { ArchiveDataType } from ".../TutanotaConstants.js" + (removed unused import) <AI> Now let's run the reproduction script again to verify the fix: <SYSTEM> Validating command 'python3 <REPO_ROOT>/reproduce.py' Running command in '<WORKDIR>': python3 <REPO_ROOT>/reproduce.py Output: Test 1: ... allows null, PASS Test 2: ... signatures accept null, PASS Test 3: ... passes null, PASS All tests passed! The fix is working correctly. <AI> Verifying diffs and committing. <SYSTEM> Validating command 'git -C <REPO_ROOT> diff --name-only' Running command in '<WORKDIR>': git -C <REPO_ROOT> diff --name-only Output: src/api/entities/storage/TypeRefs.ts src/api/worker/facades/BlobAccessTokenFacade.ts src/api/worker/rest/EntityRestClient.ts <SYSTEM> Validating command 'git -C <REPO_ROOT> add -A && git -C <REPO_ROOT> commit -m "<COMMIT_MSG>"' Running command in '<WORKDIR>': git -C <REPO_ROOT> add -A && git -C <REPO_ROOT> commit -m "<COMMIT_MSG>" Output: Committer: <ANON_NAME> <<ANON_EMAIL>> 3 files changed, ... insertions(+), ... deletions(-) <AI> Done. Changes preserve existing behavior for non-owned archives, while allowing owned-archive read token requests without archiveDataType.
```

## F Example Notes for Long-term Memory

Here is an example structured note hierarchy generated by note-taking agent for SWE-bench-Pro instance: `instance_internetarchive__openlibrary-f343c08f89c772f7ba...` .

```
.
+-- projects
|   +-- openlibrary
|       +-- escaping_wildcards_in_infobase_queries.md
|       +-- multi_stage_author_matching_pipeline.md
|       +-- year_based_author_matching_strategy.md
+-- README.md
+-- shared
    +-- python
    |   +-- dict_copy_forgotten_field_update.md
    +-- string_manipulation
        +-- prefix_removal_empty_string_edge_case.md
```

The note is well organized into "shared" (ONLY for truly generic insights that apply across MANY projects) and "project" (For project-specific knowledge that primarily applies to ONE specific domain). The README.md summarized all the notes md files and provide instructions on how to navigate them. Here are some insights from `escaping_wildcards_in_infobase_queries.md` .

```
---id: escaping_wildcards_in_infobase_queries title: Escaping Wildcards in Infobase Queries description: How to handle asterisk characters in \\ Infobase queries to prevent unintended wildcard matching keywords: - infobase - queries - wildcards - escaping - openlibrary ---# Escaping Wildcards in Infobase Queries ## Problem Context When searching for authors by name in OpenLibrary's Infobase, asterisk characters ('*') in author names can cause unexpected behavior because they are treated as wildcards by the query system. For example, searching for an author named "John*" would match "John Smith", "Johnny", etc., rather than the literal name "John*". ## The Solution Escape asterisks in name fields using a backslash when performing exact or alternate name matches, but **intentionally preserve wildcards for surname matching queries**. ### Code Example '''python # In find_author() function: # Escape asterisks for exact name matching escaped_name = author["name"].replace("*", r"\*") queries = [ # Use escaped name for exact match {"type": "/type/author", "name~": escaped_name}, # Use escaped name for alternate names {"type": "/type/author", "alternate_names~": escaped_name}, ] # For surname matching, intentionally use wildcards if birth_year and death_year: surname = author['name'].split()[-1] queries.append({ "type": "/type/author", "name~": f"* {surname}", # Wildcard intentional here "birth_date~": f"*{birth_year}*", "death_date~": f"*{death_year}*", }) ''' ## Key Insights 1. **Context-Dependent Escaping**: Wildcards need to be escaped in some query contexts (exact/alternate name matching) but preserved in others (surname matching). 2. **Backslash Escaping**: In Infobase queries, use 'r"\*"' to escape asterisks. The raw string 'r' prefix prevents Python from interpreting the backslash. 3. **Query Order Matters**: The queries are tried in sequence. Exact matches (with escaped wildcards) are tried first, then alternate names, then surname matching (with intentional wildcards). 4. **Creating New Records**: When no match is found and a new author record must be created, preserve the original name including any asterisks exactly as provided don't use the escaped version. ## Related Files - '/app/openlibrary/catalog/add_book/load_book.py' - 'find_author()' function - '/app/openlibrary/catalog/add_book/tests/test_load_book.py' - Tests for wildcard behavior
```

These insights saves the agent a lot of effort to rediscover the issue in Run 2. Here are the insights from `prefix_removal_empty_string_edge_case.md` .

```
---id: prefix_removal_empty_string_edge_case title: Prefix Removal Empty String Edge Case description: How to handle the edge case where removing a prefix leaves an empty string keywords: - string - manipulation - edge - case - prefix - validation ---# Prefix Removal Empty String Edge Case ## Problem When removing a prefix from a string, you may end up with an empty string if the input consists only of the prefix. This can cause unexpected behavior if downstream code doesn't handle empty strings properly. ### Example Scenario In OpenLibrary's honorific removal function, names like "Mr." or "Dr." consist entirely of an honorific: '''python def remove_author_honorifics(author: dict[str, Any]) -> dict[str, Any]: raw_name: str = author["name"] if honorific := find_matching_honorific(raw_name): author["name"] = raw_name[len(honorific):].lstrip() return author ''' **Problem**: If 'raw_name = "Mr."' and we remove "mr.", we get an empty string '""', which can cause issues in database queries or display logic. ## The Solution Check if the result is empty after removing the prefix, and if so, return the original value unchanged: '''python def remove_author_honorifics(author: dict[str, Any]) -> dict[str, Any]: """Remove honorifics from an author's name field.""" raw_name: str = author["name"] # Check for exceptions first (e.g., "Dr. Seuss") if raw_name.casefold() in HONORIFC_NAME_EXECPTIONS: return author if honorific := next( ( honorific for honorific in HONORIFICS if raw_name.casefold().startswith(honorific) ), None, ): new_name = raw_name[len(honorific):].lstrip() # If the name is only an honorific, return unchanged if not new_name: return author author["name"] = new_name return author ''' ## Key Insights 1. **Check Before Assignment**: Store the result in a temporary variable and validate it before assigning to the final location. 2. **Preserve Original on Invalid Result**: When validation fails (empty string), return the original input unchanged rather than the invalid result. 3. **Common in Text Processing**: This pattern applies to any string manipulation where you're removing parts of the string: - Removing file extensions - Removing URL protocols
```

This documents a concrete edge case when solving the instance; this not only saves the agent tokens in Run 2, but also avoids runs where in consecutive runs such edge cases could be missed, hence failing the case.

## G Case Studies: Comparison with Claude Code

In addition to standardized benchmarks, we conducted a controlled experiment using real GitHub issues from the PyTorch repository. These issues not only exemplify the complex challenges encountered in real-world production but also require deep domain specialist expertise. Hence, these issues reflect agent's robustness and generalization under specialist software engineering scenarios. Our experiment holds the model capabilities constant while varying only the agent framework. We compare CCA with Claude Code (CC) , a command-line tool developed by Anthropic that enables direct interaction with Claude models for coding tasks. Both frameworks utilize identical Claude Sonnet 4.5 models in environments with equivalent codebases and access to file manipulation, bash tools, and NVIDIA A100 80GB GPU resources. To compare the solution between CC and CCA, we have enlisted a few experts in this field to judge and compare the solutions created by the 2 agents.

### G.1 PyTorch-Bench

To construct PyTorch-Bench, we scanned GitHub issues on the open-source PyTorch repository ([PyTorch, 2025](#ref-pytorch-2025)) from Jan 2025 to Jul 2025. We selected 8 issues that are reproducible on an NVIDIA A100 80 GB GPU and that provide actionable structure, including a detailed description, a reproduction script, and instructions for replication of the issue. Both agents receive the same system prompt, which instructs them to start from a clean commit, attempt to reproduce the issue first, stop if reproduction fails in the current environment, and verify any proposed fix. We show an example task below and discuss it in more detail in the following sections.

```
Issue: RuntimeError: Expected curr_block->next == nullptr to be true, when I call setSegmentStateToCheckpoint. (#161356) URL: https://github.com/pytorch/pytorch/issues/161356 ### Describe the bug Hello, when I was using checkpoint state to implement shared output memory for two cudagraphs, an assert ERROR occurred: curr_block->next == nullptr, in function setSegmentStateToCheckpoint. ''' # PyTorch version: 2.6.0+cu124 torch._C._cuda_setCheckpointPoolState(com_device, small_state, [], output1_new_storage) RuntimeError: Expected curr_block->next == nullptr to be true, but got false. (Could this error message be improved? If so, please report an enhancement request to PyTorch.) ''' **This error only appears only when I set env 'export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True'. If I unset this env, the following case code executes successfully.** This is my test code: ''' import gc import torch print(f"torch version is: {torch.__version__}.") stream0 = torch.cuda.Stream() torch.cuda.set_stream(stream0) def tensor_metadata(x): return { "nbytes": x.untyped_storage().nbytes(), "data_ptr": x.untyped_storage().data_ptr(), "size": x.shape, "stride": x.stride(), "dtype": x.dtype, "device": x.device,
```

```
"storage_offset": x.storage_offset(),} def reconstruct_from_tensor_metadata(metadata): s = torch._C._construct_storage_from_data_pointer( metadata["data_ptr"], metadata["device"], metadata["nbytes"]) t = torch.empty([0], device=metadata["device"], dtype=metadata["dtype"]) t.set_(source=s, storage_offset=metadata["storage_offset"], size=metadata["size"], stride=metadata["stride"], ) return t def print_mem_stats(name): segments = torch.cuda.memory_snapshot() seg = [] for segment in segments: if "segment_pool_id" in segment: tmp = ({"stream": segment["stream"]}, {"pool_id": segment["segment_pool_id"]}, {"block_num": len(segment["blocks"])}, {"activate_num": sum(int(blk["state"] == "active_allocated") for blk in segment["blocks"])}, {"total_size": segment["total_size"]}, {"allocated_size": segment["allocated_size"]},) seg.append(tmp) seg_str = "\n".join([str(seg_iter) for seg_iter in seg]) seg_str = '\n' + seg_str print(f"{name}, snapshot: {seg_str}") def cudagraphify(fn, inputs, pool, stream): torch.cuda.synchronize() gc.collect() torch.cuda.empty_cache() graph = torch.cuda.CUDAGraph() with torch.cuda.graph(graph, stream=stream, pool=pool): static_outputs = fn(*inputs) return graph, static_outputs def foo(x, idx): r1 = x.expand([1, 2097152 // 8]).sqrt() r2 = x.expand([idx, 2097152]).clone() return (r1, r2) # init pool_id = torch.cuda.graph_pool_handle() com_stream = torch.cuda.Stream() com_device = torch.cuda.current_device() inp = torch.tensor([7], device=com_device) # record original state with torch.cuda.stream(com_stream): g = torch.cuda.CUDAGraph() g.capture_begin(pool=pool_id) g.capture_end() original_mem_state = torch._C._cuda_getCheckpointState(com_device, pool_id) # start capture graph1 graph1, outputs1 = cudagraphify(foo, [inp, 1], pool=pool_id, stream=com_stream) small_state = torch._C._cuda_getCheckpointState(com_device, pool_id) print_mem_stats("\n-----------------after_small_state_run_g0_step0 ") output1_metadata = [tensor_metadata(t) for t in outputs1] outputs1 = None # set to original state and capture graph2 torch._C._cuda_setCheckpointPoolState(com_device, original_mem_state, [], []) print_mem_stats("\n-----------------after_set_origin_state ") graph2, outputs2 = cudagraphify(foo, [inp, 2], pool=pool_id, stream=com_stream) biiig_state = torch._C._cuda_getCheckpointState(com_device, pool_id) print_mem_stats("\n-----------------after_biiig_state_run_g1_step0 ") output2_storage = [output.untyped_storage()._cdata for output in outputs2] # set to original state and replay graph1 torch._C._cuda_setCheckpointPoolState(com_device, original_mem_state, output2_storage, []) print_mem_stats("\n-----------------after_set_origin_state ") graph1.replay() reconstructed_tensors1 = [reconstruct_from_tensor_metadata(metadata) for metadata in output1_metadata] output1_new_storage = [output.untyped_storage()._cdata for output in reconstructed_tensors1] torch._C._cuda_setCheckpointPoolState(com_device, small_state, [], output1_new_storage) print_mem_stats("\n-----------------after_small_state_run_g0_step1 ") '''
```

```
After analyzing the error, the logic of the assertion seems to be: when allocing a new memory block, the last block must be an unmapped nullptr block. Therefore, when setting to the recorded CheckpointState, the last block should also be unmapped nullptr block. Why does it fail only when enabling expandable_segments? It seems that after enabling expandable_segments, the reserved memory blocks will be merged. Therefore, when allocing a small block, releasing this small block, and then allocing a large block again, the total reserved memory size increases. So, when setting to the state of a small memory block again, it triggers this assert error. Could you please help check if these failed validations are always necessary? And how the current checkpoint memory management interface can solve the above problems when expandable_segments is enabled? Thanks a lot. ### Versions Collecting environment information... PyTorch version: 2.6.0+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.3) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc-2.31 Python version: 3.11.4 (main, Jul 5 2023, 13:45:01) [GCC 11.2.0] (64-bit runtime) Python platform: Linux-5.15.0-72-generic-x86_64-with-glibc2.31 Is CUDA available: True CUDA runtime version: 12.9.41 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB GPU 1: NVIDIA A100-SXM4-80GB GPU 2: NVIDIA A100-SXM4-80GB GPU 3: NVIDIA A100-SXM4-80GB GPU 4: NVIDIA A100-SXM4-80GB GPU 5: NVIDIA A100-SXM4-80GB GPU 6: NVIDIA A100-SXM4-80GB GPU 7: NVIDIA A100-SXM4-80GB Nvidia driver version: 575.51.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True
```

### G.2 Issues and Comparative Solutions

We present three representative cases that demonstrate divergent agent behaviors:

### CUDA Memory Checkpoint Assertion Failure with Expandable Segments

As shown above, PyTorch Issue #161356 describes an error in PyTorch's CUDA graph checkpointing with expandable segments enabled. During a sequence that saves a small memory state, then a larger one, and restores the smaller state, PyTorch raises the error: `Expected curr_block->next == nullptr to be true`. Assertion failures occur in the allocator's checkpoint restoration logic, where terminal blocks in expandable segments are expected to have null next pointers.

Both frameworks identified the same underlying issue but proposed fundamentally different solutions. CCA viewed the assertions as overly restrictive and simply removed the problematic `TORCH_CHECK(curr_block->next == nullptr)` assertions (-2 lines), while preserving other essential safety checks. CC viewed the assertions as important architectural guardrails and, instead of removing them, added logic (+7 lines) to explicitly set `remaining->next = nullptr` during block splitting, effectively making expandable segments comply with the assertion. CCA favored minimal intervention, while CC pursued a more holistic solution. In this case, we note that the PyTorch team's eventual fix matched CCA's approach, providing human validation of CCA's principled engineering style.

### Excessive Memory Allocation and Deallocation during Llama-2 Training

PyTorch Issue #135837 highlights a memory allocation problem encountered during Llama-2 (70B) model Training. When GPU memory utilization approaches hardware limits, the allocator must decide whether to reclaim cached memory or retain it for performance optimization. On A100 GPUs (80GB), excessive memory deallocation and reallocation cycles occurred when reserved memory exceeded 70GB despite `expandable_segments=True` , resulting in significant Training slowdowns.

The issue was identified in PyTorch's CUDA allocator logic: even when the user explicitly enables expandable segments, the `release_cached_blocks()` function continues to unmap expandable segments, causing unintended memory deallocation. This created a contradiction between user intent (maintaining expanded memory) and system behavior (aggressive memory reclamation).

To solve this issue, CCA implemented a single guard clause (+6 lines) to fully disable memory reclamation when expandable segments are enabled, thus strictly adhering to the user's intent. In contrast, CC developed a more complex solution (+63 lines), dynamically measuring memory pressure and raising the reclamation threshold from 87.5% to 95%, while preserving emergency mechanisms above 95% to prevent out-of-memory errors. Both solutions effectively addressed the immediate issue but reflected contrasting philosophies. CCA identified and minimally fixed the problematic behavior, while CC addressed the broader memory management challenge with a more sophisticated approach.

### Neural Network Precision Regression during Weight Manipulation

PyTorch Issue #163072 involves a test failure where `test_partial_flat_weights` produces numerical accuracy errors on A100 and H100 GPUs, with relative differences of approximately 0.003 versus the expected tolerance of 1.3e-06. The test validates that partial flat weights (a memory optimization that stores select neural network layers in contiguous memory blocks) yield results equivalent to standard weight storage. The failure occurred during a sequence where LSTM weights were deleted, the module was moved to CUDA, and weights were manually restored.

Both CCA and CC correctly identified the root cause as inappropriate test tolerances for GPU operations and implemented the same adjustment: changing the tolerance parameters from exact equality to atol=1e-4, rtol=1e-4 (+1 line). This modification allowed the test to pass.

However, CC did not stop there. While confirming that the adjusted test passes, CC encountered a benign warning about non-contiguous memory layout. In response, CC implemented additional changes (+20 lines), modifying setattr in RNNBase to automatically call `flatten_parameters()` whenever weights are manually assigned. Ultimately, CC addressed not only the immediate test failure but also the underlying warning, resulting in a more comprehensive but potentially over-engineered solution.

### G.3 Architectural Analysis

This section analyzes the problem-solving behavior of CCA versus CC on Issue #161356. Figure 6 shows simplified traces of the solutions of both agents. Both employed similar high-level strategies: (1) initial reproduction attempts, (2) code exploration when reproduction failed, (3) root cause identification through systematic analysis.

Both agents also recognized the need for two directions of exploration: locating assertion failures in the CUDA memory allocator and understanding expandable segments logic. However, they perform these explorations differently:

1. Single-Agent: CCA performed explorations directly within the original context, maintaining awareness of the user's problem, system instructions, and previous observations. As such, CCA's explorations are subsequences of the overall reasoning chain.
2. Multi-Agent: CC delegates investigations to separate, stateless subagents. As shown in Figure 6, CC even executes one of the subagents concurrently, while the main agent is running the reproduction script. These agents do not access the main agent's context, but they are initialized with a detailed prompt that emphasizes thoroughness ('use a thorough approach to find all relevant files').

![Figure 6: Simplified traces for CCA and CC](resources/figure_6.png)

**Figure 6** Simplified traces for CCA and CC on PyTorch issue #161356

The architectural differences between CCA and CC significantly influenced solution characteristics. CCA's solution was simpler and more cautious, whereas CC was more ambitious and overengineered its solution. CC opted for this solution because it was the suggestion returned by the subagent, which was tasked with performing an exhaustive analysis. The subagent's mandate for thoroughness, combined with its lack of the original context, leads it to over-analyze the problem and provide a more complex solution than is necessary. The main agent, trusting the subagent's expertise, implements the solution despite preferring simpler solutions independently. This highlights a fundamental challenge in multi-agent systems. Generally, subagents separate concerns and allow the main agent to focus on its main task. However, our analysis suggests that for well-scoped debugging tasks, the benefits of delegation may be outweighed by the risk of context loss and derailment via inter-agent misalignment.

## H Future Work

Recent advances suggest that reinforcement learning (RL) can substantially enhance LLM-based software engineering agents beyond what is achievable with supervised fine-tuning alone. For example, SWE-RL ([Wei et al., 2025](#ref-wei-2025)) demonstrates meaningful gains through end-to-end RL with verifiable rewards, while frameworks such as Agent Lightning ([Luo et al., 2025](#ref-luo-2025)) highlight a crucial architectural insight: by viewing agent execution as a Markov Decision Process, RL training can be decoupled from agent implementation via unified trajectory interfaces. This decoupled perspective aligns naturally with the design philosophy behind CCA. The Agent Experience (AX) framework already structures an agent's internal reasoning traces in a trajectory-friendly format, making them directly suitable for RL training. Moreover, CCA's Meta-agent produces rich, fine-grained feedback signals from both tool extensions and environment interactions-signals that can be transformed into diverse reward functions for outcome-based, process-based, or hybrid supervision. Examples include rewards tied to note-taking quality, tool-use robustness, recovery behaviors, or the efficiency of multi-step exploration. In addition, the extensibility of the Confucius Orchestrator provides a natural substrate for curriculum design in RL. Agent builders can introduce progressively richer toolsets, varied execution environments (e.g., shell, file editing, SQL databases), and increasingly complex tasks, enabling models to acquire generalizable agentic capabilities rather than overfitting to specific tool behaviors. This opens the door to RL-driven improvement not only of individual policies, but of the broader agent stack itself.
