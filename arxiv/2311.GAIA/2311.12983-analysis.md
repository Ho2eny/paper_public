---
title: "GAIA: A Benchmark for General AI Assistants - Analysis"
arxiv_id: "2311.12983"
source: "./2311.12983.md"
type: analysis
created: 2026-01-09
paper_type: Benchmark
venue: "ICLR 2024"
---

# GAIA: A Benchmark for General AI Assistants

**Analysis Document** | [Original Paper](./2311.12983.md) | [HuggingFace Leaderboard](https://huggingface.co/gaia-benchmark)

---

## 1. Overview

GAIA(General AI Assistants)는 범용 AI 어시스턴트를 평가하기 위한 벤치마크로, 기존 AI 평가의 패러다임을 근본적으로 전환한다. 전통적 벤치마크들이 "인간에게 어려운 과제가 AI에게도 어렵다"는 가정 하에 설계된 반면, GAIA는 **인간에게는 개념적으로 단순하지만(92% 성공률) AI에게는 도전적인(GPT-4+plugins ~15%) 466개의 실세계 질문**을 제안한다 [Section 1](./2311.12983.md#1-introduction).

이 벤치마크는 Meta FAIR, HuggingFace, AutoGPT 연구진이 협력하여 개발했으며, ICLR 2024에서 발표되었다. 핵심 철학은 Proof of Work 알고리즘에서 영감을 받아, **해결은 복잡하지만 검증은 단순한** 과제를 설계하는 것이다. 질문들은 웹 브라우징, 멀티모달 이해, 코딩, 다양한 파일 형식 처리 등의 근본적 능력을 테스트하며, 단일하고 명확한 정답을 가져 자동화된 평가가 가능하다. 저자들은 GAIA를 해결하는 시스템이 t-AGI(시간 제한 AGI)의 중요한 이정표가 될 것이라고 주장한다.

---

## 2. Core Section

### 2.1 TL;DR
> **한 줄 요약**: 인간에게 쉽고(92%) AI에게 어려운(15%) 466개 질문으로 구성된 범용 AI 어시스턴트 벤치마크

[Extended TL;DR](./2311.12983-details/tldr.md)

### 2.2 Core Contributions

| # | Contribution | Impact |
|---|-------------|--------|
| 1 | 새로운 벤치마크 철학 (인간에게 쉬움 != AI에게 쉬움) | 평가 패러다임 전환 |
| 2 | 466개 고품질 질문 데이터셋 (3단계 난이도) | 실용적 평가 도구 |
| 3 | 포괄적 평가 프레임워크 (자동화된 스코어링) | 재현 가능한 평가 |
| 4 | 확장 가능한 질문 생성 방법론 | 커뮤니티 확장 가능 |
| 5 | AGI 진척도 측정 프레임워크 (t-AGI 연결) | 장기적 목표 제시 |

[Contributions Deep Analysis](./2311.12983-details/contributions.md)

### 2.3 Key vs Non-Key Sections

**Must Read (P1):**
- Section 1: Introduction - 핵심 동기와 철학
- Section 3.1: Design - 4가지 설계 원칙
- Section 4: Results - 주요 실험 결과 (Figure 4, 5)

**Recommended (P2):**
- Section 3.3: Composition - 데이터셋 구성과 능력 분포
- Section 5: Discussion - 시사점과 미래 방향

**Optional (P3-P4):**
- Section 2: Related Work - 배경 지식 있으면 스킵 가능
- Section 3.4, 6, Appendix - 재현/확장 시에만 필요

[Section Reading Guide](./2311.12983-details/key-sections.md)

---

## 3. Paper Type Analysis

### Classification: **Benchmark Paper**

**Benchmark 특성:**
- **평가 대상**: 범용 AI 어시스턴트 (Tool-augmented LLMs)
- **데이터셋**: 466개 질문, 3단계 난이도
- **평가 방식**: Quasi-exact match, 자동화된 스코어링
- **공개**: 개발 세트 166개 + 테스트 세트 300개 (리더보드)

**핵심 특징:**
1. **역발상적 설계**: 인간에게 쉬운 과제 선택
2. **실세계 기반**: 웹, 파일, 멀티모달 통합
3. **게이밍 저항**: 단일 정답, 추론 추적 가능
4. **확장성**: 질문 생성 가이드라인 제공

[Methodology Deep Dive](./2311.12983-details/methodology.md)

---

## 4. Visual Analysis

### 4.1 Key Figures

**Figure 1: 난이도별 질문 예시** [Section 1](./2311.12983.md#1-introduction)
```
Level 1: NIH 웹사이트에서 임상시험 등록자 수 찾기
         → 정답: 90 (웹 검색 + 정보 추출)

Level 2: 아이스크림 버터팻 함량 vs 미국 연방 기준 비교
         → 정답: +4.6 (이미지 분석 + 웹 검색 + 계산)

Level 3: NASA 사진 속 우주비행사 그룹 중 최소 우주 체류 시간 계산
         → 정답: White; 5,876 (다중 웹 검색 + 추론 + 계산)
```

**Figure 4: 모델별 성능 비교** [Section 4](./2311.12983.md#4-llms-results-on-gaia)
```
                Level 1    Level 2    Level 3
Human           93.9%      91.8%      87.3%
GPT-4+plugins   30.3%       9.7%       0.0%
AutoGPT         14.4%       0.4%       0.0%
GPT-4 Turbo     13.0%       5.5%       0.0%
GPT-4            9.1%       2.6%       0.0%
```
**인사이트**: 도구 통합이 성능을 3배 향상시키지만, 인간과의 격차는 여전히 큼

**Figure 5: 능력별 성능** [Section 4](./2311.12983.md#4-llms-results-on-gaia)
- GPT-4 alone: 파일/멀티모달 처리 불가
- GPT-4+plugins: 웹 브라우징에서 가장 큰 향상
- 비도구 모델도 일부 웹 질문 해결 (사전 학습 지식 활용)

### 4.2 Key Tables

**Table 4: 상세 성능 및 소요 시간** [Appendix D.1](./2311.12983.md#d1-extended-evaluation)

| 모델 | L1 Score | L1 Time | L2 Score | L2 Time |
|-----|----------|---------|----------|---------|
| Human | 93.9% | 6.8분 | 91.8% | 10.5분 |
| GPT-4+plugins | 30.3% | 0.65분 | 9.7% | 0.53분 |
| Search Engine | 7.4% | 7.4분 | 0% | N/A |

**인사이트**: GPT-4+plugins는 빠르지만 정확도 낮음, 인간-AI 협업이 최적

### 4.3 Mathematical Formulations

GAIA는 수학적 모델보다는 평가 프로토콜에 집중. 핵심 개념:

**난이도 정의:**
```
Level(q) = f(num_steps, num_tools, complexity)
  where:
    Level 1: steps <= 5, tools <= 1
    Level 2: 5 < steps <= 10, tools > 1
    Level 3: steps > 10 or complex navigation
```

**평가 메트릭:**
```
Score = (1/N) * Σ I[normalize(pred_i) == normalize(gt_i)]
  where:
    I[.] = indicator function
    normalize() = type-specific normalization
```

---

## 5. Critique & Related Works

### 5.1 Strengths

1. **패러다임 전환**: 기존 "어려운 과제 = 좋은 벤치마크" 가정을 뒤집음
2. **실용성**: 실제 AI 어시스턴트 사용 사례 반영
3. **자동 평가**: 단일 정답으로 객관적 평가 가능
4. **확장성**: 명확한 가이드라인으로 커뮤니티 확장 가능
5. **투명성**: 166개 질문 + 스코어링 함수 공개

### 5.2 Limitations

1. **추론 과정 미평가**: 최종 답변만 평가, 중간 단계 무시
2. **높은 어노테이션 비용**: 질문당 ~2시간 필요
3. **언어 제한**: 영어 전용 (비영어권 80% 인구 미대응)
4. **재현성 문제**: 플러그인 가용성 변동, API 변화
5. **정적 특성**: 웹 콘텐츠 변화로 인한 질문 무효화 가능

### 5.3 2026년 관점에서의 평가

**현재 상황 (2026년 초):**
- GPT-4o, Claude 3.5 Sonnet 등 최신 모델들이 GAIA에서 상당한 진전을 보임
- 웹 에이전트 (WebAgent, Agent-E) 연구가 활발히 진행 중
- Computer Use API 등 새로운 도구 통합 방식 등장

**GAIA의 지속적 가치:**
- 여전히 Level 3는 도전적 (완벽 해결 미달성)
- 실세계 기반 평가의 표준으로 자리잡음
- AGI 벤치마크로서의 상징적 의미 유지

**보완 필요 영역:**
- 다국어 확장 필요
- 안전성/윤리성 평가 통합 필요
- 동적 질문 생성 메커니즘 필요

### 5.4 Related Works

**선행 벤치마크:**
- MMLU, HELM - 정적 지식 평가 [Hendrycks et al., 2021; Liang et al., 2022]
- AgentBench - 폐쇄형 환경 에이전트 평가 [Liu et al., 2023]
- ToolQA, APIBench - 도구 사용 평가 [Zhuang et al., 2023; Patil et al., 2023]

**후속 연구 방향:**
- WebArena, OSWorld - 실제 환경 웹 에이전트 평가
- SWE-bench - 코드 에이전트 평가
- 멀티에이전트 협업 벤치마크

---

## Navigation

| Document | Description |
|----------|-------------|
| [Original Paper](./2311.12983.md) | 전체 논문 원문 |
| [TL;DR](./2311.12983-details/tldr.md) | 확장 요약 |
| [Contributions](./2311.12983-details/contributions.md) | 기여 심층 분석 |
| [Key Sections](./2311.12983-details/key-sections.md) | 섹션 읽기 가이드 |
| [Methodology](./2311.12983-details/methodology.md) | 방법론 상세 분석 |

---

**Citation:**
```bibtex
@inproceedings{mialon2024gaia,
  title={GAIA: a benchmark for General AI Assistants},
  author={Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Swift, Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  booktitle={International Conference on Learning Representations},
  year={2024}
}
```
