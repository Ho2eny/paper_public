---
title: "ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark - Analysis"
arxiv_id: "2408.04682"
type: "analysis"
paper_type: "benchmark"
analyzed_date: "2026-01-10"
source: "./2408.04682.md"
details: "./2408.04682-details/"
venue: "arXiv 2024 (Apple Research)"
---

# ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark - Analysis

## 1. Overview

**1문단: 필요성**
이 논문은 Tool-use LLM을 **stateful, conversational, interactive** 환경에서 평가하는 벤치마크를 제안한다. 기존 벤치마크들(BFCL, ToolEval)은 **stateless** RESTful API 기반이거나, **단일 턴** 프롬프트만 평가하거나, **off-policy** 대화 궤적에 의존한다. 하지만 실제 task-oriented dialog는 world state가 존재하고, 도구 간 **implicit dependency**가 있으며, 사용자와의 대화형 상호작용이 필요하다. 예를 들어 cellular service가 꺼져있을 때 메시지를 보내려면 먼저 cellular를 켜야 한다.

**2문단: 설계**
ToolSandbox는 세 가지 핵심 특징을 갖는다: (1) **Stateful Tools** - 44%의 도구가 world state(cellular, wifi, location service, low battery mode)에 의존하며 implicit dependency chain을 형성, (2) **LLM User Simulator** - GPT-4o 기반 on-policy 대화형 평가로 realistic 시나리오 재현, (3) **Milestone/Minefield Evaluation** - 고정된 궤적이 아닌 동적 평가로 다양한 성공 경로 허용. 34개 도구, 11개 도메인, 1,032개 테스트 시나리오를 포함한다.

**3문단: 평가**
13개 모델(7개 proprietary + 6개 open-source) 평가 결과, GPT-4o가 **73.0%**로 최고 성능을 보였고, Claude-3-Opus가 69.2%로 뒤를 이었다. 특히 **State Dependency** 카테고리에서 대형 모델(GPT-4, Claude-3-Opus)이 중소형 모델(GPT-3.5, Claude-3-Sonnet)보다 **낮은** 성능을 보여, erroneous parallel tool call 문제를 드러냈다. **Canonicalization**과 **Insufficient Information** 카테고리도 모든 모델에게 도전적이었다.

**4문단: 의의**
ToolSandbox는 tool-use 평가에서 **state dependency**와 **interactive evaluation**의 중요성을 처음으로 체계적으로 제시한다. Open-source와 proprietary 모델 간 **20점 이상의 격차**를 보여주며, 특히 State Dependency, Canonicalization, Insufficient Information이 SOTA 모델에게도 여전히 도전적임을 밝혔다. Apple에서 공개한 이 벤치마크는 realistic tool-use 평가의 새로운 기준을 제시한다.

---

## 2. Core Section

### TL;DR

> **Stateful tool dependency** + **LLM user simulator** + **Milestone-based dynamic evaluation**을 결합한 tool-use 벤치마크로, 1,032개 테스트 시나리오에서 GPT-4o가 73%로 최고 성능을 보이며, State Dependency와 Canonicalization이 SOTA 모델에게도 핵심 도전 과제임을 밝힌다.

→ 상세: [tldr.md](./2408.04682-details/tldr.md)

### Core Contributions

1. **Implicit State Dependencies**: Cellular/WiFi/Location service 등 world state 간 dependency chain 모델링 → 실제 환경의 복잡성 반영
2. **LLM User Simulator**: Knowledge Boundary + Demonstration 프롬프팅으로 hallucination 최소화 → On-policy conversational evaluation 가능
3. **Milestone/Minefield Evaluation**: DAG 기반 동적 평가로 다양한 성공 경로 허용 → Trajectory-agnostic 평가
4. **Challenging Categories**: State Dependency, Canonicalization, Insufficient Information 정의 → SOTA 모델의 한계 진단

→ 상세: [contributions.md](./2408.04682-details/contributions.md)

### Key vs Non-Key Sections

| Priority | Sections | Reason |
|----------|----------|--------|
| ⭐⭐⭐ Must Read | Section 2 (Design), Table 1 | 3가지 핵심 특징 (Stateful, Conversational, Interactive) |
| ⭐⭐⭐ Must Read | Section 4 (Results), Table 5 | 모델별 성능 및 카테고리별 분석 |
| ⭐⭐ Important | Section 3 (Test Scenarios) | 6가지 시나리오 카테고리 정의 |
| ⭐⭐ Important | Section 2.2 (User Simulator), Table 2-3 | User simulator 설계와 error rate |
| ⭐ Reference | Appendix A (Implementation) | Execution Context, Tools 상세 |
| Skip | Appendix C (Example Trajectories) | 개별 사례 (필요시 참조) |

→ 상세: [key-sections.md](./2408.04682-details/key-sections.md)

---

## 3. Paper Type

**Type**: Benchmark

| Aspect | Value |
|--------|-------|
| **Evaluation Target** | Tool-use LLM in Stateful, Conversational Environment |
| **Task Count** | 1,032 test scenarios |
| **Domains** | Contact, Messaging, Reminder, Settings, Time, Math, Map, Weather, Stock, Conversion, Holiday (11) |
| **Tools** | 34 tools (44% stateful) |
| **Main Metrics** | Milestone Similarity Score (0-1), Turn Count |

→ 상세 방법론: [methodology.md](./2408.04682-details/methodology.md)

---

## 4. Visual Analysis

### Key Figures

#### Figure 1: Example Evaluation Trajectory

**구성 요소**:
- **Message Bus**: User, Agent, Execution Environment 간 대화 기록
- **World State**: 각 턴에서의 데이터베이스 스냅샷 (Settings, Contacts, Messages)
- **Milestones**: 달성해야 할 핵심 이벤트 (DAG 구조)

**핵심 통찰**:
- 사용자가 "메시지 보내줘" 요청 시 cellular service가 꺼져있음
- Agent는 (1) contacts 검색 → (2) cellular 켜기 → (3) 메시지 전송 순서로 진행해야 함
- **Milestone matching**: 순서 유지하며 최적 궤적 평가

**Source**: [Figure 1](./2408.04682.md#figure-1-an-example-evaluation-trajectory-from-toolsandbox)

---

#### Figure 3: Insufficient Information Example (Minefield)

**구성 요소**:
- **상황**: 현재 타임스탬프를 알 수 없어 태스크 완료 불가
- **Minefield**: `timestamp_diff` 호출 시 0점 (hallucination 탐지)
- **정답 행동**: "할 수 없다"고 응답

**핵심 통찰**:
- GPT-4가 타임스탬프를 **hallucinate**하여 `timestamp_diff` 호출
- Minefield 매칭 → 전체 점수 0점
- **Insufficient Information 처리 능력**이 중요

**Source**: [Figure 3](./2408.04682.md#figure-3-example-gpt-4-trajectory-for-insufficient-information)

---

### Tables Interpretation

#### Table 5: Main Results by Category

| Model | Avg Score | State Dep | Canon | Insuff Info |
|-------|-----------|-----------|-------|-------------|
| GPT-4o | **73.0** | 84.0 | 76.6 | **42.0** |
| Claude-3-Opus | 69.2 | 74.5 | 71.1 | **57.3** |
| GPT-3.5-Turbo | 65.6 | **82.6** | 70.4 | 22.3 |
| Gemini-1.5-Pro | 60.4 | 70.5 | 51.6 | **76.2** |
| Hermes-2-Pro (Open) | 31.4 | 27.1 | 19.9 | 48.3 |

**주요 발견**:
1. **State Dependency 역설**: 대형 모델(GPT-4, Opus)이 중형(GPT-3.5, Sonnet)보다 **낮음** → Parallel call 오류
2. **Insufficient Info 역상관**: 강한 모델일수록 hallucination 위험 높음 (Gemini 76.2% 예외)
3. **Open vs Proprietary Gap**: 20점+ 격차, 특히 State Dependency (0~27% vs 70%+)
4. **Canonicalization 공통 난점**: 상대적 시간, 위치→좌표 변환 등

**실무적 의미**:
- Parallel tool call 기능 사용 시 dependency 체크 필수
- Insufficient Information 시나리오에서 명시적 guardrail 필요

**Source**: [Table 5](./2408.04682.md#table-5-comparing-the-average-similarity-score)

---

#### Table 2 & 3: User Simulator Error Rate

| Setup | Hallucination | Instruction Following |
|-------|---------------|----------------------|
| User Goal only | 12.4% | 6.2% |
| + Knowledge Boundary | 7.75% | 3.88% |
| + Demonstration | **6.97%** | **0.77%** |

**핵심 통찰**:
- **Knowledge Boundary**: 유저가 알아야/몰라야 할 것 명시 → Hallucination 감소
- **Demonstration**: Few-shot 예시로 instruction following 개선
- 최종 에러율 ~8% → 모델 간 비교에 미치는 영향 일정

**Source**: [Table 2](./2408.04682.md#table-2-percentage-of-user-simulation-failures), [Table 3](./2408.04682.md#table-3-percentage-of-user-simulation-failures)

---

## 5. Critique & Related Works

### Expert Critique

#### Strengths
1. **State Dependency 최초 체계화**: Implicit dependency chain 평가 → 실제 환경 복잡성 반영
2. **On-policy Evaluation**: LLM user simulator로 interactive 평가 가능
3. **Flexible Milestone System**: 다양한 성공 경로 허용 → Off-policy 한계 극복
4. **Detailed Error Analysis**: State Dependency, Canonicalization, Insufficient Info 세분화

#### Limitations
1. **LLM User Simulator 한계**: 여전히 ~8% 에러율, 복잡한 상황에서 불안정
2. **도메인 제한**: 34개 도구, 11개 도메인 → 실제 production보다 좁은 범위
3. **Reproducibility**: 일부 도구가 외부 API 의존 (RapidAPI)
4. **Milestone Annotation 비용**: 전문가 수작업 필요, 확장성 한계

#### Adoption Status
- [x] Open source (GitHub 공개)
- [ ] Widely used (specialized benchmark)
- [ ] Clear leaderboard (논문 내 결과만)
- [x] Active maintenance (Apple Research)

#### 2026 Perspective
- **Still Valid**: State dependency 개념, Milestone 평가 방식
- **Outdated**: 평가 모델 (GPT-4o 초기 버전)
- **Missing**: Multi-agent 시나리오, 더 복잡한 dependency chain, Vision 통합

### Related Works

1. **BFCL** (Berkeley, 2024) - De facto standard for function calling → Stateless, ToolSandbox와 상호보완
2. **ToolBeHonest** (2024) - Hallucination diagnostic → Insufficient Information과 유사 목적
3. **τ-bench** (Princeton, 2024) - Tool-Agent-User interaction → ToolSandbox와 유사하지만 28 tools로 제한
4. **MultiWOZ** (2018) - Dialogue State Tracking classic → Explicit state vs ToolSandbox의 implicit state
5. **AgentBoard** (2024) - Multi-turn agent evaluation → ToolSandbox가 state dependency에서 더 깊이

---

## Navigation

- **Source**: [원본 논문](./2408.04682.md)
- **Details**:
  - [TL;DR 상세](./2408.04682-details/tldr.md)
  - [Contributions 상세](./2408.04682-details/contributions.md)
  - [Key Sections 상세](./2408.04682-details/key-sections.md)
  - [Methodology 상세](./2408.04682-details/methodology.md)
