---
title: "ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent Dialogue Synthesis"
arxiv_id: "2410.18447"
authors:
  - "Zezhong Wang"
  - "Xingshan Zeng"
  - "Weiwen Liu"
  - "Liangyou Li"
  - "Yasheng Wang"
  - "Lifeng Shang"
  - "Xin Jiang"
  - "Qun Liu"
  - "Kam-Fai Wong"
published: "2024-10-24"
updated: "2025-03-17"
categories:
  - "cs.CL"
url: "https://arxiv.org/abs/2410.18447"
pdf: "https://arxiv.org/pdf/2410.18447.pdf"
converted_date: "2026-01-09"
translated: "Korean"
---

# ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent Dialogue Synthesis

## 저자

Zezhong Wang, Xingshan Zeng, Weiwen Liu, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong

## 초록

Supervised fine-tuning (SFT)은 Large Language Models (LLMs)의 tool calling 능력을 향상시키는 일반적인 방법이며, 학습 데이터는 주로 합성을 통해 생성된다. 현재의 데이터 합성 과정은 일반적으로 tool 집합을 sampling하고, 이러한 tool들을 기반으로 요구사항을 수립한 후, call statement를 생성하는 방식으로 진행된다. 그러나 무작위로 sampling된 tool들은 연관성이 부족하여 결합하기 어렵고, 결과적으로 데이터의 다양성이 감소한다. 또한, 현재 연구는 dialogue turn 간의 coherence를 간과하여 합성된 데이터와 실제 시나리오 사이에 격차가 발생한다. 이러한 문제를 해결하기 위해, 우리는 더 연관성 있는 tool 조합을 sampling하기 위한 Graph-based Sampling 전략과 coherent한 dialogue 합성을 안내하는 계획을 생성하는 Planned-generation 전략을 제안한다. 이 두 전략을 통합하고 여러 agent가 상호작용하며 dialogue 데이터를 합성할 수 있도록 하여, tool-calling 데이터 합성 pipeline인 ToolFlow를 개발하였다. 데이터 품질 평가 결과, 합성된 dialogue의 자연스러움과 coherence가 개선되었음을 보여준다. 마지막으로, ToolFlow로 생성된 8,000개의 합성 dialogue를 사용하여 LLaMA-3.1-8B에 SFT를 적용하였다. 결과적으로 해당 model은 강력한 일반 능력을 유지하면서 GPT-4와 비슷하거나 그 이상의 tool-calling 성능을 달성하였다.

---

## 저자 및 소속

**Zezhong Wang**<sup>[1](#aff-1)[\*](#fn-star)</sup>, **Xingshan Zeng**<sup>[2](#aff-2)[+](#fn-dagger)</sup>, **Weiwen Liu**<sup>[2](#aff-2)</sup>, **Liangyou Li**<sup>[2](#aff-2)</sup>, **Yasheng Wang**<sup>[2](#aff-2)</sup>, **Lifeng Shang**<sup>[2](#aff-2)</sup>, **Xin Jiang**<sup>[2](#aff-2)</sup>, **Qun Liu**<sup>[2](#aff-2)</sup>, **Kam-Fai Wong**<sup>[1](#aff-1)</sup>

<a id="aff-1"></a>**[1]** The Chinese University of Hong Kong

<a id="aff-2"></a>**[2]** Huawei Noah's Ark Lab

**연락처:** {zzwang,kfwong}@se.cuhk.edu.hk | {zeng.xingshan,liuweiwen8,liliangyou}@huawei.com | {wangyasheng,Shang.Lifeng,Jiang.Xin,qun.liu}@huawei.com

## 1 서론

Large Language Models (LLMs)가 tool calling을 수행할 수 있도록 하면 그 능력과 실용적 응용이 크게 향상된다. 이를 위해서는 model이 강력한 이해력, reasoning 능력, 그리고 instruction-following 능력을 갖추어야 한다. 맞춤형 fine-tuning은 LLMs의 tool-calling 능력을 향상시키는 데 널리 사용되는 방법이다 [[1]](#ref-1)[[15]](#ref-15)[[19]](#ref-19)[[16]](#ref-16). 그러나 fine-tuning 데이터에 대한 접근이 제한될 수 있다. 한 가지 실행 가능한 해결책은 LLMs를 활용하여 데이터를 합성하는 것이다 [[3]](#ref-3)[[25]](#ref-25)[[26]](#ref-26)[[29]](#ref-29).

<a id="fn-star"></a>**[\*]** Huawei Noah's Ark Lab 인턴십 기간 중 수행된 연구.

<a id="fn-dagger"></a>**[+]** 교신저자

일반적인 tool-calling 데이터 합성 과정은 세 단계로 구성된다: (1) 후보 tool 선택, (2) 해당 tool들을 기반으로 요구사항 생성, (3) call statement 생성 [[22]](#ref-22)[[12]](#ref-12). 그러나 이 방법으로 합성된 데이터는 종종 현실성과 자연스러움이 부족하다. 무작위로 sampling된 tool들은 서로 연결되지 않는 경우가 많아 복잡한 task를 위해 결합하기 어렵다. 결과적으로, 후속 합성을 위한 요구사항은 단순해지는 경향이 있으며, 이는 데이터의 다양성과 복잡성을 감소시킨다. 더욱이, 기존 연구의 대부분은 single-turn tool-calling instruction 생성에만 집중하고 dialogue turn 간의 coherence를 무시한다 [[16]](#ref-16)[[27]](#ref-27). 실제 상호작용에서 LLMs는 일반적으로 single-round Q&A가 아닌 dialogue를 통해 사용자와 소통한다. 이는 Q&A 유형의 학습 데이터와 실제 적용 사이에 격차를 만들어 합성된 데이터의 자연스러움을 궁극적으로 저하시킨다.

이러한 두 가지 과제를 해결하기 위해, 우리는 TOOLFLOW를 제안한다. 이는 선택된 tool 간의 상관관계를 향상시키는 graph-based sampling algorithm과 합성된 tool call dialogue의 자연스러움 및 coherence를 향상시키는 planned-generation 전략을 사용하는 tool-calling 데이터 합성 pipeline이다.

구체적으로, 우리는 유사한 parameter나 return value를 가진 tool들이 서로 관련되어 있다고 본다. 예를 들어, "book\_flight"와 "get\_weather"는 모두 위치와 관련된 parameter를 필요로 한다. 실제 시나리오에서 이 두 tool은 여행 context에서 자주 함께 사용되므로 실제로 상호 연결되어 있다. 이러한 가정을 바탕으로, tool들의 parameter와 return value 간의 유사성을 나타내는 tool graph를 구성한다. graph의 각 node는 tool을 나타내고, edge는 tool 쌍 간의 관련성을 나타낸다. tool을 sampling할 때, 이 tool graph에서 subgraph를 무작위로 선택하여 sampling된 tool들이 효과적으로 상호작용할 가능성이 높도록 하며, 이를 통해 복잡한 요구사항 생성을 용이하게 한다.

한편, dialogue를 합성하기 전에 먼저 LLM이 선택된 tool subset을 기반으로 계획을 생성하도록 한다. 이 계획은 사용자가 dialogue의 각 turn에서 해야 할 요청을 개요로 제시한다. 계획을 구성하는 동안 model은 문구나 세부 사항에 신경 쓰지 않고 dialogue framework를 수립하는 데 집중한다. 이러한 접근 방식을 통해 model은 요구사항 간의 논리적 관계와 상호작용에 집중할 수 있으며, 결과적으로 더 coherent한 요구사항이 생성된다. 또한, LLM이 계획에 non-tool-call 요청을 포함할 수 있도록 한다. 이는 대화 내용의 다양성을 향상시킬 뿐만 아니라 주제 간의 자연스러운 전환을 촉진하여 새로운 요구사항으로 자연스럽게 이어지게 한다.

우리는 User, Assistant, Tool의 세 agent를 사용하여 dialogue를 생성한다. 선택된 tool subset과 수립된 계획을 기반으로, 이 agent들이 상호작용하여 dialogue를 완성한다. "sampling-planning-generation" 과정을 반복하여 총 8,000개의 dialogue를 합성하였다. 제안된 방법의 효과를 평가하기 위해, 동일한 크기의 dialogue를 이러한 module 없이 선택적으로 생성하여 graph-based sampling과 planning 전략에 대한 포괄적인 ablation study를 수행하였다. 데이터 품질에 대한 철저한 평가를 수행하였으며, TOOLFLOW가 생성된 dialogue의 자연스러움, coherence, 다양성을 효과적으로 향상시킬 수 있음을 보여준다. 마지막으로, 합성된 데이터를 사용하여 LLaMA-3.1-8B-Instruct [[5]](#ref-5)에 supervised fine-tuning을 적용하고 TOOLFLOW를 통해 model의 일반 능력을 유지하면서 tool-calling 능력이 개선되었음을 검증하였다.

우리의 기여는 다음 세 가지 핵심 포인트로 요약된다:

- 합성 tool calling 요구사항의 다양성과 복잡성을 향상시키기 위해 관련 tool을 선택하는 Graph-based Sampling 전략을 제안한다.
- 합성 dialogue의 자연스러움과 coherence를 향상시키기 위한 Planned-Generation 전략을 도입한다.
- 이 두 전략을 통합하여 tool calling dialogue 데이터 합성

pipeline인 TOOLFLOW를 제안하며, 광범위한 실험과 분석을 통해 그 효과를 보여준다.

## 2 관련 연구

외부 tool과 large language models (LLMs)의 통합은 기능적 범위를 크게 확장하여 복잡한 문제에 대해 더 전문적이고 정확하며 신뢰할 수 있는 해결책을 제공할 수 있게 한다 [[16]](#ref-16). LLMs에 tool-use 능력을 내장하는 데에는 일반적으로 두 가지 주요 전략이 있다: prompt-based 방법과 tool-augmented SFT. Prompt-based 방법은 추가 학습 없이 prompt에 tool의 설명과 예시를 제공하여 LLMs가 tool을 활용할 수 있게 한다 [[18]](#ref-18)[[8]](#ref-8)[[13]](#ref-13). ReAct [[28]](#ref-28)는 이 범주 내에서 주목할 만한 방법 중 하나이다. 이는 LLMs가 reasoning과 action 실행 사이를 전환하며 어려운 task를 처리할 수 있게 한다. 그러나 이러한 접근법의 효과는 model의 고유한 능력에 의해 제한될 수 있다. 반면, tool-augmented tuning은 LLM의 tool 사용 능력을 직접적으로 향상시키기 때문에 점점 더 많은 관심을 받고 있다 [[1]](#ref-1)[[15]](#ref-15)[[19]](#ref-19)[[16]](#ref-16). tool calling dataset의 가용성이 제한적이기 때문에, Basu et al. [[3]](#ref-3)은 다양한 다른 domain의 데이터를 tool calling 연구에 적용하였다. 다른 연구들 [[12]](#ref-12)[[22]](#ref-22)[[16]](#ref-16)은 주로 기본적인 tool calling 요구사항을 포함하는 single-turn instruction을 합성하였다. 그러나 LLMs는 일반적으로 single-turn Q&A가 아닌 dialogue를 통해 사용자와 상호작용한다. 이러한 불일치는 데이터가 부자연스러워 실제 시나리오와의 격차를 만든다. 우리의 TOOLFLOW는 데이터 합성에서 dialogue의 coherence와 자연스러움을 향상시키는 데 초점을 맞추어 실제 응용에 더 부합하도록 한다.

## 3 방법론

현실적이고 coherent한 dialogue를 생성하기 위해, 세 단계의 데이터 합성 과정을 제안한다: (1) graph-based sampling을 사용한 tool subset 선택; (2) 선택된 tool subset을 기반으로 dialogue 계획 생성; (3) tool subset과 dialogue 계획에 따라 dialogue 합성.

## 3.1 Tool 선택을 위한 Graph-based Sampling

Tool calling 데이터 합성은 일반적으로 사용 가능한 toolset에서 하나 이상의 tool을 선택하는 것으로 시작된다. 이전 연구의 대부분은

Figure 1: dialogue 합성 pipeline. 왼쪽은 tool을 나타내는 파란색 박스와 parameter 또는 return value를 나타내는 보라색 박스가 있는 Tool Graph를 보여준다. 중간에는 sampling된 tool에 따라 생성된 dialogue 합성 계획이 있다. 오른쪽은 User, Assistant, Tool agent에 의한 데이터 합성 예시이다.

<!-- image -->

무작위 sampling에만 의존하며 tool 선택의 중요성을 간과하지만 [[16]](#ref-16)[[15]](#ref-15), 선택된 tool은 합성된 dialogue의 품질을 형성하는 데 중요한 역할을 한다. 실제 시나리오에서 사용자 요구사항은 종종 해결책을 달성하기 위해 여러 tool의 결합된 사용을 필요로 한다. 더 복잡한 사용자 요구를 합성하려면 선택된 tool이 함께 작동할 수 있도록 하는 것이 중요하다. 이를 해결하기 위해, 관련성 있고 호환 가능한 tool 조합을 식별하는 Graph Sampling 전략을 제안한다.

## 3.1.1 Tool Graph 구성

먼저 graph $G = (V, E)$를 구성한다. 여기서 node $v_i \in V$는 tool $i$를 나타내고, edge $e_{i,j} \in E$는 tool $i$와 tool $j$가 관련되어 있는지를 나타낸다. Figure 1의 왼쪽은 tool graph의 예시를 보여준다. 우리는 유사한 parameter나 return value를 가진 tool들이 서로 관련되어 있다고 본다:

P-P Similarity: 두 tool이 유사한 parameter를 공유할 때, tool들이 관련되어 있을 가능성이 높다. 예를 들어, 의미적으로 유사한 두 parameter인 "location"과 "destination"을 기반으로 "get\_weather"와 "book\_flight"와 같은 tool을 식별할 수 있으며, 이들은 여행 관련 context에서 자주 함께 사용된다.

P-R Similarity: 한 tool의 return value가 다른 tool의 input parameter와 유사한 경우에도 두 tool이 관련되어 있을 가능성이 높다. 예를 들어, "check\_calendar" tool은 일반적으로 이벤트의 위치를 반환하고, "navigate" tool은 위치를 input으로 필요로 한다. 사용자가 "오후 회의 장소로 네비게이션"을 요청하면 두 tool 모두 호출된다.

parameter나 return value 간의 유사도를 도출하기 위해, 먼저 "{Name}: {Description}" template을 사용하여 parameter나 return value의 이름과 설명을 연결한다. 예를 들어, 특정 tool의 'Date' parameter는 "Date: Departure date, format as dd/mm/yyyy."로 표현된다. 그런 다음, 이러한 문자열을 Sentence-BERT [[17]](#ref-17)를 사용하여 encoding하여 해당 embedding을 얻는다. $p_i^k$와 $r_i^l$을 각각 tool $v_i$의 $k$번째 parameter와 $l$번째 return value로 표시한다. 그리고 $\mathbf{p}_i^k$와 $\mathbf{r}_i^l$을 각각 $p_i^k$와 $r_i^l$의 embedding으로 표시한다. $v_i$의 parameter $p_i^k$와 $v_j$의 parameter $p_j^l$ 간의 유사도는 다음과 같이 정의된다:

$$\text{sim}(p_i^k, p_j^l) = \cos(\mathbf{p}_i^k, \mathbf{p}_j^l)$$

마찬가지로, $v_i$의 return value $r_i^k$와 $v_j$의 parameter $p_j^l$ 간의 유사도는 $\cos(\mathbf{r}_i^k, \mathbf{p}_j^l)$로 정의된다. 유사도가 미리 정의된 threshold $\tau$보다 크면 두 tool이 상관관계가 있다고 본다. 우리의 예비 연구에 따라 $\tau$를 0.82로 설정하였다. 이는 두 tool의 parameter 쌍 간의 유사도가 $\tau$를 초과하거나, 두 tool의 return value와 input parameter 간의 유사도가 $\tau$를 초과할 때 두 tool 사이에 edge를 할당함을 의미한다:

$$e_{i,j} = \begin{cases} 1 & \text{if } \exists p_i^k \subseteq v_i, p_j^l \subseteq v_j : \text{sim}(p_i^k, p_j^l) > \tau \\ & \text{or } \exists r_i^k \subseteq v_i, p_j^l \subseteq v_j : \cos(\mathbf{r}_i^k, \mathbf{p}_j^l) > \tau \\ 0 & \text{otherwise} \end{cases}$$

여기서 $i, j = 1 \cdots N$이고 $i \neq j$이다. $\subseteq$는 parameter나 return value가 tool에 포함되어 있음을 나타낸다.

## 3.1.2 Graph-based Sampling

구성된 tool graph를 통해 상관관계가 있을 수 있는 $n$개의 tool로 구성된 subset을 sampling할 수 있다. 일반적으로 graph에서 node를 무작위로 시작점으로 선택한 다음, graph의 edge를 따라 random walk를 수행한다. 경로

**Algorithm 1: Graph-based Sampling**

| | |
|---|---|
| **Require:** | $G = (V, E)$ with $\|V\| = N$, integer $n \leq N$ (desired sample size) |
| **Ensure:** | Subset $\hat{V} \subseteq V$ with $\|\hat{V}\| = n$ |

```
1:  // V에서 node를 무작위로 선택하여 V_hat에 추가
2:  V_hat <- {Uniform(V)}
3:  while |V_hat| < n do
4:      v_i <- last element of V_hat
5:      // v_i의 모든 이웃 찾기
6:      N(v_i) <- {v_j in V | e_{i,j} = 1, for all e_{i,j} in E}
7:      // v_i의 이웃 중 하나를 무작위로 선택
8:      v_j <- Uniform(N(v_i))
9:      if v_j not in V_hat then
10:         // v_j가 아직 포함되지 않았으면 V_hat에 추가
11:         V_hat <- V_hat union {v_j}
12:     end if
13: end while
14: return V_hat
```

길이가 $n$에 도달하면 멈추고, 경로에 포함된 node들이 sampling된 tool subset을 구성한다. 자세한 내용은 Algorithm 1에 나와 있다.

## 3.2 Dialogue 계획 생성

Coherent한 dialogue는 일반적으로 현재 tool calling이 이전 call의 return value에 의존하거나 현재 tool calling을 위한 parameter가 이미 dialogue history에 존재하는 경우와 같은 복잡한 tool-calling 시나리오를 포함한다. 더욱이, 인간과 AI assistant 간의 현실적인 dialogue는 항상 tool 사용을 필요로 하지 않으며, tool 기반 task 사이에 chitchat과 같은 non-tool 관련 교환이 자주 포함된다. 이러한 현실적인 시나리오에서 LLM의 성능을 향상시키려면 이러한 균형을 반영하는 학습 예시가 필수적이다.

이러한 필요성을 해결하고 합성된 dialogue의 coherence를 향상시키기 위해, 생성 전에 계획을 수립하는 Planned-Generation 전략을 제안한다. LLM이 먼저 tool subset을 기반으로 사용자 요구사항 집합을 공식화하여 dialogue 계획을 생성하도록 한다. 이러한 요구사항은 tool call 요청(이러한 tool의 사용을 필요로 하는 task) 또는 chitchat과 같은 non-tool 상호작용을 포함할 수 있다. Figure 1의 중간 부분은 계획의 예시를 보여준다. 이 단계에서 LLM은 상호작용의 문구나 다른 뉘앙스에 대해 깊이 들어가지 않고 요구사항의 논리, coherence, 자연스러운 흐름에만 집중한다. 직접 합성된 dialogue와 비교하여, dialogue 계획을 기반으로 생성된 dialogue는 coherence가 현저히 개선되었다. 이 coherence에 대한 자세한 평가는 후속 섹션에서 제공한다. 계획 합성 prompt는 Table 14를 참조하라.

## 3.3 Multi-Agent Dialogue 합성

dialogue를 협력적으로 합성하기 위해 LLMs로 user, assistant, tool의 세 agent를 설정한다. Figure 1의 오른쪽은 하나의 dialogue turn에 대한 합성 과정을 보여준다.

user agent는 dialogue 계획에 따라 요청을 시작하는 역할을 한다. 먼저 계획의 현재 요청이 완료되었는지 확인하여 현재 task를 계속할지 다음으로 넘어갈지 결정한다. 이를 통해 dialogue가 계획의 흐름과 순서에 맞게 유지된다.

assistant agent는 사용자의 요청을 평가하여 tool이 필요한지 결정한다. chitchat과 같이 tool이 필요하지 않은 경우 assistant agent가 직접 응답한다. tool call이 필요한 경우, assistant는 tool documentation을 기반으로 모든 필수 parameter가 있는지 확인한다. parameter가 누락된 경우 assistant는 사용자에게 설명을 요청하고, 그렇지 않으면 tool call statement를 생성한다.

tool agent는 tool documentation과 assistant의 call statement를 기반으로 요청된 tool의 return value를 시뮬레이션한다.

세 agent 간의 상호작용은 계획의 모든 요청이 처리되거나 미리 설정된 turn 한도에 도달할 때까지 각 dialogue turn마다 계속된다. 그 후 모든 dialogue turn이 수집되고 저품질 데이터를 제거하기 위해 rule-based 데이터 필터링 module이 적용된다 [[12]](#ref-12)[[11]](#ref-11). 필터링 규칙은 주로 tool call statement의 형식과 불완전한 dialogue나 누락된 tool call turn과 같은 기타 문제를 확인한다.

## 3.4 구현 세부사항

이 연구에서 tool 선택을 위해 ToolBench [[16]](#ref-16)의 tool(16,000개 이상의 RESTful API 포함)을 사용 가능한 tool로 직접 활용한다. tool 설명을 표준화하기 위해 OpenAI Function Calling 1의 설정을 따르고 LLM (Llama-3.1-8B)에 prompt하여 이러한 모든 tool을 JSON 형식으로 변환한다. parameter 설명 누락과 같이 정보가 불완전한 경우, 변환 과정에서 LLM이 누락된 세부 사항을 추론하고 채우도록 한다. 예시 tool은 Figure 2에 제시되어 있다.

별도로 명시하지 않는 한, dialogue 계획 생성 및 agent 시뮬레이션을 포함한 모든 생성 작업에 GPT-4 [[14]](#ref-14)를 사용한다.

1 https://platform.openai.com/docs/guides/functioncalling

Table 1: TOOLFLOW와 ablation 설정으로 합성된 dialogue dataset의 기본 정보. # token, # call, # call turn은 각각 dataset의 token 수, tool call 수, tool call을 포함하는 turn 수를 나타낸다.

| 설정   | 설정   | 통계   | 통계   | 통계   |
|-----------|-----------|--------------|--------------|--------------|
| Graph     | Plan      | # tokens     | # call       | # call turn  |
| ✓         | ✓         | 8,054,298    | 21,069       | 17,112       |
| ✓         | ✗         | 8,145,545    | 25,158       | 21,504       |
| ✗         | ✓         | 7,956,087    | 18,117       | 15,931       |
| ✗         | ✗         | 8,069,304    | 23,301       | 20,804       |

다양성을 향상시키기 위해 각 dialogue에 대해 여러 버전의 GPT-4가 무작위로 선택된다.

## 4 데이터 품질 평가

## 4.1 기본 데이터 정보

이 섹션에서는 합성 데이터의 품질을 평가한다. Graph-based Sampling (Graph라고 함)과 Planned-Generation 전략 (Plan이라고 함)의 효과를 평가하기 위해, 다른 조건에서 세 세트의 추가 비교 데이터를 합성하였다: Graph 제거, Plan 제거, Graph와 Plan 모두 제거. 각 dataset은 8,000개의 dialogue를 포함한다. Table 1은 이러한 dataset의 총 token 수, tool call 수, tool call을 포함하는 dialogue turn 수를 보여준다.

Table 1에서 보듯이, 다른 전략에 의해 생성된 총 token 수는 약 800만으로 유사하다. Planned-Generation 전략을 사용하여 합성된 dialogue는 더 많은 non-tool 상호작용을 포함하여 tool call 요청의 비율이 낮다. 반면, Graph-based Sampling 전략은 tool call 수를 증가시킨다. 이는 tool 간의 연결 때문으로, 후속 tool call에 대한 관련 정보가 일반적으로 dialogue history에 포함되어 있어 누락된 정보를 요청하는 추가 turn의 필요성이 줄어든다.

## 4.2 품질 평가

합성된 dialogue의 품질을 더 평가하기 위해 자동 평가와 model 기반 평가를 모두 구현하였다.

자동 평가는 주로 dialogue의 coherence와 다양성을 평가한다. Dziri et al. [[6]](#ref-6)를 따라, dialogue의 coherence를 Natural Language Inference (NLI) task로 평가한다. dialogue의 두 연속 turn을 각각 premise와 hypothesis로 취급하고, entailment 관계의 비율 (EnR)과

Table 2: TOOLFLOW와 ablation 설정으로 합성된 데이터에 대한 자동 평가 및 GPT-4 평가 결과.

| 자동 평가   | 자동 평가   | 자동 평가   | 자동 평가   | 자동 평가   | 자동 평가   |
|------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|
| 설정                | 설정                | Coherence              | Coherence              | Diversity              | Diversity              |
| Graph                  | Plan                   | SS                     | EnR                    | H                      | D-3                    |
| ✓                      | ✓                      | 63.36                  | 47.3                   | 10.36                  | 0.4865                 |
| ✓                      | ✗                      | 62.03                  | 32.1                   | 10.14                  | 0.4364                 |
| ✗                      | ✓                      | 61.72                  | 48.1                   | 9.82                   | 0.3393                 |
| ✗                      | ✗                      | 58.65                  | 35.4                   | 9.75                   | 0.3078                 |
| GPT-4 평가       | GPT-4 평가       | GPT-4 평가       | GPT-4 평가       | GPT-4 평가       | GPT-4 평가       |
| Graph                  | Plan                   | NAT                    | COH                    | HELP                   | ACC                    |
| ✓                      | ✓                      | 3.72                   | 3.91                   | 4.71                   | 4.92                   |
| ✓                      | ✗                      | 3.00                   | 3.72                   | 4.51                   | 4.84                   |
| ✗                      | ✓                      | 3.51                   | 3.88                   | 4.39                   | 4.87                   |
| ✗                      | ✗                      | 2.93                   | 3.66                   | 4.18                   | 4.90                   |

그들 사이의 semantic similarity (SS)를 계산한다. turn 간의 EnR 또는 SS가 높을수록 dialogue가 더 coherent함을 나타낸다.

다양성과 관련하여, 단어 빈도를 기반으로 텍스트의 Shannon entropy ($H$)를 계산한다 [[20]](#ref-20). 또한 dataset에 대해 $N = 3$ (D-3)인 Distinct-N Score [[9]](#ref-9)를 계산한다. entropy나 Distinct-N Score가 높을수록 dataset이 더 많은 정보를 포함하고 더 큰 다양성을 가짐을 나타낸다.

또한, model 기반 평가를 위해 각 dataset에서 200개의 dialogue를 무작위로 sampling하였다. GPT-4 [[14]](#ref-14)를 사용하여 naturalness (NAT), coherence (COH), helpfulness (HELP), accuracy (ACC)의 네 가지 차원에 따라 각 dialogue를 신중하게 평가하였다. GPT-4 평가를 위한 prompt는 Table 15에 나와 있다.

평가 결과는 Table 2에 나와 있다. 두 가지 주요 관찰 사항이 있다:

- H와 D-3 Score는 Graph Sampling이 데이터의 다양성을 향상시킴을 보여준다.
- 두 평가 (SS, EnR, COH) 모두 Planned-generation이 dialogue의 coherence를 향상시킴을 보여준다.

더 자세한 설정, 분석 및 설명은 Appendix A.1을 참조하라.

## 4.3 자연 Dialogue Dataset과의 비교

우리의 합성 dialogue가 인간이 만든 것과 어떻게 비교되는지 더 잘 이해하기 위해, 기존 dataset과의 비교 연구를 수행하였다. 비교를 위한 자연 dialogue dataset으로 MultiWOZ dataset [[4]](#ref-4)을 선택하였다. MultiWOZ (Multi-Domain Wizard-of-Oz)는

Table 3: TOOLFLOW와 MultiWOZ의 평가 점수 비교.

| Dataset   |   NAT |   COH |   HELP |   ACC |
|-----------|-------|-------|--------|-------|
| TOOLFLOW  |  3.72 |  3.91 |   4.71 |  4.92 |
| MultiWOZ  |  3.98 |  4.03 |   4.41 |  4.95 |

잘 알려진 task-oriented dialogue dataset이며, 이 dataset과의 비교가 설득력 있을 것이라 믿는다. MultiWOZ에서 200개의 dialogue를 무작위로 sampling한 후 GPT-4 평가 실험을 반복하였다. 그런 다음 동일한 점수 매기기 prompt를 사용하여 GPT-4가 네 가지 차원에서 이러한 dialogue를 평가하도록 하였다. 결과는 Table 3에 나와 있다.

MultiWOZ는 ToolFlow dataset에 비해 naturalness, coherence, accuracy에서 약간 높은 점수를 받았지만 차이는 미미하다 (평균 점수 차이 0.1-0.2). helpfulness 점수와 관련하여 ToolFlow는 MultiWOZ보다 0.3점 높았다. 이러한 결과는 TOOLFLOW의 합성 dialogue가 MultiWOZ의 인간이 만든 dialogue와 비슷한 품질을 달성하며, 특히 helpfulness와 같은 task-oriented 측면에서 강력한 성능을 보임을 시사한다.

## 5 실험

## 5.1 설정

## 5.1.1 Datasets

TOOLFLOW로 학습된 model의 tool call 능력을 검증하기 위해 다음 세 가지 tool-calling dataset에서 실험을 수행하였다.

- **BFCL-v2** [[15]](#ref-15)는 주로 Python 스타일 tool call 데이터로 구성되며 Simple, Multiple, Parallel, Parallel Multiple의 네 가지 범주로 나뉜다. 버전 2는 동적인 실제 시나리오에서 더 많은 데이터를 추가한다. 통계적으로 안정적이고 평가하기 쉬운 Abstract Syntax Tree (AST)로 평가할 수 있는 범주를 선택하였다. accuracy가 보고된다.
- **API-Bank** [[10]](#ref-10)는 dialogue 스타일 tool call dataset으로, Call과 Retrieve + Call의 두 가지 설정을 포함한다. model은 dialogue에서 사용자 요구사항에 따라 미리 정의된 로컬 Python tool을 호출해야 한다. tool return value가 ground truth와 일치하는지 평가하여 accuracy를 측정한다.
- **ToolAlpaca** [[22]](#ref-22)는 multi-agent simulator를 구축한다. GPT-4 [[14]](#ref-14)를 활용하여 tool의 return value를

시뮬레이션한다. model은 return value (예: error message)를 기반으로 수정하고 tool을 다시 호출할 수 있다. 마지막으로 GPT-4가 Process와 Response의 accuracy를 평가한다.

또한, 일반 성능의 변화를 검토하기 위해 MMLU [[7]](#ref-7), BBH [[21]](#ref-21), MTBench [[31]](#ref-31)를 사용하여 model의 reasoning 및 대화 능력을 평가하였다.

## 5.1.2 Models

주요 실험에서는 LLaMA-3.1-8B-Instruct [[5]](#ref-5)를 base model로 사용하여 TOOLFLOW로 생성된 합성 dialogue의 효과를 검토한다. 간략하게, 이 논문의 나머지 부분에서는 fine-tuned model을 TOOLFLOW라고 지칭한다. 비교한 model에는 GPT-3.5, GPT-4, GPT-4o [[14]](#ref-14), Claude [[2]](#ref-2), LLaMA-3.1 [[5]](#ref-5) 등과 Lynx-7B [[10]](#ref-10), ToolAlpaca-7B [[22]](#ref-22) 등 평가된 dataset 논문의 baseline이 포함된다. 특정 checkpoint 정보는 실험 결과 표를 참조하라.

## 5.2 결과

## 5.2.1 TOOLFLOW는 GPT-4o와 비슷한 tool-calling 능력을 달성한다.

BFCL에서 TOOLFLOW의 tool-calling 능력을 평가하였다. 이 dataset은 네 가지 범주의 질문을 포함한다. Simple 범주에서 각 질문은 하나의 tool을 포함하며, LLM은 요구사항에 따라 올바르게 호출해야 한다. Multiple 질문은 2-4개의 tool을 포함하며, model이 가장 적합한 것을 선택하고 호출해야 한다. Parallel 범주에서는 한 turn에서 여러 tool을 호출해야 한다. Multiple Parallel은 Parallel 설정에 혼란을 주는 후보 tool을 추가한다.

결과는 Table 4에 나와 있다. 전반적으로 TOOLFLOW는 GPT-4o와 비슷한 성능을 달성한다. Non-Live subset에서 TOOLFLOW는 GPT-4와 GPT-4o를 능가했지만 Claude-3.5-Sonnet보다 약간 약했다. Live subset에서 TOOLFLOW는 여전히 이러한 선도적인 closed-source LLMs에 뒤처진다. 이는 Live subset이 실제 세계에서 사용자가 기여한 더 많은 test case를 추가하여 model에게 더 도전적이기 때문이다. 이 격차는 TOOLFLOW가 8B parameter만 가지고 있다는 점에서 주로 model 크기의 차이에 기인한다. ablation 실험은 Graph-based Sampling과

Table 4: BFCL-v2 leaderboard 결과 (2024년 08/16에 업데이트). "Non-Live"와 "Live"는 각각 v1과 v2 subset의 결과를 나타낸다. 표 값은 백분율로 표시된다. 각 범주에서 최고 결과는 **굵게** 표시된다. 우리 model의 최고 결과는 밑줄이 그어진다.

|                   |                   | Non-Live   | Non-Live   | Non-Live   | Non-Live          | Live   | Live     | Live     | Live              | Overall   | Overall   | Overall   |
|-------------------|-------------------|------------|------------|------------|-------------------|--------|----------|----------|-------------------|-----------|-----------|-----------|
| Baselines         | Baselines         | Simple     | Multiple   | Parallel   | Parallel Multiple | Simple | Multiple | Parallel | Parallel Multiple | Non-Live  | Live      | Overall   |
| Claude-3.5-Sonnet | Claude-3.5-Sonnet | 88.55      | 95.00      | 91.50      | 92.50             | 86.82  | 80.06    | 81.25    | 45.83             | 91.22     | 80.75     | 85.98     |
| GPT-4-turbo-0409  | GPT-4-turbo-0409  | 87.45      | 96.50      | 91.00      | 89.00             | 87.98  | 84.14    | 100.00   | 79.17             | 90.28     | 85.02     | 87.65     |
| GPT-4o-0513       | GPT-4o-0513       | 80.55      | 91.00      | 90.00      | 83.00             | 81.78  | 77.24    | 87.50    | 75.00             | 85.02     | 78.20     | 81.61     |
| LLaMA-3.1 8B      | LLaMA-3.1 8B      | 90.36      | 89.50      | 73.50      | 73.50             | 74.03  | 73.31    | 56.25    | 54.17             | 83.40     | 72.88     | 78.14     |
| Graph Plan        | Graph Plan        |            |            |            |                   |        |          |          |                   |           |           |           |
| ✓                 | ✓                 | 92.18      | 92.50      | 90.00      | 85.00             | 73.64  | 75.22    | 75.00    | 70.83             | 90.30     | 74.83     | 82.57     |
|                   | ✓ ✗               | 90.73      | 93.00      | 89.50      | 84.50             | 71.71  | 74.35    | 75.00    | 66.67             | 89.60     | 73.71     | 81.66     |
| Ours ✗            | ✓                 | 91.45      | 93.00      | 88.00      | 84.00             | 70.93  | 74.73    | 68.75    | 66.67             | 89.50     | 73.78     | 81.64     |
| ✗                 | ✗                 | 92.36      | 91.50      | 86.50      | 82.50             | 72.48  | 73.58    | 75.00    | 62.50             | 89.00     | 73.18     | 81.09     |

Generated Plan을 모두 포함하는 전략으로 합성된 데이터로 학습된 model이 가장 좋은 성능을 보임을 보여준다. 이는 특히 Parallel Multiple 유형 질문에서 두드러진다.

다른 model이나 학습 전략이 특정 범주에서 약간의 variance를 보이지만, 평균 성능의 차이는 유의미하지 않다. 따라서 추가 tool call dataset에서 추가 비교 실험을 수행하였다.

## 5.2.2 TOOLFLOW는 dialogue 데이터에서 SOTA를 달성한다.

BFCL은 Q&A 형식으로 model의 tool calling 능력을 테스트한다. 그러나 우리는 대화 형식이 실제 응용 시나리오에 더 가깝다고 믿는다. 반면, 우리의 합성 학습 데이터도 dialogue 형식이다. 따라서 BFCL 테스트에서 TOOLFLOW의 장점이 충분히 입증될 수 없다.

API-Bank는 dialogue dataset이다. 평가 중 model은 사용자 요구를 받은 후 tool call 요청을 하고 tool의 return value를 기반으로 응답을 제공해야 한다. 이 과정은 단일 dialogue 내에서 여러 번 발생할 수 있다. Call과 Retrieve + Call의 두 가지 테스트 설정을 포함한다. Call 설정에서 assistant는 사용자 요청을 충족하기 위해 후보 tool set에서 tool을 선택한다. Retrieve + Call 설정에서 assistant는 검색 tool에만 접근할 수 있다. assistant는 먼저 관련 tool을 검색한 다음 호출해야 한다.

Table 5의 결과에서 TOOLFLOW는 state-of-the-art 평균 accuracy를 달성한다. Call 설정에서 TOOLFLOW는 모든 baseline을 능가한다. retrieve + call 설정에서 TOOLFLOW는 GPT-3.5-turbo에는 뒤처지지만 GPT-4를 포함한

Table 5: API-Bank Dataset 결과. L1과 L2는 각각 Call과 Retrieve + Call 설정을 나타낸다. 표 값은 백분율로 표시된다. †는 원본 논문에서 도출된 결과를 나타낸다.

| Baselines             | Baselines             | L1    | L2    | Avg.   |
|-----------------------|-----------------------|-------|-------|--------|
| GPT-4-turbo-0409      | GPT-4-turbo-0409      | 72.43 | 39.26 | 55.85  |
| GPT-4o-0513           | GPT-4o-0513           | 76.19 | 42.96 | 59.58  |
| GPT-3.5-turbo-0125    | GPT-3.5-turbo-0125    | 70.43 | 52.59 | 61.51  |
| Lynx-7B †             | Lynx-7B †             | 49.87 | 30.37 | 40.12  |
| Llama-3.1-8B-Instruct | Llama-3.1-8B-Instruct | 71.18 | 37.04 | 54.11  |
| Graph                 | Plan                  |       |       |        |
|                       | ✓                     | 77.52 | 46.68 | 62.10  |
| Ours                  | ✗                     | 76.26 | 42.23 | 59.25  |
|                       | ✓                     | 79.30 | 38.53 | 58.92  |
|                       | ✗                     | 76.76 | 39.27 | 58.02  |

다른 baseline보다 우수하다. ablation 실험은 Graph-based Sampling 전략이 이 설정에서 tool call의 accuracy를 향상시킬 수 있음을 보여준다. 이는 Graph sampling을 통해 얻은 tool들이 종종 순차적 상관관계를 가지기 때문이다. 결과적으로 학습 데이터에는 순차적으로 tool을 호출하는 더 많은 예시가 포함되어 retrieve + call 설정의 요구사항에 더 잘 부합한다.

## 5.2.3 TOOLFLOW는 error message를 기반으로 실수를 수정할 수 있다.

오류 수정은 LLM tool call의 핵심 능력이다 [[24]](#ref-24). ToolAlpaca dataset의 Simulated 설정에서 테스트를 수행하였다. 이 dataset은 GPT-4를 활용하여 call이 실패할 때의 error message를 포함한 tool의 return value를 모방하는 시뮬레이션 환경을 구축하였다. model은 이러한 error message를 기반으로 self-correct하고 call을 다시 시도할 수 있다. 이 dataset에서 TOOLFLOW의 tool call 및 수정 능력을

Table 6: ToolAlpaca Dataset 결과. Proc.와 Resp.는 각각 Procedure와 Response를 나타낸다. 표 값은 백분율로 표시된다. †는 원본 논문에서 도출된 결과를 나타낸다.

| Baselines        | Baselines        |   Proc. |   Resp. |   Overall |
|------------------|------------------|---------|---------|-----------|
| GPT-3.5 †        | GPT-3.5 †        |      77 |      85 |        75 |
| ToolAlpaca-13B † | ToolAlpaca-13B † |      63 |      69 |        60 |
| ToolAlpaca-7B †  | ToolAlpaca-7B †  |      70 |      73 |        70 |
| LLaMA-3.1 8B     | LLaMA-3.1 8B     |      74 |      80 |        74 |
|                  | Graph Plan ✓ ✓   |      85 |      88 |        84 |
| Ours             | ✓ ✗              |      80 |      85 |        80 |
| Ours             | ✗ ✓              |      81 |      83 |        79 |
| Ours             | ✗ ✗              |      78 |      83 |        77 |

평가한다.

dataset은 tool call Procedure와 model의 최종 Response의 accuracy를 평가한다. procedure는 model의 call이 ground truth와 일치할 때 정확한 것으로 간주된다. response는 model의 응답이 사용자의 instruction을 만족시킬 수 있을 때 정확한 것으로 간주된다. 둘 다 정확하면 model의 Overall 성능이 정확한 것으로 간주된다. 이 평가는 GPT-4에 의해 수행되었다. 결과는 Table 6에 제시되어 있다.

TOOLFLOW의 Procedure accuracy는 85%에 도달하여 GPT-3.5의 77%를 능가하였다. Procedure 평가에서 오류 수정은 중복 action으로 간주되어 부정확한 것으로 판단된다. 따라서 이 accuracy는 대부분의 경우 TOOLFLOW의 첫 번째 tool call이 정확함을 의미한다. 반면, TOOLFLOW의 Response accuracy 88%가 Procedure accuracy 85%보다 높다는 것은 TOOLFLOW가 일부 test case에서 오류를 수정했음을 나타낸다. 이는 학습 데이터에 오류 수정 sample이 포함되어 있지 않음에도 불구하고 TOOLFLOW가 error message를 기반으로 self-correct할 수 있는 능력이 있음을 시사한다.

## 5.2.4 TOOLFLOW의 일반 능력은 Fine-tuning으로 손상되지 않는다.

fine-tuned model은 tool call 능력은 향상되지만 다른 능력은 저하되는 catastrophic forgetting의 위험이 있다. AI assistant로서 LLM의 reasoning 및 대화 능력도 똑같이 중요하다. 따라서 catastrophic forgetting 문제가 발생했는지 검토하기 위해 MMLU, BBH, MTBench dataset에서 tuned model을 테스트하였다. 결과는 Table 7에 나와 있다.

MMLU와 BBH에 대한 테스트 결과는 학습 전후 model 간에 성능의 유의미한 차이가 없음을 보여준다.

Table 7: MTBench, MMLU, BBH에 대한 일반 능력 테스트 결과. 첫 번째 행의 결과는 tuning 전 model의 결과이다. 최고 결과는 **굵게** 표시된다. 빨간색은 성능 저하를 나타낸다.

| 설정   | 설정   | MTBench   | MTBench   | MTBench   | MMLU   | BBH   |
|-----------|-----------|-----------|-----------|-----------|--------|-------|
| Graph     | Plan      | Turn      | 1 Turn 2  | Avg.      | MMLU   | BBH   |
| -         | -         | 7.84      | 6.85      | 7.34      | 69.3%  | 63.2% |
| ✓         | ✓         | 8.08      | 7.16      | 7.62      | 69.8%  | 63.3% |
| ✓         | ✗         | 7.39      | 6.43      | 6.91      | 69.3%  | 63.5% |
| ✗         | ✓         | 7.59      | 7.04      | 7.31      | 69.1%  | 63.2% |
| ✗         | ✗         | 7.16      | 6.85      | 7.01      | 68.9%  | 63.5% |

그러나 MTBench dataset에서는 Graph-based Sampling이나 Plan-Generation 없이 데이터로 학습된 model이 성능 저하를 보였다. 특히 Turn 2 평가에서 Plan-Generation 전략을 사용하여 합성된 데이터로 학습된 model은 약간의 성능 향상을 보였다. 이러한 향상은 Plan-Generation이 합성 dialogue의 자연스러움과 coherence를 향상시켜 model의 대화 능력을 향상시키기 때문이다.

## 6 상관관계 분석

dialogue 데이터의 다양성과 coherence가 model 성능에 미치는 영향을 더 조사하기 위해 추가 상관관계 분석을 수행하였다. 이전 실험에서 TOOLFLOW와 ablation 설정을 사용하여 총 $8,000 \times 4 = 32,000$개의 dialogue를 합성하였다. 이 데이터에서 10번 무작위로 sampling하여 매번 4,000개의 dialogue를 선택해 10개의 새로운 학습 set을 구성하였다. 각 dataset에 대해 다양성 metric D-3와 H, coherence metric SS와 EnR을 계산하였다. 그런 다음 이 데이터를 사용하여 Llama3.1-8B-Instruct를 fine-tune하였다. 이 10개의 fine-tuned model을 BFCL과 MTBench에서 테스트하였다. 마지막으로 학습 데이터의 평가 metric과 model 성능 간의 Pearson 상관 계수를 계산하여 Table 8에 보고하였다.

BFCL의 평균 결과는 학습 데이터의 다양성과 coherence 모두 model의 tool-calling 능력 향상에 크게 기여함을 보여준다. MTBench 결과는 데이터 coherence와 model의 대화 성능 사이에 강한 양의 상관관계가 있음을 보여주며, 이는 우리의 가정과 일치한다. 특히, 다양성을 평가하기 위해 entropy와 Distinct-N score를 사용하지만, 이러한 metric과 model 성능 간의 일관되지 않은 상관관계는 다양성의 다른 차원을 반영할 수 있음을 시사한다. 반면,

Table 8: 데이터 평가 metric과 model 성능 간의 Pearson 상관관계. P-values > 0.2와 < 0.2는 각각 녹색과 빨간색으로 표시된다.

| Metrics           | D-3     | H       | SS      | EnR     |
|-------------------|---------|---------|---------|---------|
| BFCL              | BFCL    | BFCL    | BFCL    | BFCL    |
| Simple            | 0.174   | -0.272  | 0.347   | 0.118   |
| Parallel          | 0.250   | 0.315   | -0.168  | 0.072   |
| Multiple          | 0.336   | -0.042  | 0.644   | 0.612   |
| Parallel Multiple | -0.274  | 0.479   | -0.310  | -0.102  |
| Avg.              | 0.250   | 0.273   | 0.255   | 0.378   |
| MTBench           | MTBench | MTBench | MTBench | MTBench |
| Turn 1            | -0.087  | 0.221   | 0.262   | 0.298   |
| Turn 2            | -0.185  | 0.045   | 0.708   | 0.415   |
| Avg.              | -0.179  | 0.146   | 0.650   | 0.454   |

coherence는 BFCL의 parallel test set에 긍정적인 영향을 미치지 않는 것으로 보이는데, 이는 아마도 이러한 테스트가 단일 turn 내에서 여러 call을 포함하는 특성 때문일 것이다. 그럼에도 불구하고, TOOLFLOW가 Section 4.2에서 다양성과 coherence 증가의 이점을 입증했지만, 이 섹션의 상관관계 결과는 전체 성능에 대한 긍정적인 효과를 더욱 검증한다.

## 7 Dataset 중복 분석

평가의 신뢰성을 보장하기 위해 학습 및 테스트 dataset 간의 중복 분석을 수행하였다. 이 검토는 이러한 테스트 데이터의 독립성을 확인하고 잠재적인 데이터 유출 문제를 방지하는 데 도움이 된다. N-gram 기반 및 유사도 기반 방법을 모두 사용하여 TOOLFLOW dataset에 유의미한 데이터 유출이 없음을 입증하였다. 비교를 위해 잘 알려진 xLam agent 학습 set [[30]](#ref-30)도 대조군으로 포함하였다.

**N-gram 기반 방법** LLaMA-2 [[23]](#ref-23)에서 사용된 접근 방식을 따라, token이 평가 sample과 학습 set 모두에서 10 token보다 긴 token n-gram에 나타나면 오염된 것으로 간주하였다. JSON 문자열의 token 중 10% 이상이 오염되면 tool이 유출된 것으로 분류하였다.

**유사도 기반 방법** 주어진 tool과 평가 dataset의 어떤 tool 간의 cosine similarity가 0.9를 초과하면 tool이 유출된 것으로 정의하였다. HuggingFace 2의 all-MiniLM-L12-v2 encoder를 사용하여 모든 tool의 representation을 얻었다.

다양한 평가 metric에 걸친 데이터 유출 비율을 Table 9에 제시한다.

2 https://huggingface.co/

이러한 결과는 학습 set으로서 TOOLFLOW와 테스트 set 간에 심각한 데이터 유출이 없음을 시사한다.

Table 9: 학습 및 테스트 Set 간 중복

| Training   | ToolFlow   | ToolFlow   | xLam   | xLam       |
|------------|------------|------------|--------|------------|
| Test       | N-gram     | Similarity | N-gram | Similarity |
| BFCL       | 0.239%     | 2.922%     | 0.656% | 5.247%     |
| APIBank    | 0.000%     | 1.887%     | 0.000% | 3.774%     |
| ToolAlpaca | 0.000%     | 0.000%     | 0.000% | 0.000%     |

## 8 결론

이 연구에서는 합성 데이터의 다양성과 coherence를 향상시키기 위해 Graph-based Sampling과 Planned Generation 전략을 제안한다. 이 두 전략을 기반으로 tool calling 데이터를 합성하는 TOOLFLOW라는 pipeline을 도입하고 8,000개의 학습 sample을 생성한다. 이 dataset을 사용하여 Llama3.1-8B-Instruct에 SFT를 수행하여 model의 tool calling 능력을 향상시켰다. 이후 데이터 다양성과 coherence가 model 성능에 미치는 영향을 입증하기 위해 상관관계 분석을 수행한다. 이는 tool-enhanced agent를 위한 학습 데이터 구성에 참고가 된다.

## 한계점

한계점을 두 가지로 요약한다.

Section 3.4에서 설명한 바와 같이, seed 데이터는 16,000개의 API를 포함하는 사전 수집된 tool set이다. TOOLFLOW가 더 다양한 데이터를 합성할 수 있지만, tool set의 크기와 다양성도 데이터의 다양성에 영향을 미친다는 것은 부인할 수 없다. 그러나 seed 데이터를 풍부하게 하는 방법은 이 연구에서 아직 연구되지 않았다.

반면, TOOLFLOW는 데이터 합성에 GPT-4를 활용한 다음 이 데이터를 사용하여 8B-model을 학습한다. 따라서 여전히 강력한 model을 사용하여 약한 model을 학습시키는 paradigm에 해당한다. model이 자체적으로 합성한 데이터로 학습하여 개선될 수 있는지는 아직 알려지지 않았다. 이 weak-to-strong 설정이 더 도전적이지만 더 의미 있다고 믿는다.

## 윤리 선언

이 연구에서 GPT-4는 윤리 지침에 부합하는 방식으로 평가자 및 생성자로 사용되었다. 사용에 대한 투명성, 출력에 대한 책임, 잠재적 편향 완화가 우선시되었다. 데이터 프라이버시와 보안이 엄격하게 유지되었으며, AI의 한계가 인정되어 인간의 판단을 대체하기보다 보완하도록 보장하였다. 이 접근 방식은 학술적 무결성과 윤리 기준을 유지하면서 연구 품질을 향상시키는 것을 목표로 하였다.

## 감사의 글

이 연구는 Hong Kong RGC GRF No. 14206324, CUHK direct grant No. 4055209, CUHK Knowledge Transfer Project Fund No. KPF23GWP20의 부분적인 지원을 받았다.

## 참고문헌

<a id="ref-1"></a>**[1]** Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Sadhana Kumaravel, Matthew Stallone, Rameswar Panda, Yara Rizk, GP Bhargav, Maxwell Crouse, Chulaka Gunasekara, Shajith Ikbal, Sachin Joshi, Hima Karanam, Vineet Kumar, Asim Munawar, Sumit Neelam, Dinesh Raghu, Udit Sharma, Adriana Meza Soria, Dheeraj Sreedhar, Praveen Venkateswaran, Merve Unuvar, David Cox, Salim Roukos, Luis Lastras, and Pavan Kapanipathi. 2024. Granite-function calling model: Introducing function calling abilities via multi-task learning of granular tasks. Preprint, [arXiv:2407.00121](https://arxiv.org/abs/2407.00121).

<a id="ref-2"></a>**[2]** Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, and Anna Chen. 2022. Constitutional ai: Harmlessness from ai feedback. Preprint, [arXiv:2212.08073](https://arxiv.org/abs/2212.08073).

<a id="ref-3"></a>**[3]** Kinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell Crouse, Asim Munawar, Vernon Austel, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, and Luis Lastras. 2024. APIBLEND: A comprehensive corpora for training and benchmarking API LLMs. In *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 12859-12870, Bangkok, Thailand. Association for Computational Linguistics.

<a id="ref-4"></a>**[4]** Pawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gasic. 2018. MultiWOZ - a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 5016-5026, Brussels, Belgium. Association for Computational Linguistics.

<a id="ref-5"></a>**[5]** Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, and Alan Schelten. 2024. The llama 3 herd of models. Preprint, [arXiv:2407.21783](https://arxiv.org/abs/2407.21783).

<a id="ref-6"></a>**[6]** Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and Osmar Zaiane. 2019. Evaluating coherence in dialogue systems using entailment. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 3806-3812, Minneapolis, Minnesota. Association for Computational Linguistics.

<a id="ref-7"></a>**[7]** Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Preprint, [arXiv:2009.03300](https://arxiv.org/abs/2009.03300).

<a id="ref-8"></a>**[8]** Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. 2023. Tool documentation enables zero-shot tool-usage with large language models. Preprint, [arXiv:2308.00675](https://arxiv.org/abs/2308.00675).

<a id="ref-9"></a>**[9]** Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting objective function for neural conversation models. In *Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 110-119, San Diego, California. Association for Computational Linguistics.

<a id="ref-10"></a>**[10]** Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-bank: A comprehensive benchmark for tool-augmented LLMs. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 3102-3116, Singapore. Association for Computational Linguistics.

<a id="ref-11"></a>**[11]** Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, Zezhong Wang, et al. 2024a. Toolace: Winning the points of llm function calling. Preprint, [arXiv:2409.00920](https://arxiv.org/abs/2409.00920).

<a id="ref-12"></a>**[12]** Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles, Huan Wang, Shelby Heinecke, and Caiming Xiong. 2024b. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Preprint, [arXiv:2406.18518](https://arxiv.org/abs/2406.18518).

<a id="ref-13"></a>**[13]** Gregoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023. Augmented language models: a survey. Preprint, [arXiv:2302.07842](https://arxiv.org/abs/2302.07842).

<a id="ref-14"></a>**[14]** OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, et al. 2024. Gpt-4 technical report. Preprint, [arXiv:2303.08774](https://arxiv.org/abs/2303.08774).

<a id="ref-15"></a>**[15]** Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive apis. Preprint, [arXiv:2305.15334](https://arxiv.org/abs/2305.15334).

<a id="ref-16"></a>**[16]** Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. Preprint, [arXiv:2307.16789](https://arxiv.org/abs/2307.16789).

<a id="ref-17"></a>**[17]** Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. Preprint, [arXiv:1908.10084](https://arxiv.org/abs/1908.10084).

<a id="ref-18"></a>**[18]** Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. 2023. Tptu: Large language model-based ai agents for task planning and tool usage. Preprint, [arXiv:2308.03427](https://arxiv.org/abs/2308.03427).

<a id="ref-19"></a>**[19]** Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Preprint, [arXiv:2302.04761](https://arxiv.org/abs/2302.04761).

<a id="ref-20"></a>**[20]** C. E. Shannon. 1948. A mathematical theory of communication. *The Bell System Technical Journal*, 27(3):379-423.

<a id="ref-21"></a>**[21]** Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. Preprint, [arXiv:2210.09261](https://arxiv.org/abs/2210.09261).

<a id="ref-22"></a>**[22]** Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. Preprint, [arXiv:2306.05301](https://arxiv.org/abs/2306.05301).

<a id="ref-23"></a>**[23]** Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, and Soumya Batra. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, [arXiv:2307.09288](https://arxiv.org/abs/2307.09288).

<a id="ref-24"></a>**[24]** Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, and Yu Su. 2024. Llms in the imaginarium: Tool learning through simulated trial and error. Preprint, [arXiv:2403.04746](https://arxiv.org/abs/2403.04746).

<a id="ref-25"></a>**[25]** Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. Preprint, [arXiv:2212.10560](https://arxiv.org/abs/2212.10560).

<a id="ref-26"></a>**[26]** Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. Preprint, [arXiv:2304.12244](https://arxiv.org/abs/2304.12244).

<a id="ref-27"></a>**[27]** Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. 2023. Gpt4tools: Teaching large language model to use tools via self-instruction. Preprint, [arXiv:2305.18752](https://arxiv.org/abs/2305.18752).

<a id="ref-28"></a>**[28]** Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. Preprint, [arXiv:2210.03629](https://arxiv.org/abs/2210.03629).

<a id="ref-29"></a>**[29]** Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024. Metamath: Bootstrap your own mathematical questions for large language models. Preprint, [arXiv:2309.12284](https://arxiv.org/abs/2309.12284).

<a id="ref-30"></a>**[30]** Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, Zhiwei Liu, Yihao Feng, Tulika Awalgaonkar, Rithesh Murthy, Eric Hu, Zeyuan Chen, Ran Xu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, and Caiming Xiong. 2024. xlam: A family of large action models to empower ai agent systems. Preprint, [arXiv:2409.03215](https://arxiv.org/abs/2409.03215).

<a id="ref-31"></a>**[31]** Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. In *Proceedings of the 37th International Conference on Neural Information Processing Systems*, NIPS '23, Red Hook, NY, USA. Curran Associates Inc.

## A 부록

## A.1 데이터 품질 평가 세부사항

Dziri et al. [[6]](#ref-6)를 따라, dialogue 데이터를 Natural Language Inference (NLI) 형식으로 변환하였다. 이 형식에서 이전 dialogue round의 요청과 응답은 *premise* 역할을 하고, 현재 round의 요청은 *hypothesis* 역할을 한다. 그런 다음 학습된 BERT [[17]](#ref-17)를 사용하여 둘 사이의 관계를 예측하고 entailment 예측의 비율 (EnR)을 계산한다. 비율이 높을수록 연속 dialogue round 간의 coherence가 더 높음을 나타낸다. 또한 premise와 hypothesis 사이의 semantic similarity (SS)를 측정한다. BERT를 사용하여 문장 representation을 추출하고 cosine similarity를 계산한다. similarity score가 높을수록 더 coherent한 dialogue를 나타낸다.

다양성과 관련하여, 단어 빈도를 기반으로 텍스트의 Shannon entropy ($H$)를 계산한다. 또한 dataset에 대해 $N = 3$ (D-3)인 Distinct-N Score [[9]](#ref-9)를 계산한다. entropy나 Distinct-N Score가 높을수록 dataset이 더 많은 정보를 포함하고 더 큰 다양성을 가짐을 나타낸다. 또한, 각 set에서 200개의 dialogue를 sampling하였다. GPT-4를 사용하여 naturalness (NAT), coherence (COH), helpfulness (HELP), accuracy (ACC)의 네 가지 차원에 따라 각 dialogue를 신중하게 평가하였다. GPT-4 평가를 위한 prompt는 Table 15에 나와 있다.

자동 평가는 Planned-Generation 전략이 대화 coherence를 향상시킴을 나타낸다. SS와 EnR의 두 coherence metric 모두 이러한 개선을 반영한다. 직관적으로, 계획은 model에 의해 미리 신중하게 설계되어 더 coherent한 dialogue로 이어진다. 반면, Graph Sampling 전략은 데이터 다양성을 증가시킬 수 있다. 이는 해당 전략이 강한 연관성을 가진 tool을 sampling하고, 이러한 tool의 조합이 데이터 다양성을 향상시키기 때문이다.

GPT-4의 평가는 Planned-Generation 전략이 dialogue의 자연스러움을 향상시킴을 나타낸다. 이 metric은 dialogue가 실제 세계에서 현실적으로 발생할 수 있는지 평가한다. 계획 없이 합성된 데이터에서 대부분의 사용자 요청은 chitchat이 거의 없는 tool call이며, 이는 실제 시나리오에서 드물어 낮은 자연스러움을 초래한다. GPT-4의 coherence 평가는 자동 평가와 밀접하게 일치한다. helpfulness 측면에서 Graph Sampling과 Planned-Generation 전략 모두 개선을 보인다. Helpfulness의 낮은 점수는 주로 assistant가 parameter에 대한 후속 질문을 자주 하여 dialogue turn을 소비하기 때문이다. 이러한 전략은 그러한 행동을 줄이는 데 도움이 된다. 합성 데이터의 accuracy는 네 가지 설정 모두에서 높은데, 이는 품질 관리 도구로 사전 필터링을 하기 때문일 것이다.

## A.2 평가자로서 GPT-4의 신뢰성 분석

GPT-4 평가의 신뢰성을 검증하기 위해 추가 인간 평가를 수행하였다. 구체적으로, GPT-4가 이전에 평가한 데이터에서 50개의 예시를 sampling하여 인간이 평가하도록 하였다. 컴퓨터 과학 박사 과정 학생 4명을 모집하여 GPT-4와 동일한 기준인 naturalness, coherence, helpfulness, accuracy를 사용하여 이 50개 sample을 평가하였다. 인간 평가 결과를 수집하고 평가자 간 Cohen's Kappa 일치도 점수와 인간 및 GPT-4 평가 간의 Pearson 상관 계수를 계산하였다.

Table 10: 인간과 GPT-4 평가 간의 상관관계

| Metric              |   NAT |   COH |   HELP |   ACC |
|---------------------|-------|-------|--------|-------|
| Cohen's Kappa       |  0.63 |  0.71 |   0.92 |  0.95 |
| Pearson Correlation |  0.61 |  0.76 |   0.84 |  0.86 |

Table 10의 결과는 특히 helpfulness와 accuracy metric에서 높은 평가자 간 일치도와 GPT-4 평가와의 강한 상관관계를 보여준다. naturalness와 coherence에 대한 일치도와 상관관계는 다른 두 metric에 비해 상대적으로 낮았지만, 여전히 최소 0.61의 상관관계를 유지하여 인간 및 GPT-4 평가 간에 강한 양의 상관관계를 나타낸다. 이러한 결과가 GPT-4 평가의 신뢰성을 입증한다고 믿는다.

## A.3 데이터 합성을 위한 GPT-4 vs. Open Source LLM

closed-source, 고성능 LLMs (예: GPT-4)에 대한 합성의 의존도를 평가하기 위해 open-source 대안을 사용한 비교 실험을 수행하였다. 동일한 pipeline을 따르되 GPT-4를 LLaMA-3.1-8B-instruct로 대체하여 예비 합성 실험을 수행하였다. 초기 검증 단계이지만 4,235개의 dialogue 인스턴스를 합성하였다. 이 데이터를 사용하여 LLaMA-3.1-8B-instruct를 fine-tune하는 실험을 복제하였으며, BFCL 테스트 결과는 Table 11에 나와 있다. 여기서 "N/A"는

| Data Source           |   Simple |   Multiple |   Parallel |   Parallel multiple |   Avg. |
|-----------------------|----------|------------|------------|---------------------|--------|
| N/A                   |    90.36 |      89.5  |      73.5  |               73.5  |  83.4  |
| LLaMA-3.1-8B-instruct |    89.88 |      90.1  |      85.55 |               80.15 |  86.42 |
| GPT-4                 |    91.23 |      91.85 |      87.1  |               84.45 |  88.66 |

Table 11: GPT-4와 LLaMA가 합성한 데이터로 학습된 model의 성능 비교.

Table 12: BFCL에서 Mistral 7B와 Qwen2 7B의 결과. '+TOOLFLOW'는 우리 데이터로 학습 후 model의 결과를 나타낸다.

| Models                   | Simple   | Multiple   | Parallel   | Parallel Multiple   | Avg.   |
|--------------------------|----------|------------|------------|---------------------|--------|
| Mistral-7B-Instruct-v0.1 | 61.27%   | 54.00%     | 50.50%     | 47.50%              | 53.32% |
| + TOOLFLOW               | 86.91%   | 81.00%     | 89.00%     | 80.50%              | 84.35% |
| Qwen2-7B                 | 76.73%   | 63.50%     | 82.00%     | 55.50%              | 69.43% |
| + TOOLFLOW               | 90.18%   | 81.50%     | 89.00%     | 78.50%              | 84.80% |

Table 13: API-Bank dataset에서 Mistral 7B와 Qwen2 7B의 결과. '+TOOLFLOW'는 우리 데이터로 학습 후 model의 결과를 나타낸다.

| Models                   |    L1 |    L2 |   Avg. |
|--------------------------|-------|-------|--------|
| Mistral-7B-Instruct-v0.1 | 59.14 | 32.48 |  45.81 |
| + TOOLFLOW               | 68.42 | 45.19 |  56.81 |
| Qwen2-7B                 | 58.63 | 25.19 |  41.91 |
| + TOOLFLOW               | 64.97 | 36.93 |  50.95 |

LLaMA-3.1-8B-instruct의 baseline 성능을 나타낸다 (본문에서 보고된 결과와 일치). 특히, 우리는 Section 6의 상관관계 분석에서 각각 4,000개 dialogue의 10개 subset으로 downsampling한 기존 결과를 활용하였으며, 이는 LLaMA-3.1-8B-instruct 합성 dataset 크기와 비슷하다. 표는 이 10개 subset에 대한 평균 결과를 보고한다. LLaMA-3.1-8B-instruct가 합성한 데이터는 GPT-4가 합성한 데이터에 비해 낮은 성능을 보이지만, 격차는 크지 않다. 더욱이, LLaMA-3.1-8B-instruct의 초기 성능과 비교하여 자체 합성 데이터로 학습하면 tool-calling 능력이 향상됨을 보여준다.

## A.4 TOOLFLOW로 다른 base model 학습

Table 12와 13은 TOOLFLOW로 생성된 데이터로 fine-tune된 두 다른 base model의 결과를 보여준다. 결과는 TOOLFLOW가 여러 LLMs에서 tool-calling 성능을 향상시킴을 보여주어, 합성 데이터 접근 방식의 일반화 가능성을 검증한다.

## A.5 Prompts 및 예시

자세한 내용은 Table 14~18을 참조하라.

## Plan Generation을 위한 Prompt

You are a conversation planning assistant. Given an available tools list and the target turn, you need to devise a dialogue flow between the user and the assistant centered around the use of these tools.

Here are definitions of the tools: [tool\_defs]

The number of target turns: [tgt\_turn\_num]

The planned conversation flow should adhere to the following requirements:

1. Only output the user's general requests for each interaction, without specifying the names of tools to be called, allowing users the freedom to expand upon their requests.
2. Each request must be categorized as one of the following types: a tool call request or chitchat.
3. Not every user request can be a tool call request; Chitchat can be appropriately included as a transition between conversations. But be careful not to include too much chitchat.
4. There must be a connection between consecutive requests, ensuring a smooth progression of the conversation. For instance, initiating with a request to book a meeting room, followed by request to send meeting invitation emails.
5. Please try to design more complex requests, which either calls multiple tools or requires multiple calls to resolve.
6. The outputted conversation flow must align with the anticipated number of turns for the interaction.

Here is an example:

The number of target turns: 5

The planned conversation flow:

1. Tool Call Request: The user expresses the need to reserve a meeting room.
2. Tool Call Request: The user requests to modify the reserved meeting room.
3. Chitchat: The user engages in casual conversation with the assistant about the topic of having too many meetings.
4. Tool Call Request: The user wishes to book multiple meetings, add corresponding schedules, and then send email reminders to the meeting participants.

Please directly output your planned dialogue flow without any additional analysis or explanation:

1. ...

2. ...

...

Table 14: Plan generation을 위한 prompt

## 데이터 평가를 위한 Prompt

You are asked to evaluate some synthetic dialogue data. These synthetic dialogue occur between the user, the AI assistant, and the tool. Please evaluate the data based on the following criteria, assigning a score from 1 to 5 for each category. Use the detailed descriptions below to guide your assessment:

- Naturalness (1-5 points): Only evaluate whether the user's request and response is natural and realistic. Focus more on the natural flow of the conversation and less on the choice of words. For example, pay attention to whether users will ask similar questions in real scenarios.  And assess whether user behavior is natural. For example, real users rarely ask similar questions consecutively or ask longer questions.

- Coherence (1-5 points): Evaluate the overall flow and logical connection between the turns in the conversation. Focus on checking whether the user's previous and subsequent rounds of requests are relevant.

- Helpfulness (1-5 points): Determine the effectiveness and value of the AI assistant's responses in addressing the user's needs.

- Accuracy (1-5 points): Check for the accuracy and consistency of the information provided. Everything returned by the tool is assumed to be accurate.

Please use a more **CRITICAL** and **STRICT** evaluation method. After scoring, please provide brief comments or feedback for each category to explain your ratings.

Please provide your evaluation in the following format:

Evaluation of Synthetic Dialogue Data

1. Naturalness: [Score] / 5
- Comments: [Brief comments or feedback]
2. Coherence: [Score] / 5
- Comments: [Brief comments or feedback]
3. Helpfulness: [Score] / 5
- Comments: [Brief comments or feedback]
4. Accuracy: [Score] / 5
- Comments: [Brief comments or feedback]

The dialogue you need to evaluate are as follows:

Table 15: GPT-4 데이터 평가를 위한 prompt

## User Agent를 위한 Prompt

Please continue the next turn of dialogue based on the given tool definitions, the history of the dialogue, and the dialogue flow plan.

## 요구사항은 다음과 같다:

1. Only continue one turn, and the turn to be continued must be a user turn.
2. If the last turn of the history is the assistant asking for clarification, the user turn to be continued must provide the necessary parameters for the tool call as much as possible.
3. If the last turn of the history is the assistant fulfilling the requirement and returning the result, the user should make a new requirement. The new requirement should strictly adhere to the planned dialogue flow.
4. Please use [style] language style for the user turn to be continued. You can use references, transitions, and other methods to make the conversation more natural.
5. The requirement for the user turn to be continued must be clear and a problem that the assistant can immediately solve using the tool. The necessary parameters for the assistant to ask for clarification should also be provided immediately and not later. 6. Before continuing the writing, please clarify which step of the planned dialogue flow the current conversation is in, and output it.

## 참고용 예시는 다음과 같다:

- &lt;Start of Tool Definition&gt;
- {{"name": "book\_flight", "description": "Flight boo king tools", "arguments": {{"type": "object", "properties":
- {{"from\_city\_name": {{"description": "departure city", "type": "string"}}, "to\_city\_name": {{"description": "arrival city",

"type": "string"}}, "depart\_date": {{"description": "Departure date, in the format of YYYY-MM-dd, for example: 2024-02-24",

- "type": "string"}}, "flightNo": {{"description": "flight number", "type": "string"}}, "carbinClass": {{"description": "Class of service, such as Economy Class Y", "type": "string"}}}}, "required": ["from\_city\_name", "to\_city\_name", "depart\_date"]}}}}
- &lt;End of Tool Definition&gt;
- &lt;Start of Dialogue Flow Plan&gt;
1. Tool call requirement: The user proposes a request to book a flight ticket.
2. Tool call requirement: The user adds a request to book a flight ticket.
3. Chitchat: The user chats with the assistant about business trip issues.
- &lt;End of Dialogue Flow Plan&gt;
- &lt;Start of Dialogue History&gt;
- &lt;system&gt;The current time is 2023-5-24 08:00:00.&lt;/system&gt;
- &lt;system&gt;The user is currently in Shenzhen.&lt;/system&gt;
- &lt;user&gt;I need help booking a flight to Beijing on June 1st. Can you assist me?&lt;/user&gt;
- &lt;assistant&gt;Sure, I will help you book a flight. Please wait a moment. book\_flight|{{"from\_city\_name": "Shenzhen", "to\_city\_name": "Beijing", "depart\_date": "2023-06-01"}}&lt;/assistant&gt;
- &lt;tool&gt;{{"flight\_message": "Flight booking successful. The details are as follows: Flight number YX002, flight time 2023-0601 11:00:00 -- 15:00:00, from Shenzhen Bao'an Airport to Beijing Capital International Airport."}}&lt;/tool&gt;

&lt;assistant&gt;I have booked a flight for you on June 1st, 2023, from 11:00 AM to 3:00 PM, from Shenzhen Bao'an Airport to

Beijing Capital International Airport. The flight number is YX002.&lt;/assistant&gt;

- &lt;End of Dialogue History&gt;

## 이 예시에서 예상되는 출력은 다음과 같다:

- &lt;Start of Completed Process&gt;
1. Tool call requirement: User requests to book a flight ticket.
- &lt;End of Completed Process&gt;
- &lt;Start of Current Process&gt;
2. Tool call requirement: User needs to add a flight ticket booking requirement.
- &lt;End of Current Process&gt;
- &lt;Start of Continuation&gt;
- &lt;user&gt;Please book another return ticket, coming back on June 5th.&lt;/user&gt;
- &lt;End of Continuation&gt;

## 계속해야 할 dialogue는 다음과 같다:

&lt;Start of Tool Definition&gt; [tool\_definition] &lt;End of Tool Definition&gt; &lt;Start of Dialogue Flow Plan&gt; [conversation\_plan] &lt;End of Dialogue Flow Plan&gt; &lt;Start of Dialogue History&gt; [conversation\_begin] &lt;End of Dialogue History&gt;

## Assistant Agent를 위한 Prompt

You are an experienced assistant who resolves user issues by calling tools when necessary, and responds appropriately when tool call is not required. When you receive the return value from the tool, reply to the user based on the return value.

There are three examples,

## 예시 1.

```
<user>I need help booking a flight to Beijing on June 1st. Can you assist me?</user> <assistant>Sure, I will help you book a flight. Please wait a moment. book_flight|{{"from_city_name": "Shenzhen", "to_city_name": "Beijing", "depart_date": "2023-06-01"}}</assistant>
```

## 예시 2.

&lt;tool&gt;{{"flight\_message": "Flight booking successful. The details are as follows: Flight number YX002, flight time 2023-0601 11:00:00 -- 15:00:00, from Shenzhen Bao'an Airport to Beijing Capital International Airport."}}&lt;/tool&gt; &lt;assistant&gt;I have booked a flight for you on June 1st, 2023, from 11:00 AM to 3:00 PM, from Shenzhen Bao'an Airport to Beijing Capital International Airport. The flight number is YX002.&lt;/assistant&gt;

## 예시 3.

&lt;user&gt; All this traveling recently is really taking a toll on my sleep. &lt;/user&gt;

&lt;assistant&gt;This might be due to jet lag. It could be helpful if you rest a bit earlier tonight. Before going to bed, you might want to try some relaxing activities, like reading a physical book, listening to gentle music, or practicing meditation, to help ease the stress that has built up during the day. &lt;/assistant&gt;

tool call에 대한 가이드라인은 다음과 같다:

1. Inquiry for missing required parameters from the user only when necessary. If the required parameters are either empty or fully provided, proceed with the tool call directly.',
2. Do not fabricate information for historical data or parameters not specified by the system or the user. Avoid using any vague pronouns such as "my XX," "the user," etc.',
3. Any information already provided by the user should be included as parameter values where applicable, even if they are optional parameters.

Table 17: Assistant agent를 위한 prompt

## Tool Agent를 위한 Prompt

Now, you are to act as a tool. Given the definition of the tool and the corresponding tool call statement, you need to simulate the tool's response based on your own knowledge, and then return the result to the user in a specific format.

```
Here is the definition of the tool: [tool_defs] [tool_calls]
```

The tool call statement you receive is as follows: Please return the simulated tool-generated response in the following JSON format, where each list element represents the return of a single call. Ensure the entire result can be read by json.loads. Please don't output any additional analysis or explanation. [ { "name": ..., "results": ... }, { "name": ..., "results": ... } ... ]

Table 18: Tool agent를 위한 prompt

```
1 { 2 "type": "function", 3 "function": { 4 "name": "getcurrency", 5 "description": "Get the current exchange rate for a specific currency pair", 6 "parameters": { 7 "type": "object", 8 "properties": { 9 "basecurrency": { 10 "type": "string", 11 "description": "The base currency code , e.g., USD" 12 }, 13 "targetcurrency": { 14 "type": "string", 15 "description": "The target currency code , e.g., EUR" 16 } 17 }, 18 "required": ["basecurrency", "targetcurrency"] 19 }, 20 "results": { 21 "type": "object", 22 "properties": { 23 "exchangerate": { 24 "type": "number", 25 "description": "The current exchange rate from base currency to target currency" 26 }, 27 "last_updated": { 28 "type": "string", 29 "description": "The date and time when the exchange rate was last updated" 30 } 31 } 32 } 33 } 34 }
```

Figure 2: JSON 형식의 예시 tool.
